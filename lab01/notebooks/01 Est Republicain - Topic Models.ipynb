{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 01 Topic models - Est Republicain - Gensim - LDA vs LSA\n",
    "\n",
    "Dans ce premier lab nous allons explorer un corpus en français propre et bien structuré de façon a nous familiariser avec les outils de topic models.\n",
    "\n",
    "Nous allons d'abord travailler avec la librairie [Gensim](https://radimrehurek.com/gensim/) en python, \n",
    "\n",
    "Comparer LDA vs LSA\n",
    "\n",
    "Nous verrons comment numeriser le texte original brut afn de le rendre compatible avec une analyse avec Gensim. Le but est de construire la matrice mots-documents\n"
    "Les etapes sont:\n",
    "* \n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le corpus \n",
    "\n",
    "Constitué des articles publiés dans l'Est Républicain de mai a septembre 1999. \n",
    "Le corpus original se trouve sur http://www.cnrtl.fr/corpus/estrepublicain/. \n",
    "\n",
    "133 fichiers au format XML.\n",
    "\n",
    "\n",
    "\n",
    "Le corpus sur lequel nous allons travailler est sur ```../data/estrepublicain_annee_1999.csv```. \n",
    "\n",
    "Il contient 30k rows composés de paragraphes de longueur variable.\n",
    "\n",
    "# Chargement et exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "filename  = 'estrepublicain_annee_1999.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename)\n",
    "print(\"\\nLe corpus contient {} rows et {} colonne(s)\".format(df.shape[0], df.shape[1]))\n",
    "\n",
    "print(\"\\nColonnes: {} \".format(df.columns))\n",
    "\n",
    "print(\"\\n== Premier elements: \")\n",
    "\n",
    "for i,d in df[5:10].iterrows():\n",
    "    print('--- {}'.format(i))\n",
    "    print(d.text)\n",
    "\n",
    "\n",
    "# Ne garder que les 100 premiers commentaires\n",
    "\n",
    "df = df[0:100]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrice Mots - Documents\n",
    "\n",
    "Pour appliquer les models LDA et LSI de Gensim on doit d'abord construire la matrice mots - documents.\n",
    "Ce qui implique de tokenizer le contenu.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "_converting a sequence of characters into a sequence of tokens_\n",
    "\n",
    "Dans le context de l'analyse textuelle, un token peut etre \n",
    "* un mot\n",
    "* un bigram, n-gram, skip-gram, ....\n",
    "\n",
    "Pour extraire les tokens d'une phrase, on peut simplement splitter sur l'espace\n",
    "\n",
    "``` 'les médiateurs confirment « l'absence de solution parfaite ». '.split(' ') ```\n",
    "\n",
    "Ce qui donne\n",
    "\n",
    "``` ['les', 'médiateurs', 'confirment', '«', \"l'absence, 'de', 'solution', 'parfaite', '».', ''] ```\n",
    "\n",
    "Noter l'espace à la fin, et les tokens de ponctuation\n",
    "\n",
    "On peut aussi utiliser NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> Séparation de ```'»', '.'```\n",
    "\n",
    "mais pas de ```l``` et ```'```.\n",
    "\n",
    "\n",
    "## Ponctuation. \n",
    "\n",
    "Enlever des tokens les caractères de ponctuation\n",
    "\n",
    "On utilise pour cela la librairie ```string``` et on crée un ```translator``` a partir de la liste des caracteres a supprimer.\n",
    "\n",
    "```\n",
    "punctuation_chars = ''.join([s for s in string.punctuation if s not in ['_',\"'\",'-']  ]) + '\\r\\n“”…»«' + \"’\"\n",
    "translator = str.maketrans(' ', ' ', punctuation_chars)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "print(\"Liste original de signe de ponctuations\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "\n",
    "punctuation_chars = ''.join([s for s in string.punctuation if s not in ['_', '-']  ]) + '“”…»«’\\r\\n'\n",
    "print(\"Liste étendue de signe de ponctuations\")\n",
    "print(\"\\t{}\".format(punctuation_chars))\n",
    "\n",
    "\n",
    "#  Construction d'un dict { '~':' ', '$': ' ', ... }\n",
    "d = {}\n",
    "for k in punctuation_chars:\n",
    "    d[k] = ' '\n",
    "\n",
    "# L'operateur de translation\n",
    "translator = str.maketrans(d)\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». Et ils n'en “pensent pas moins”!\"\n",
    "new_sentence = sentence.translate(translator)\n",
    "\n",
    "print()\n",
    "print(\"=== phrase originale \\n\\t{}\\n\".format(sentence))\n",
    "print(\"=== sans la ponctuation \\n\\t{}\\n\".format(new_sentence))\n",
    "                                  \n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "Liste de mots très fréquents, qui n'apportent rien en terme d'information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"=== stopwords - français: \\n{}\".format(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Nettoyage leger du corpus\n",
    "\n",
    "\n",
    "* Enlever la ponctuation des paragraphes\n",
    "* tokeniser\n",
    "\n",
    "\n",
    "On travail maitenant sur la DataFrame avec le pattern\n",
    "\n",
    "``` df['new_column']   = df.text.apply( lambda d : fonction(d)    ) ```\n",
    "\n",
    "où new_column est la nouvelle representation des textes,  et fonction est la fonction que nous voulons appliquer: lower, stopwords, ponctuation, ...\n",
    "\n",
    "Dans l'appel a lambda ```d``` est un row de la DataFrame ```df.text```\n",
    "\n",
    "### Enlever la ponctuation \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_no_punctuation'] = df.text.apply(lambda d : ( d.translate(translator) ) )\n",
    "\n",
    "\n",
    "# df.loc[5].text: le text du 5ieme row\n",
    "\n",
    "print(\"\\n=== Avant: \\n\\t{}\".format(df.loc[5].text))\n",
    "print(\"\\n=== Apres: \\n\\t{}\".format(df.loc[5].text_no_punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization et minuscule\n",
    "df['tokens_all']  = df.text_no_punctuation.apply(lambda s : word_tokenize(s.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "\n",
    "list_stopwords = stopwords.words('french')\n",
    "\n",
    "# a partir d'une liste de tokens, retourne une liste sans les stopwords\n",
    "def remove_stopword(tokens):\n",
    "     return [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "df['tokens'] = df.tokens_all.apply(lambda tks : remove_stopword(tks) )\n",
    "\n",
    "print(\"\\n=== Original: \\n\\t{}\".format(df.loc[6].text_no_punctuation))\n",
    "print(\"\\n=== Tokens: \\n\\t{}\".format(df.loc[6].tokens_all))\n",
    "print(\"\\n=== Sans stopwords: \\n\\t{}\".format(df.loc[6].tokens))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enlever les variables intermediaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df a maintenant {} rows et {} colonnes: \\n{}\".format( df.shape[0], df.shape[1], df.columns ))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n* Ne garder que les colonnes text et tokens \")\n",
    "\n",
    "df = df[['text', 'tokens']]\n",
    "\n",
    "print(\"\\n* Calcul du nombre de tokens\")\n",
    "\n",
    "df['len_tokens'] = df.tokens.apply(len)\n",
    "\n",
    "print(\"\\n* Distribution de la longueur des paragraphes tokenizés\")\n",
    "\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ne garder que les longs paragraphes\n",
    "\n",
    "Exemple de subsetting une dataframe avec une condition sur une colonne\n",
    "\n",
    "```\n",
    "condition = df.len_token > 20\n",
    "df = df[condition] ```\n",
    "\n",
    "où ```condition``` est une serie de booleen ```[True, False, True, True, .... , False]```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ df.len_tokens > 60 ]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encore un effort\n",
    "\n",
    "Nous sommes presque a l'etape du LDA\n",
    "\n",
    "\n",
    "#### Dictionnaire\n",
    "\n",
    "Il faut d'abord construire le dictionnaire global\n",
    "\n",
    "```dictionary  = corpora.Dictionary(df.tokens)```\n",
    "\n",
    "\n",
    "Dictionary(3884 unique tokens: ['andré', 'bauer', 'bonhomme', 'st-dié', 'alain']...)\n",
    "\n",
    "#### Corpus\n",
    "\n",
    "Il faut ensuite construire la matrice mots - documents, numeriser les documents grace au dictionary\n",
    "\n",
    "```df['corpus'] = df.tokens.apply(lambda d : dictionary.doc2bow(d))```\n",
    "\n",
    "on obtient:\n",
    "0    [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1) ...]\n",
    "1    [(61, 3), (122, 1), (123, 1), (124, 3), (125, ...]\n",
    "\n",
    "Et ça ce lit de la façcon suivante:\n",
    "\n",
    "Dans le document 1, le mot 61 apparait 3 fois, le mot 122: 1 fois, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Dictionnaire de tous les tokens\n",
    "dictionary  = corpora.Dictionary(df.tokens)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus'] = df.tokens.apply(lambda d : dictionary.doc2bow(d))\n",
    "\n",
    "print(df['corpus'] .head())\n",
    "\n",
    "# aggreger les rows\n",
    "\n",
    "corpus = [c for c in df.corpus ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LDA\n",
    "\n",
    "* Definir le nombre de topic\n",
    "* Appliquer ```models.LdaModel```\n",
    "* Print les resultats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de topics: le parametre principal\n",
    "\n",
    "num_topics= 8\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    alpha = 'asymmetric',\n",
    "    eta='auto',\n",
    "    passes = 2,\n",
    "    iterations = 20\n",
    ")\n",
    "\n",
    "\n",
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autre Technique: LSI\n",
    "\n",
    "\n",
    "Necessite la TF-IDF matrice. Version normalisée de la matrice words - documents\n",
    "\n",
    "```tfidf = models.TfidfModel(corpus)```\n",
    "\n",
    "\n",
    "```corpus_tfidf = tfidf[corpus]```\n",
    "\n",
    "```lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "for doc in corpus_tfidf[0:3]: print(doc)\n",
    "\n",
    "num_topics= 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "\n",
    "lsi = models.LsiModel(corpus_tfidf , id2word=dictionary, num_topics=num_topics)\n",
    "for t in lsi.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(t)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A votre tour\n",
    "\n",
    "En utilisant maintenant tout le corpus à disposition, essayez d'extraire les meilleurs topics \n",
    "\n",
    "* Faites varier le numbre de topics\n",
    "* Ajoutez des stopwords à la liste\n",
    "* Comparez LDA vs LSI. Qu'est ce qui marche le mieux?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
