{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 01 Topic models - Est Republicain - Gensim - LDA vs LSA\n",
    "\n",
    "Dans ce premier lab nous allons explorer un corpus en français propre et bien structuré de façon à nous familiariser avec les outils de topic models.\n",
    "\n",
    "Nous travaillerons avec la librairie:\n",
    "\n",
    "* [Gensim](https://radimrehurek.com/gensim/) en python\n",
    "\n",
    "Nous verrons comment numériser le texte original brut afn de le rendre compatible avec une analyse avec Gensim. \n",
    "\n",
    "Le but étant de construire la matrice mots-documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le corpus \n",
    "\n",
    "Constitué des articles publiés dans l'Est Républicain de mai à septembre 1999. \n",
    "\n",
    "Le corpus original se trouve sur http://www.cnrtl.fr/corpus/estrepublicain/. \n",
    "\n",
    "133 fichiers au format XML a partir desquels ont été extrait 30.000 paragraphes.\n",
    "\n",
    "Le corpus sur lequel nous allons travailler est sur ```../data/estrepublicain_annee_1999.csv```. \n",
    "\n",
    "\n",
    "\n",
    "# 1. Chargement des données\n",
    "\n",
    "* Charger le document dans une dataframe Panda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Le corpus contient 30241 rows et 1 colonne(s)\n",
      "\n",
      "Colonnes: Index(['text'], dtype='object') \n",
      "\n",
      "== Premier elements: \n",
      "[[ \"André Bauer, le Bonhomme de St-Dié ; Alain Dagosto, Bières de Vézelize (54) ; Paulette Gay, Pieds de cochons de Dommartin ; Corinne Dexemple, Tourte néocastrienne ; Bernadette Paulin, Pâté lorrain de Châtenois ; Andrée Labrux, Pétrou de Senones ; Marie-Thérèse Muller, Cochonneux de la Seille de Sillegny (57) ; Ginette Laporte, Eau de Contrexéville ; Danièle Richard, Framboise saulxuronne ; Josette Pouchucq, Cuisses de grenouilles de Vittel ; Daniel Léonard, Nostre damme de Chiney (Belgique) ; Sophie Valdenaire, Pissenlit de Xertigny ; Simone Leick, Cochon d'autrefois de Sierck les Bains (57) ; Yves Lievens, Kuulkappers de St-Gilles (Belgique) ; Brigitte Lievens, Miel de montagne de Plombières ; Louise Fallot, Madeleine de Commercy ; Claude Himbert, Macaron et bergamote de Nancy ; Robert Fuchs, Marmite d'or ; Jean-Pierre Ruspini, Saumon de Salm ; Jean-Marie Mougel, Andouille du Val d'Ajol ; Jean-François Ancel, Rognons blancs de Raon aux Bois ; Evelyne Bailleux, Fine coquille de Poseïdon de Recey sur Ource (21) ; Dominique Thaller, Tête de veau de Rambervillers ; Nicole Dubois, Boudin noir de Soissons (02) ; Martine Pattey, Image d'Epinal.\"]\n",
      " [ \"Le Smash Entente Club de Lunéville (SECL) a repris ses activités au complexe Berte. Les séances d'entraînement du mercredi soir, de 20 h à 22 h, sont ouvertes à toute personne adulte désirant pratiquer le volley-ball dans un esprit de loisir et de détente. Pour ceux ou celles qui souhaitent participer au championnat loisir, un entraînement spécifique leur est proposé le lundi soir, aux mêmes horaires. L'assemblée générale du club aura lieu le mercredi 29 septembre, à 20 h, dans la salle de réunion de la maison des sports. Tous les membres du club y sont conviés ainsi que toutes les personnes intéressées. Pour tout renseignement complémentaire, s'adresser au président J. Ravaine (03.83.74.12.28).\"]\n",
      " [ \"En tout cas, du côté du PS Dole qui reste en course pour l'accession parmi l'élite régionale après un nouveau et net succès contre Devecey (4-1), on espère bien mettre à profit ces paramètres pour une qualification dans le dernier carré : « nous aborderons cette rencontre sans aucun complexe » précisait hier le président jurassien Alain Viennot. « Nous disposons d'un groupe homogène capable de se transcender dans les grandes occasions ». Celle-ci, surtout à domicile, en est une et les Dolois comptent bien tout mettre en oeuvre au côté de son buteur maison, Euvrard (17 buts et leader des buteurs de PH-B) pour parvenir à leurs fins.\"]\n",
      " [ \"« Le nombre des donneurs était en légère baisse lors de la dernière collecte à Neuves-Maisons. C'est inquiétant car les besoins sont énormes. Au niveau départemental, les stocks sont très faibles, et ne permettent de faire face à la demande que pour une période de cinq jours au grand maximum. Que se passerait-il si un sinistre d'une gravité exceptionnelle se produisait ? Il serait sans doute impossible de répondre à la demande. C'est pourquoi, nous invitons les habitants de Neuves-Maisons à participer massivement à la prochaine collecte prévue le dimanche 5 septembre, de 8 h 30 à 12 h, au centre socio culturel Jean-L'Hôte...» explique Jean Marie Greiner, président de l'association des donneurs de sang du secteur de Neuves-Maisons.\"]\n",
      " [ \"ELOYES._ Les Ramoncenais n'auront tenu qu'une heure face aux réservistes loyas, incontestablement supérieurs techniquement et au niveau de la condition physique. Hajaji allait se montrer l'attaquant le plus remuant d'entrée de jeu (6') ou avec son compère El Khala (13') permettant à Paillard de sauver les meubles. Cela tenait, jusqu'à une belle combinaison entre les deux, plus Abou en relayeur, et Hajaji trouvait les filets (17'). On retrouvait les mêmes par la suite : El Khala (20'), Hajaji (30', 37' et 44'), souvent bien servis par le jeune Claudel. Les visiteurs s'étaient néanmoins montrés dangereux par une action Peduzzi-Blouin (10') et les raids de Martins (18' et 32').\"]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "filename  = 'estrepublicain_annee_1999.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename)\n",
    "\n",
    "print(\"\\nLe corpus contient {} rows et {} colonne(s)\".format(df.shape[0], df.shape[1]))\n",
    "\n",
    "print(\"\\nColonnes: {} \".format(df.columns))\n",
    "\n",
    "print(\"\\n== Premier elements: \")\n",
    "\n",
    "print(df.head().values)\n",
    "\n",
    "# Ne garder que les 1000 premiers commentaires\n",
    "\n",
    "df = df[0:1000]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transformation avant numerisation\n",
    "\n",
    "Pour appliquer les models LDA et LSI de Gensim on doit d'abord construire la matrice mots - documents.\n",
    "Ce qui implique de tokenizer le contenu.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "_converting a sequence of characters into a sequence of tokens_\n",
    "\n",
    "Dans le context de l'analyse textuelle, un token peut etre \n",
    "\n",
    "* une syllabe,\n",
    "* un mot\n",
    "* un bigram ou n-gram, un skip-gram, ....\n",
    "\n",
    "Pour extraire les tokens d'une phrase, on peut simplement splitter sur l'espace:\n",
    "\n",
    "``` 'les médiateurs confirment « l'absence de solution parfaite ». '.split(' ') ```\n",
    "\n",
    "Ce qui donne\n",
    "\n",
    "``` ['les', 'médiateurs', 'confirment', '«', \"l'absence, 'de', 'solution', 'parfaite', '».', ''] ```\n",
    "\n",
    "Noter l'espace à la fin, et les tokens de ponctuation\n",
    "\n",
    "On peut aussi utiliser NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', 'confirment', '«', \"l'absence\", 'de', 'solution', 'parfaite', '»', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> Séparation de ```'»', '.'```\n",
    "\n",
    "\n",
    "## Ponctuation. \n",
    "\n",
    "Enlever les caractères de ponctuation des tokens.\n",
    "\n",
    "On utilise pour cela la librairie ```string``` et on crée un ```translator``` a partir de la liste des caracteres à supprimer.\n",
    "\n",
    "* Lister les caracteres à éviter\n",
    "```\n",
    "punctuation_chars = ''.join([s for s in string.punctuation if s not in ['_',\"'\",'-']  ]) + '\\r\\n“”…»«' + \"’\"\n",
    "```\n",
    "\n",
    "* Transformer en dictionnaire \n",
    "```\n",
    "{ \n",
    "    ';': ' ',    # point virgule transformé en espace\n",
    "    '.': ' ',    # point  transformé en espace\n",
    "    ect ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste original de signe de ponctuations\n",
      "\t!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Liste étendue de signe de ponctuations\n",
      "\t!\"#$%&'()*+,./:;<=>?@[\\]^`{|}~“”…»«’\r\n",
      "\n",
      "\n",
      "=== phrase originale \n",
      "\tles médiateurs confirment « l'absence de solution parfaite ». Et ils n'en “pensent pas moins”!\n",
      "\n",
      "=== sans la ponctuation \n",
      "\tles médiateurs confirment   l absence de solution parfaite    Et ils n en  pensent pas moins  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(\"Liste original de signe de ponctuations\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "\n",
    "punctuation_chars = ''.join([s for s in string.punctuation if s not in ['_', '-']  ]) + '“”…»«’\\r\\n'\n",
    "print(\"Liste étendue de signe de ponctuations\")\n",
    "print(\"\\t{}\".format(punctuation_chars))\n",
    "\n",
    "\n",
    "#  Construction d'un dict { '~':' ', '$': ' ', ... }\n",
    "d = {}\n",
    "for k in punctuation_chars:\n",
    "    d[k] = ' '\n",
    "\n",
    "# L'operateur de translation\n",
    "translator = str.maketrans(d)\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». Et ils n'en “pensent pas moins”!\"\n",
    "new_sentence = sentence.translate(translator)\n",
    "\n",
    "print()\n",
    "print(\"=== phrase originale \\n\\t{}\\n\".format(sentence))\n",
    "print(\"=== sans la ponctuation \\n\\t{}\\n\".format(new_sentence))\n",
    "                                  \n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "Liste de mots très fréquents, qui n'apportent rien en terme d'information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"=== stopwords - français: \\n{}\".format(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage  du corpus\n",
    "\n",
    "\n",
    "* Enlever la ponctuation des paragraphes\n",
    "* tokeniser\n",
    "\n",
    "\n",
    "On travail maintenant sur la DataFrame avec le pattern\n",
    "\n",
    "``` df['new_column']   = df.text.apply( lambda d : fonction(d)    ) ```\n",
    "\n",
    "où new_column est la nouvelle representation des textes,  et fonction est la fonction que nous voulons appliquer: lower, stopwords, ponctuation, ...\n",
    "\n",
    "Dans l'appel a ```lambda```,  ```d``` est un row de la DataFrame ```df.text```\n",
    "\n",
    "Le code ci dessus est equivalent à \n",
    "\n",
    "```\n",
    "    for i, d in df.iterrows():\n",
    "          df.lock[i,'new_column'] = fonction(d)\n",
    "```\n",
    "\n",
    "Mais la pattern ```df apply lambda``` est bien plus rapide\n",
    "\n",
    "## Enlever la ponctuation \n",
    "\n",
    "On applique le translator definit auparavant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_no_punctuation'] = df.text.apply(lambda d : ( d.translate(translator) ) )\n",
    "\n",
    "\n",
    "# df.loc[5].text: le text du 5ieme row\n",
    "\n",
    "print(\"\\n=== Avant: \\n\\t{}\".format(df.loc[5].text))\n",
    "print(\"\\n=== Apres: \\n\\t{}\".format(df.loc[5].text_no_punctuation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "On applique la fonction ```word_tokenize``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenization et minuscule\n",
    "df['tokens_all']  = df.text_no_punctuation.apply(lambda s : word_tokenize(s.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "On part d'une liste predéfinie que l'on complete au fur et a mesure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "\n",
    "list_stopwords = stopwords.words('french')\n",
    "\n",
    "# a partir d'une liste de tokens, retourne une liste sans les stopwords\n",
    "def remove_stopword(tokens):\n",
    "     return [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "df['tokens'] = df.tokens_all.apply(lambda tks : remove_stopword(tks) )\n",
    "\n",
    "print(\"\\n=== Original: \\n\\t{}\".format(df.loc[6].text_no_punctuation))\n",
    "print(\"\\n=== Tokens: \\n\\t{}\".format(df.loc[6].tokens_all))\n",
    "print(\"\\n=== Sans stopwords: \\n\\t{}\".format(df.loc[6].tokens))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réduire la dataframe\n",
    "\n",
    "Enlever les variables intermediaires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df a maintenant {} rows et {} colonnes: \\n{}\".format( df.shape[0], df.shape[1], df.columns ))\n",
    "\n",
    "\n",
    "print(\"\\n* Ne garder que les colonnes text et tokens \")\n",
    "\n",
    "df = df[['text', 'tokens']]\n",
    "\n",
    "print(\"\\n* Calcul du nombre de tokens\")\n",
    "\n",
    "df['len_tokens'] = df.tokens.apply(len)\n",
    "\n",
    "print(\"\\n* Distribution de la longueur des paragraphes tokenizés\")\n",
    "\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ne garder que les longs paragraphes\n",
    "\n",
    "Exemple de subsetting une dataframe avec une condition sur une colonne\n",
    "\n",
    "```\n",
    "condition = df.len_token > 20\n",
    "df = df[condition] ```\n",
    "\n",
    "où ```condition``` est une serie de booleen ```[True, False, True, True, .... , False]```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[ df.len_tokens > 60 ]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerisation  \n",
    "Encore un effort\n",
    "\n",
    "Nous sommes presque à l'etape du LDA.\n",
    "\n",
    "\n",
    "#### Dictionnaire\n",
    "\n",
    "Il faut d'abord construire le dictionnaire global\n",
    "\n",
    "```dictionary  = corpora.Dictionary(df.tokens)```\n",
    "\n",
    "\n",
    "Dictionary(3884 unique tokens: ['andré', 'bauer', 'bonhomme', 'st-dié', 'alain']...)\n",
    "\n",
    "#### Corpus\n",
    "\n",
    "Il faut ensuite construire la matrice mots - documents, numeriser les documents grace au dictionary\n",
    "\n",
    "```df['corpus'] = df.tokens.apply(lambda d : dictionary.doc2bow(d))```\n",
    "\n",
    "on obtient:\n",
    "0    [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1) ...]\n",
    "1    [(61, 3), (122, 1), (123, 1), (124, 3), (125, ...]\n",
    "\n",
    "Et ça ce lit de la façcon suivante:\n",
    "\n",
    "Dans le document 1, le mot 61 apparait 3 fois, le mot 122: 1 fois, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Dictionnaire de tous les tokens\n",
    "dictionary  = corpora.Dictionary(df.tokens)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['corpus'] = df.tokens.apply(lambda d : dictionary.doc2bow(d))\n",
    "\n",
    "print(df['corpus'] .head())\n",
    "\n",
    "# aggreger les rows\n",
    "\n",
    "corpus = [c for c in df.corpus ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LDA\n",
    "\n",
    "* Definir le nombre de topic\n",
    "* Appliquer ```models.LdaModel```\n",
    "* Print les resultats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nombre de topics: le parametre principal\n",
    "\n",
    "num_topics= 8\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    alpha = 'asymmetric',\n",
    "    eta='auto',\n",
    "    passes = 2,\n",
    "    iterations = 20\n",
    ")\n",
    "\n",
    "\n",
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autre Technique: LSI\n",
    "\n",
    "\n",
    "Necessite la TF-IDF matrice. Version normalisée de la matrice words - documents\n",
    "\n",
    "```tfidf = models.TfidfModel(corpus)```\n",
    "\n",
    "\n",
    "```corpus_tfidf = tfidf[corpus]```\n",
    "\n",
    "```lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "for doc in corpus_tfidf[0:3]: print(doc)\n",
    "\n",
    "num_topics= 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSA\n",
    "\n",
    "lsi = models.LsiModel(corpus_tfidf , id2word=dictionary, num_topics=num_topics)\n",
    "for t in lsi.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(t)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A votre tour\n",
    "\n",
    "En utilisant maintenant tout le corpus à disposition, essayez d'extraire les meilleurs topics \n",
    "\n",
    "* Faites varier le numbre de topics\n",
    "* Ajoutez des stopwords à la liste\n",
    "* Comparez LDA vs LSI. Qu'est ce qui marche le mieux?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
