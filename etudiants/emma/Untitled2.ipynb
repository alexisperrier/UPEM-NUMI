{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STM R: LAB 03\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "#  Initialization\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# set working directory\n",
    "\n",
    "# A CHANGER pour votre propre repertoire sur votre machine\n",
    "\n",
    "LAB_PATH  <- '../data/\n",
    "filename  = 'fbget_original_full_data_2016_and_2017.csv'\n",
    "\n",
    "setwd(LAB_PATH)\n",
    "\n",
    "\n",
    "\n",
    "# Chargement des packages\n",
    "\n",
    "library('stmBrowser')\n",
    "\n",
    "library('stm')\n",
    "\n",
    "library('stringr')\n",
    "\n",
    "library('wordcloud')\n",
    "\n",
    "library('geometry')\n",
    "\n",
    "library('Rtsne')\n",
    "\n",
    "library('SnowballC')\n",
    "\n",
    "library('GetoptLong')\n",
    "\n",
    "\n",
    "\n",
    "# formatter la façon dont le text est printé\n",
    "\n",
    "qq.options(\"cat_prefix\" = function(x) format(Sys.time(), \"\\n[%H:%M:%S] \"))\n",
    "\n",
    "\n",
    "\n",
    "# initialisation des parametres de l'experience\n",
    "\n",
    "s.source_file   <- 'fbget_sample_02.csv'\n",
    "\n",
    "s.data_path     <- qq(\"@{LAB_PATH}\")\n",
    "\n",
    "s.experiment    <- '01'\n",
    "\n",
    "s.source        <- strsplit(s.source_file, \"\\\\.\")[[1]][1]\n",
    "\n",
    "\n",
    "\n",
    "# Input File\n",
    "\n",
    "s.input_file    <- qq(\"@{s.data_path}@{s.source_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Output Files\n",
    "\n",
    "s.envt_filename <- qq(\"@{s.data_path}@{s.source}_@{s.experiment}.RData\")\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "#  Load data\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "qqcat(\"Load data from @{s.input_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Chargement du corpus dans une dataframe\n",
    "\n",
    "s.max_rows <- 5000\n",
    "\n",
    "df <- read.csv(s.input_file, nrows = s.max_rows, sep=\",\", encoding = \"UTF-8\")\n",
    "\n",
    "View(df)\n",
    "\n",
    "# combien de rows\n",
    "\n",
    "qqcat(\"dimensions:\")\n",
    "\n",
    "dim(df)\n",
    "\n",
    "\n",
    "\n",
    "# Vérifier le contenu\n",
    "\n",
    "qqcat(\" 2 premiers paragraphes\")\n",
    "\n",
    "df$message[0:2]\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# 1) pre process the text with basic NLP massaging\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "qqcat(\"pre processing\\n\")\n",
    "\n",
    "processed <- textProcessor(df[,'message'],\n",
    "                           \n",
    "                           language         = \"en\",\n",
    "                           \n",
    "                           lowercase        = TRUE,\n",
    "                           \n",
    "                           removestopwords  = TRUE,\n",
    "                           \n",
    "                           # customstopwords  = c('des'),\n",
    "                           \n",
    "                           removenumbers    = TRUE,\n",
    "                           \n",
    "                           removepunctuation = FALSE,\n",
    "                           \n",
    "                           wordLengths      = c(3,Inf),\n",
    "                           \n",
    "                           striphtml        = TRUE,\n",
    "                           \n",
    "                           stem             = FALSE,\n",
    "                           \n",
    "                           verbose          = FALSE,\n",
    "                           \n",
    "                           metadata         = df)\n",
    "\n",
    "print(processed)\n",
    "Encoding(df$message)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# 2) Réduire le corpus\n",
    "\n",
    "#   Exclure les mots trop frequents ou pas assez avec lower.thresh et upper.thresh\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "out   <- prepDocuments(processed$documents,\n",
    "                       \n",
    "                       processed$vocab,\n",
    "                       \n",
    "                       processed$meta,\n",
    "                       \n",
    "                       lower.thresh = 20,\n",
    "                       \n",
    "                       upper.thresh = Inf\n",
    "                       \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "docs  <- out$documents\n",
    "\n",
    "vocab <- out$vocab\n",
    "\n",
    "meta  <- out$meta\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "#  3) Topic modeling\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# specifier  num_topics  ou laisser le modele trouver le nombre de topic optimal avec num_topics = 0\n",
    "\n",
    "\n",
    "\n",
    "qqcat(\"fit stm\\n\")\n",
    "\n",
    "num_topics <- 0\n",
    "\n",
    "fit <- stm(out$documents, out$vocab,\n",
    "           \n",
    "           num_topics,\n",
    "           \n",
    "           prevalence  =~ log_klout ,\n",
    "           \n",
    "           data = meta,\n",
    "           \n",
    "           reportevery = 10,\n",
    "           \n",
    "           max.em.its  = 100,\n",
    "           \n",
    "           emtol       = 1.5e-4,\n",
    "           \n",
    "           init.type   = \"Spectral\",\n",
    "           \n",
    "           seed        = 1\n",
    "           \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "#  4) Exploration\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Plot les topics\n",
    "\n",
    "plot.STM(fit,type = \"summary\", labeltype= 'frex', main= 'Topic proportions', n = 10, xlim =c(0, 0.2))\n",
    "\n",
    "# ou\n",
    "\n",
    "plot(fit, labeltype=c(\"frex\"), main = 'Topic Most Frequent Words',bty=\"n\")\n",
    "\n",
    "\n",
    "\n",
    "# plot topics quality\n",
    "\n",
    "topicQuality(model=fit, documents=docs, main='Topic Quality',bty=\"n\")\n",
    "\n",
    "\n",
    "\n",
    "# Quels mots pour tous les topics\n",
    "\n",
    "print(labelTopics(fit, n=10))\n",
    "\n",
    "# ou pour les topics 2 et 8\n",
    "\n",
    "labelTopics(fit, n=10, c(2, 8))\n",
    "\n",
    "\n",
    "\n",
    "# Word cloud sur topic 5\n",
    "\n",
    "cloud(fit, 5)\n",
    "\n",
    "\n",
    "\n",
    "# Quels sont les documents du topic 11\n",
    "\n",
    "findThoughts(fit, texts = out$meta$note, topics = 11, n = 10 )\n",
    "\n",
    "\n",
    "\n",
    "# which topic contains the keywords: obama, clinton\n",
    "\n",
    "findTopic(fit,n = 20, c(\"obama\", 'clinton'))\n",
    "\n",
    "\n",
    "\n",
    "# find docs that have the most of topic 7\n",
    "\n",
    "which(fit$theta[,7] > 0.60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# what other topics are in doc number 123 ?\n",
    "\n",
    "which(fit$theta[123,] > 0.1)\n",
    "\n",
    "\n",
    "\n",
    "# Topic importance:\n",
    "\n",
    "colSums( fit$theta)\n",
    "\n",
    "\n",
    "\n",
    "# Influence du klout\n",
    "\n",
    "stmBrowser(fit, data=out$meta, c('log_klout'), text=\"message\", labeltype='frex')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "#  Grid search\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "n_topics = seq(from = 10, to = 80, by = 2)\n",
    "\n",
    "\n",
    "\n",
    "gridsearch <- searchK(out$documents, out$vocab,\n",
    "                      \n",
    "                      K = n_topics,\n",
    "                      \n",
    "                      reportevery = 10,\n",
    "                      \n",
    "                      # emtol       = 1.0e-4,\n",
    "                      \n",
    "                      data = meta)\n",
    "\n",
    "\n",
    "\n",
    "plot(gridsearch)\n",
    "\n",
    "print(gridsearch)\n",
    "\n",
    "\n",
    "\n",
    "# Select the best number of topics that maximizes both exclusivity  and semantic coherence\n",
    "\n",
    "plot(gridsearch$results$exclus, gridsearch$results$semcoh)\n",
    "\n",
    "text(gridsearch$results$exclus, gridsearch$results$semcoh, labels=gridsearch$results$K, cex= 0.7, pos = 2)\n",
    "\n",
    "\n",
    "\n",
    "plot(gridsearch$results$semcoh, gridsearch$results$exclus)\n",
    "\n",
    "text(gridsearch$results$semcoh, gridsearch$results$exclus, labels=gridsearch$results$K, cex= 0.7, pos = 2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
