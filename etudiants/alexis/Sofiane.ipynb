{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../etudiant/sofianedate_facebook_trump.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7b4a3d1141d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilename\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m'date_facebook_trump.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# df = df[0:1000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../etudiant/sofianedate_facebook_trump.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '../etudiant/sofiane'\n",
    "filename  = 'date_facebook_trump.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename)\n",
    "# df = df[0:1000]\n",
    "\n",
    "# sampler \n",
    "\n",
    "df = df.sample(frac= 0.5)\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= dimensions\n",
      "(15120, 1)\n",
      "= 5 premieres rangés\n",
      "                                                text\n",
      "0  Il se régale déjà, ce brave Volontaire qui a b...\n",
      "1  Les différentes affaires qui ont secoué le cyc...\n",
      "2  S3 : 1. P. Billod (TCVB) 42 pts ; 2. R. Peter ...\n",
      "3  « Faites que le rêve dévore votre vie, afin qu...\n",
      "4  1541. Kauffmann d. (Icmd courir) ; 1542. Pequi...\n",
      "= Noms des colonnes\n",
      "Index(['text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "print(\"= dimensions\")\n",
    "print(df.shape)\n",
    "print(\"= 5 premieres rangés\")\n",
    "print(df.head())\n",
    "print(\"= Noms des colonnes\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paragraphe'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changer le nom de la colonne\n",
    "df.columns = ['paragraphe']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# puis renommer colonne en 'text'\n",
    "\n",
    "df.columns = ['text']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne prendre que les 1000 premieres rangées\n",
    "\n",
    "# print(\"avant: {}\".format(df.shape))\n",
    "# df = df[0:1000]\n",
    "\n",
    "# print(\"apres: {}\".format(df.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', 'confirment', '«', \"l'absence\", 'de', 'solution', 'parfaite', '»', '.']\n",
      "['les', 'médiateurs', 'confirment', '«', \"l'absence\", 'de', 'solution', 'parfaite', '».', '']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# La phrase\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(tokens)\n",
    "tokens_on_space = sentence.split(' ')\n",
    "print(tokens_on_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste original de signe de ponctuations\n",
      "\t!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "{'!': ' ', '\"': ' ', '#': ' ', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '-': ' ', '.': ' ', '/': ' ', ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '_': ' ', '`': ' ', '{': ' ', '|': ' ', '}': ' ', '~': ' ', '«': ' ', '»': ' '}\n",
      "les médiateurs confirment « l'absence de solution parfaite ». \n",
      "les médiateurs confirment   l absence de solution parfaite    \n"
     ]
    }
   ],
   "source": [
    "# ponctuation\n",
    "\n",
    "import string\n",
    "\n",
    "print(\"Liste original de signe de ponctuations\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "punctuation_chars = string.punctuation + \"«»\"\n",
    "\n",
    "#  Construction d'un dict { '~':' ', '$': ' ', ... }\n",
    "dict_ponctuation = {}\n",
    "for k in punctuation_chars:\n",
    "    dict_ponctuation[k] = ' '\n",
    "\n",
    "print(dict_ponctuation)\n",
    "\n",
    "# # L'operateur de translation\n",
    "translator = str.maketrans(dict_ponctuation)\n",
    "\n",
    "new_sentence = sentence.translate(translator)\n",
    "print(sentence)\n",
    "print(new_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', 'confirment', 'l', 'absence', 'de', 'solution', 'parfaite']\n",
      "['médiateurs', 'confirment', 'absence', 'solution', 'parfaite']\n"
     ]
    }
   ],
   "source": [
    "# stopwords\n",
    "tokens = word_tokenize(new_sentence)\n",
    "print(tokens)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# print(\"=== stopwords - français:\")\n",
    "# print(sorted(stopwords.words('french')))\n",
    "\n",
    "# stopwords de nltk\n",
    "list_stopwords = stopwords.words('french') + ['les', 'de']\n",
    "\n",
    "# alternative\n",
    "from stop_words import get_stop_words\n",
    "list_stopwords = get_stop_words('fr')\n",
    "\n",
    "\n",
    "\n",
    "# # equivalent:\n",
    "# stopwords = stopwords.words('french')\n",
    "# stopwords.append('les')\n",
    "# stopwords.append('de')\n",
    "\n",
    "\n",
    "# \n",
    "tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "## Equivalent \n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "\n",
    "\n",
    "\n",
    "print(tokens_sans_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlevons la ponctuation\n",
    "\n",
    "df['text_no_punctuation'] = df.text.apply(lambda \n",
    "    r : ( r.translate(translator) ) \n",
    ")\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "      <th>tokens_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Premières étapes concrètes du rapprochement, g...</td>\n",
       "      <td>Premières étapes concrètes du rapprochement  g...</td>\n",
       "      <td>[premières, étapes, concrètes, du, rapprocheme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bon, allez ! On range vite les pupitres, les p...</td>\n",
       "      <td>Bon  allez   On range vite les pupitres  les p...</td>\n",
       "      <td>[bon, allez, on, range, vite, les, pupitres, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>«Nous continuerons à prier jusqu'à ce soir par...</td>\n",
       "      <td>Nous continuerons à prier jusqu à ce soir par...</td>\n",
       "      <td>[nous, continuerons, à, prier, jusqu, à, ce, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Une somme qui n'a rien à voir avec ce que coût...</td>\n",
       "      <td>Une somme qui n a rien à voir avec ce que coût...</td>\n",
       "      <td>[une, somme, qui, n, a, rien, à, voir, avec, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METZ._ Nouveau venu au sein de l'ASPTT Nancy, ...</td>\n",
       "      <td>METZ   Nouveau venu au sein de l ASPTT Nancy  ...</td>\n",
       "      <td>[metz, nouveau, venu, au, sein, de, l, asptt, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Premières étapes concrètes du rapprochement, g...   \n",
       "1  Bon, allez ! On range vite les pupitres, les p...   \n",
       "2  «Nous continuerons à prier jusqu'à ce soir par...   \n",
       "3  Une somme qui n'a rien à voir avec ce que coût...   \n",
       "4  METZ._ Nouveau venu au sein de l'ASPTT Nancy, ...   \n",
       "\n",
       "                                 text_no_punctuation  \\\n",
       "0  Premières étapes concrètes du rapprochement  g...   \n",
       "1  Bon  allez   On range vite les pupitres  les p...   \n",
       "2   Nous continuerons à prier jusqu à ce soir par...   \n",
       "3  Une somme qui n a rien à voir avec ce que coût...   \n",
       "4  METZ   Nouveau venu au sein de l ASPTT Nancy  ...   \n",
       "\n",
       "                                          tokens_all  \n",
       "0  [premières, étapes, concrètes, du, rapprocheme...  \n",
       "1  [bon, allez, on, range, vite, les, pupitres, l...  \n",
       "2  [nous, continuerons, à, prier, jusqu, à, ce, s...  \n",
       "3  [une, somme, qui, n, a, rien, à, voir, avec, c...  \n",
       "4  [metz, nouveau, venu, au, sein, de, l, asptt, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "df['tokens_all']  = df.text_no_punctuation.apply(\n",
    "    lambda r : word_tokenize(r.lower())\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "      <th>tokens_all</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Premières étapes concrètes du rapprochement, g...</td>\n",
       "      <td>Premières étapes concrètes du rapprochement  g...</td>\n",
       "      <td>[premières, étapes, concrètes, du, rapprocheme...</td>\n",
       "      <td>[premières, étapes, concrètes, rapprochement, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bon, allez ! On range vite les pupitres, les p...</td>\n",
       "      <td>Bon  allez   On range vite les pupitres  les p...</td>\n",
       "      <td>[bon, allez, on, range, vite, les, pupitres, l...</td>\n",
       "      <td>[allez, range, vite, pupitres, partitions, ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>«Nous continuerons à prier jusqu'à ce soir par...</td>\n",
       "      <td>Nous continuerons à prier jusqu à ce soir par...</td>\n",
       "      <td>[nous, continuerons, à, prier, jusqu, à, ce, s...</td>\n",
       "      <td>[continuerons, prier, jusqu, soir, disparition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Une somme qui n'a rien à voir avec ce que coût...</td>\n",
       "      <td>Une somme qui n a rien à voir avec ce que coût...</td>\n",
       "      <td>[une, somme, qui, n, a, rien, à, voir, avec, c...</td>\n",
       "      <td>[somme, rien, voir, coûterait, remplacement, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METZ._ Nouveau venu au sein de l'ASPTT Nancy, ...</td>\n",
       "      <td>METZ   Nouveau venu au sein de l ASPTT Nancy  ...</td>\n",
       "      <td>[metz, nouveau, venu, au, sein, de, l, asptt, ...</td>\n",
       "      <td>[metz, venu, sein, asptt, nancy, eric, poizat,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Premières étapes concrètes du rapprochement, g...   \n",
       "1  Bon, allez ! On range vite les pupitres, les p...   \n",
       "2  «Nous continuerons à prier jusqu'à ce soir par...   \n",
       "3  Une somme qui n'a rien à voir avec ce que coût...   \n",
       "4  METZ._ Nouveau venu au sein de l'ASPTT Nancy, ...   \n",
       "\n",
       "                                 text_no_punctuation  \\\n",
       "0  Premières étapes concrètes du rapprochement  g...   \n",
       "1  Bon  allez   On range vite les pupitres  les p...   \n",
       "2   Nous continuerons à prier jusqu à ce soir par...   \n",
       "3  Une somme qui n a rien à voir avec ce que coût...   \n",
       "4  METZ   Nouveau venu au sein de l ASPTT Nancy  ...   \n",
       "\n",
       "                                          tokens_all  \\\n",
       "0  [premières, étapes, concrètes, du, rapprocheme...   \n",
       "1  [bon, allez, on, range, vite, les, pupitres, l...   \n",
       "2  [nous, continuerons, à, prier, jusqu, à, ce, s...   \n",
       "3  [une, somme, qui, n, a, rien, à, voir, avec, c...   \n",
       "4  [metz, nouveau, venu, au, sein, de, l, asptt, ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [premières, étapes, concrètes, rapprochement, ...  \n",
       "1  [allez, range, vite, pupitres, partitions, ins...  \n",
       "2  [continuerons, prier, jusqu, soir, disparition...  \n",
       "3  [somme, rien, voir, coûterait, remplacement, 1...  \n",
       "4  [metz, venu, sein, asptt, nancy, eric, poizat,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "\n",
    "def remove_stopword(tokens):\n",
    "     return [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "# Verifier que ca marche\n",
    "remove_stopword(tokens)\n",
    "\n",
    "# appliquer a la dataframe\n",
    "\n",
    "df['tokens'] = df.tokens_all.apply(\n",
    "    lambda tks : remove_stopword(tks) \n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'text_no_punctuation', 'tokens_all', 'tokens'], dtype='object')\n",
      "Index(['text', 'tokens'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# enlever les colonnes intermediaires\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df = df[['text', 'tokens']]\n",
    "\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Premières étapes concrètes du rapprochement, g...   \n",
      "1  Bon, allez ! On range vite les pupitres, les p...   \n",
      "2  «Nous continuerons à prier jusqu'à ce soir par...   \n",
      "3  Une somme qui n'a rien à voir avec ce que coût...   \n",
      "4  METZ._ Nouveau venu au sein de l'ASPTT Nancy, ...   \n",
      "\n",
      "                                              tokens  token_count  \n",
      "0  [premières, étapes, concrètes, rapprochement, ...           63  \n",
      "1  [allez, range, vite, pupitres, partitions, ins...           65  \n",
      "2  [continuerons, prier, jusqu, soir, disparition...           51  \n",
      "3  [somme, rien, voir, coûterait, remplacement, 1...           66  \n",
      "4  [metz, venu, sein, asptt, nancy, eric, poizat,...           66  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    15120.000000\n",
       "mean        74.583929\n",
       "std         40.536422\n",
       "min         35.000000\n",
       "25%         58.000000\n",
       "50%         66.000000\n",
       "75%         80.000000\n",
       "max       1968.000000\n",
       "Name: token_count, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nombre de tokens par rangée\n",
    "pd.options.mode.chained_assignment = None\n",
    "df['token_count'] = df.tokens.apply( lambda r : len(r) )\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# repartition du nombre de tokens\n",
    "df.token_count.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# quel document a 955 tokens?\n",
    "\n",
    "condition = (df.token_count == 955)\n",
    "print(df[condition].text.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n",
      "(15114, 3)\n"
     ]
    }
   ],
   "source": [
    "# quel index\n",
    "print(df[condition].index)\n",
    "\n",
    "# \n",
    "condition = (df.token_count < 800)\n",
    "print(df[condition].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15117, 3)\n",
      "(15117, 3)\n",
      "count    15117.000000\n",
      "mean        74.341536\n",
      "std         36.091208\n",
      "min         35.000000\n",
      "25%         58.000000\n",
      "50%         66.000000\n",
      "75%         80.000000\n",
      "max        954.000000\n",
      "Name: token_count, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexis/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# enlever le paragraphe le plus long\n",
    "\n",
    "condition_filtrage = df.token_count < 955\n",
    "print(df[condition_filtrage].shape)\n",
    "\n",
    "df = df[condition_filtrage]\n",
    "print(df[condition_filtrage].shape)\n",
    "\n",
    "print(df.token_count.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(82279 unique tokens: ['premières', 'étapes', 'concrètes', 'rapprochement', 'grâce']...)\n"
     ]
    }
   ],
   "source": [
    "# Gensim - Vocabulaire\n",
    "\n",
    "from gensim import corpora, models\n",
    "dictionary  = corpora.Dictionary(df.tokens)\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ \"Premières étapes concrètes du rapprochement, grâce auquel les CCI comptent peser plus lourd dans les choix d'aménagement du territoire : les chambres vont créer un fichier consulaire unique des entreprises du département (à l'autre bout de la France, les industriels ne savent pas toujours qu'elles sont deux) ; les clubs spécifiques des deux CCI seront ouverts à toutes les entreprises du département (exemples : Vosges-accueil-tourisme, sécurité, club de présidents d'Unions commerciales pour Epinal, textile, bois, plasturgie, métiers de la montagne pour Saint-Dié). Les « doublons » (export, environnement) n'en feront plus qu'un. Chaque chambre consultera l'autre avant d'en créer d'autres et chaque « chantier » sera suivi par un ou deux élus de chaque chambre.\"\n",
      "  list([(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (19, 2), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 2), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1)])]\n",
      " [ \"Bon, allez ! On range vite les pupitres, les partitions et les instruments et on se cale dans un des fauteuils de la Maison de la Culture. Les derniers flonflons de la fête de la musique se sont en effet à peine estompés que rapplique au grand galop la fête du cinéma. On en rappelle le principe. Du dimanche 27 au mardi 29, on achète une première place au tarif plein et on reçoit, en même temps que le ticket, un carnet passeport qui donne droit à des entrées à 10 F sur toutes les autres séances auxquelles on peut assister durant les trois jours. Et ce, en quelque endroit que l'on se trouve de France et de Navarre, à condition que les salles affichent leur participation à la grande fête nationale du grand écran.\"\n",
      "  list([(22, 1), (29, 1), (53, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 3), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 2), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1)])]\n",
      " [ \"«Nous continuerons à prier jusqu'à ce soir parce que sa disparition nous choque. C'était le descendant du prophète, et un grand homme qui a fait beaucoup pour son peuple et pour son pays. On espère que son fils fera de même et, par cette prière, nous lui témoignons également notre fidélité», confie Naïma Destour, une Marocaine accompagnée de sa mère en larmes. A l'issue de la cérémonie, beaucoup sont restés sur place pour partager ensemble, dans une grande salle de la mosquée, le «Sadaka» (repas des morts) un gigantesque couscous préparé par les femmes, symbole de générosité et offrande faite à la mémoire du défunt.\"\n",
      "  list([(75, 1), (86, 1), (113, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 2), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1)])]\n",
      " [ \"Une somme qui n'a rien à voir avec ce que coûterait le remplacement des 16 kilomètres de fonte grise par de la fonte ductile : 200 MF. Alors, le calcul est vite fait. « Mais si les ruptures venaient à s'accélérer, nous envisagerions un programme de remplacement », précise le directeur général des services techniques de la CUGN, René Badot. Pour l'instant, la CUGN concentre ses efforts annuels sur l'amélioration de la qualité de l'eau (100 MF), le remplacement progressif des 20.000 branchements en plomb (200 MF sur 15 ans), le renouvellement de 9 kilomètres de réseau tous les ans et la construction du second aqueduc (soit 10 kilomètres de conduites de 1,20 m de diamètre pour 70 MF).\"\n",
      "  list([(59, 1), (97, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 3), (168, 1), (169, 3), (170, 2), (171, 1), (172, 1), (173, 2), (174, 4), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 2), (188, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 2), (200, 1), (201, 1), (202, 1), (203, 1), (204, 2), (205, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1)])]\n",
      " [ \"METZ._ Nouveau venu au sein de l'ASPTT Nancy, Eric Poizat a bien fait les choses. Pour sa quatrième épreuve sous ses nouvelles couleurs, il a enlevé, sans coup férir, le 10e triathlon de l'ESM Metz. 17e à la sortie de l'eau (600 m), il est passé à la troisième place après les 24 km à vélo. Se présentaient alors 5 km à pied pour revenir à la tête de la course. Après 2 km, il doublait le cycliste messin Harel, puis à 1.000 m de la ligne, il passait l'expérimenté Bonneau (Maizières Natation TC), pour s'imposer dans le temps de 1 h 4' 34'', devançant son suivant de neuf secondes.\"\n",
      "  list([(86, 1), (90, 1), (178, 1), (196, 1), (200, 1), (212, 2), (215, 2), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 2), (240, 1), (241, 3), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1)])]]\n"
     ]
    }
   ],
   "source": [
    "# corpus_gensim\n",
    "\n",
    "df['corpus_gensim'] = df.tokens.apply(lambda d : dictionary.doc2bow(d))\n",
    "\n",
    "print(df[['text','corpus_gensim']] .head().values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_gensim = [c for c in df.corpus_gensim ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics= 10\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus_gensim,\n",
    "    id2word      = dictionary,\n",
    "    num_topics   = num_topics,\n",
    "    alpha        = 'asymmetric',\n",
    "    eta          = 'auto',\n",
    "    passes       = 2,\n",
    "    iterations   = 20\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(\"\\n=== topic #{}\".format(t[0]))\n",
    "    print(t[1].replace('*', ': ').replace(' +',', ').replace('\"',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== topic #0\n",
      "0.624: h,  0.227: 30,  0.117: 15,  0.113: f,  0.110: 14,  0.106: 18,  0.100: 19,  0.097: 17,  0.095: 20,  0.091: ans\n",
      "\n",
      "=== topic #1\n",
      "0.538: h,  0.168: 30,  -0.119: plus,  -0.111: c,  -0.101: s,  -0.092: bien,  -0.078: ans,  -0.074: après,  -0.071: enfants,  -0.070: jeunes\n",
      "\n",
      "=== topic #2\n",
      "0.494: f,  0.302: ind,  0.257: 1,  0.212: 000,  -0.198: h,  0.197: p,  0.197: 2,  0.163: 3,  0.153: 6,  0.140: 4\n",
      "\n",
      "=== topic #3\n",
      "0.788: ind,  -0.380: f,  -0.230: 000,  0.141: p,  0.088: r,  0.077: ascap,  0.074: c,  0.072: b,  0.068: g,  -0.061: 500\n",
      "\n",
      "=== topic #4\n",
      "-0.407: ind,  -0.406: f,  0.214: 1,  -0.206: 000,  0.170: 6,  0.170: 2,  0.128: 4,  0.125: 3,  0.116: 0,  0.102: 5\n",
      "\n",
      "=== topic #5\n",
      "0.443: rue,  -0.247: f,  0.165: enfants,  -0.157: club,  -0.151: 000,  -0.147: équipe,  -0.132: saison,  -0.114: joueurs,  -0.110: championnat,  -0.105: h\n",
      "\n",
      "=== topic #6\n",
      "-0.310: enfants,  0.290: rue,  -0.173: école,  -0.153: f,  -0.138: parents,  -0.132: élèves,  0.125: conseil,  -0.117: ans,  -0.113: jeunes,  0.111: journal\n",
      "\n",
      "=== topic #7\n",
      "0.603: rue,  0.162: f,  -0.154: r,  -0.147: journal,  0.118: club,  -0.110: élèves,  -0.096: météo,  -0.094: école,  0.093: dimanche,  0.090: avenue\n",
      "\n",
      "=== topic #8\n",
      "0.246: journal,  0.245: r,  0.177: météo,  0.176: f,  -0.139: conseil,  -0.134: club,  0.132: notes,  0.131: bloc,  -0.128: jeunes,  -0.121: équipe\n",
      "\n",
      "=== topic #9\n",
      "0.270: journal,  0.249: r,  0.241: rue,  0.180: météo,  0.172: club,  0.160: équipe,  0.144: école,  0.142: bloc,  0.142: notes,  -0.137: km\n"
     ]
    }
   ],
   "source": [
    "# LSA\n",
    "\n",
    "tfidf = models.TfidfModel(corpus_gensim)\n",
    "\n",
    "corpus_tfidf = tfidf[corpus_gensim]\n",
    "\n",
    "lsi = models.LsiModel(corpus_tfidf , id2word=dictionary, num_topics=num_topics)\n",
    "for t in lsi.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(\"\\n=== topic #{}\".format(t[0]))\n",
    "    print(t[1].replace('*', ': ').replace(' +',', ').replace('\"',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
