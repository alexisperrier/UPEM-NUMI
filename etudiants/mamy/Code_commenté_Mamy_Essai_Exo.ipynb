{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "filename  = 'estrepublicain_annee_1999.csv' \n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename) #lire le fichier à partir de là où il est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= dimensions\n",
      "(30241, 1)\n",
      "= 5 premieres rangés\n",
      "                                                text\n",
      "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...\n",
      "1  Le Smash Entente Club de Lunéville (SECL) a re...\n",
      "2  En tout cas, du côté du PS Dole qui reste en c...\n",
      "3  « Le nombre des donneurs était en légère baiss...\n",
      "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...\n",
      "= Noms des colonnes\n",
      "Index(['text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#df\n",
    "#df.shape \n",
    "#df.head()\n",
    "#df.columns \n",
    "\n",
    "df\n",
    "print(\"= dimensions\") \n",
    "print(df.shape) #compte les lignes et la colonne  ****** les dimensions rangés / lignes, colonne\n",
    "print(\"= 5 premieres rangés\")\n",
    "print(df.head())  #affiche le paragraph      ********     5 premiers rangés \n",
    "print(\"= Noms des colonnes\")\n",
    "print(df.columns) # ******                               Noms des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ne garder que les 100 premiers commentaires\n",
    "df=df[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', 'confirment', '«', \"l'absence\", 'de', 'solution', 'parfaite', '»', '.']\n",
      "['les', 'médiateurs', 'confirment', '«', \"l'absence\", 'de', 'solution', 'parfaite', '».', '']\n"
     ]
    }
   ],
   "source": [
    "#tokenisation PARTIE 1\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "tokens_on_space = sentence.split(' ')\n",
    "print(tokens_on_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste original de signe de ponctuations\n",
      "\t!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Liste étendue de signe de ponctuations\n",
      "\t!\"#$%&'()*+,./:;<=>?@[\\]^`{|}~“”…»«’\r\n",
      "\n",
      "{'!': ' ', '\"': ' ', '#': ' ', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '.': ' ', '/': ' ', ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '`': ' ', '{': ' ', '|': ' ', '}': ' ', '~': ' ', '“': ' ', '”': ' ', '…': ' ', '»': ' ', '«': ' ', '’': ' ', '\\r': ' ', '\\n': ' '}\n",
      "les médiateurs confirment « l'absence de solution parfaite ». \n",
      "les médiateurs confirment   l absence de solution parfaite    \n"
     ]
    }
   ],
   "source": [
    "#ponctuation  PARTIE 2\n",
    "import string\n",
    "\n",
    "print(\"Liste original de signe de ponctuations\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "\n",
    "punctuation_chars = ''.join([s for s in string.punctuation if s not in ['_', '-']  ]) + '“”…»«’\\r\\n'\n",
    "print(\"Liste étendue de signe de ponctuations\")\n",
    "print(\"\\t{}\".format(punctuation_chars))\n",
    "\n",
    "\n",
    "#  Construction d'un dict { '~':' ', '$': ' ', ... }\n",
    "d = {}\n",
    "for k in punctuation_chars:\n",
    "    d[k] = ' '\n",
    "    \n",
    "print(d)\n",
    "\n",
    "# L'operateur de translation\n",
    "translator = str.maketrans(d)  #une fois qu'on a le dictionnaire\n",
    "\n",
    "new_sentence = sentence.translate(translator)\n",
    "print(sentence)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', 'confirment', 'l', 'absence', 'de', 'solution', 'parfaite']\n",
      "=== stopwords - français:\n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'list_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-83e4091ab25b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'french'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'les'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'de'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtokens_sans_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-83e4091ab25b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'french'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'les'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'de'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtokens_sans_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "# stopwords (MAMY)\n",
    "tokens = word_tokenize(new_sentence) #ce qu'on a fait avant dans partie 1 tokenisation\n",
    "print(tokens)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"=== stopwords - français:\")\n",
    "print(stopwords.words('french'))  #pour avoir les stopwords français\n",
    "\n",
    "stopwords = stopwords.words('french') + ['les','de'] \n",
    "\n",
    "tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords)] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', 'confirment', 'l', 'absence', 'de', 'solution', 'parfaite']\n",
      "['médiateurs', 'confirment', 'absence', 'solution', 'parfaite']\n"
     ]
    }
   ],
   "source": [
    "## PROF # stopwords\n",
    "tokens = word_tokenize(new_sentence)\n",
    "print(tokens)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# print(\"=== stopwords - français:\")\n",
    "# print(sorted(stopwords.words('french')))\n",
    "\n",
    "list_stopwords = stopwords.words('french') + ['les', 'de']\n",
    "\n",
    "# # equivalent:\n",
    "# stopwords = stopwords.words('french')\n",
    "# stopwords.append('les')\n",
    "# stopwords.append('de')\n",
    "\n",
    "\n",
    "# \n",
    "tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "## Equivalent \n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "\n",
    "\n",
    "\n",
    "print(tokens_sans_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>André Bauer, le Bonhomme de St-Dié ; Alain Dag...</td>\n",
       "      <td>André Bauer  le Bonhomme de St-Dié   Alain Dag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Smash Entente Club de Lunéville (SECL) a re...</td>\n",
       "      <td>Le Smash Entente Club de Lunéville  SECL  a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En tout cas, du côté du PS Dole qui reste en c...</td>\n",
       "      <td>En tout cas  du côté du PS Dole qui reste en c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>« Le nombre des donneurs était en légère baiss...</td>\n",
       "      <td>Le nombre des donneurs était en légère baiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELOYES._ Les Ramoncenais n'auront tenu qu'une ...</td>\n",
       "      <td>ELOYES _ Les Ramoncenais n auront tenu qu une ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
       "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
       "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
       "3  « Le nombre des donneurs était en légère baiss...   \n",
       "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
       "\n",
       "                                 text_no_punctuation  \n",
       "0  André Bauer  le Bonhomme de St-Dié   Alain Dag...  \n",
       "1  Le Smash Entente Club de Lunéville  SECL  a re...  \n",
       "2  En tout cas  du côté du PS Dole qui reste en c...  \n",
       "3    Le nombre des donneurs était en légère baiss...  \n",
       "4  ELOYES _ Les Ramoncenais n auront tenu qu une ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enlevons la ponctuation (PARTIE 2)\n",
    "\n",
    "df['text_no_punctuation'] = df.text.apply(lambda \n",
    "    r : ( r.translate(translator) ) \n",
    ")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "      <th>tokens_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>André Bauer, le Bonhomme de St-Dié ; Alain Dag...</td>\n",
       "      <td>André Bauer  le Bonhomme de St-Dié   Alain Dag...</td>\n",
       "      <td>[andré, bauer, le, bonhomme, de, st-dié, alain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Smash Entente Club de Lunéville (SECL) a re...</td>\n",
       "      <td>Le Smash Entente Club de Lunéville  SECL  a re...</td>\n",
       "      <td>[le, smash, entente, club, de, lunéville, secl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En tout cas, du côté du PS Dole qui reste en c...</td>\n",
       "      <td>En tout cas  du côté du PS Dole qui reste en c...</td>\n",
       "      <td>[en, tout, cas, du, côté, du, ps, dole, qui, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>« Le nombre des donneurs était en légère baiss...</td>\n",
       "      <td>Le nombre des donneurs était en légère baiss...</td>\n",
       "      <td>[le, nombre, des, donneurs, était, en, légère,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELOYES._ Les Ramoncenais n'auront tenu qu'une ...</td>\n",
       "      <td>ELOYES _ Les Ramoncenais n auront tenu qu une ...</td>\n",
       "      <td>[eloyes, _, les, ramoncenais, n, auront, tenu,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
       "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
       "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
       "3  « Le nombre des donneurs était en légère baiss...   \n",
       "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
       "\n",
       "                                 text_no_punctuation  \\\n",
       "0  André Bauer  le Bonhomme de St-Dié   Alain Dag...   \n",
       "1  Le Smash Entente Club de Lunéville  SECL  a re...   \n",
       "2  En tout cas  du côté du PS Dole qui reste en c...   \n",
       "3    Le nombre des donneurs était en légère baiss...   \n",
       "4  ELOYES _ Les Ramoncenais n auront tenu qu une ...   \n",
       "\n",
       "                                          tokens_all  \n",
       "0  [andré, bauer, le, bonhomme, de, st-dié, alain...  \n",
       "1  [le, smash, entente, club, de, lunéville, secl...  \n",
       "2  [en, tout, cas, du, côté, du, ps, dole, qui, r...  \n",
       "3  [le, nombre, des, donneurs, était, en, légère,...  \n",
       "4  [eloyes, _, les, ramoncenais, n, auront, tenu,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenisation (PARTIE 1)\n",
    "df['tokens_all']  = df.text_no_punctuation.apply(\n",
    "    lambda r : word_tokenize(r.lower())\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "      <th>tokens_all</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>André Bauer, le Bonhomme de St-Dié ; Alain Dag...</td>\n",
       "      <td>André Bauer  le Bonhomme de St-Dié   Alain Dag...</td>\n",
       "      <td>[andré, bauer, le, bonhomme, de, st-dié, alain...</td>\n",
       "      <td>[andré, bauer, bonhomme, st-dié, alain, dagost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Smash Entente Club de Lunéville (SECL) a re...</td>\n",
       "      <td>Le Smash Entente Club de Lunéville  SECL  a re...</td>\n",
       "      <td>[le, smash, entente, club, de, lunéville, secl...</td>\n",
       "      <td>[smash, entente, club, lunéville, secl, a, rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En tout cas, du côté du PS Dole qui reste en c...</td>\n",
       "      <td>En tout cas  du côté du PS Dole qui reste en c...</td>\n",
       "      <td>[en, tout, cas, du, côté, du, ps, dole, qui, r...</td>\n",
       "      <td>[tout, cas, côté, ps, dole, reste, course, acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>« Le nombre des donneurs était en légère baiss...</td>\n",
       "      <td>Le nombre des donneurs était en légère baiss...</td>\n",
       "      <td>[le, nombre, des, donneurs, était, en, légère,...</td>\n",
       "      <td>[nombre, donneurs, légère, baisse, lors, derni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELOYES._ Les Ramoncenais n'auront tenu qu'une ...</td>\n",
       "      <td>ELOYES _ Les Ramoncenais n auront tenu qu une ...</td>\n",
       "      <td>[eloyes, _, les, ramoncenais, n, auront, tenu,...</td>\n",
       "      <td>[eloyes, _, ramoncenais, tenu, heure, face, ré...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
       "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
       "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
       "3  « Le nombre des donneurs était en légère baiss...   \n",
       "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
       "\n",
       "                                 text_no_punctuation  \\\n",
       "0  André Bauer  le Bonhomme de St-Dié   Alain Dag...   \n",
       "1  Le Smash Entente Club de Lunéville  SECL  a re...   \n",
       "2  En tout cas  du côté du PS Dole qui reste en c...   \n",
       "3    Le nombre des donneurs était en légère baiss...   \n",
       "4  ELOYES _ Les Ramoncenais n auront tenu qu une ...   \n",
       "\n",
       "                                          tokens_all  \\\n",
       "0  [andré, bauer, le, bonhomme, de, st-dié, alain...   \n",
       "1  [le, smash, entente, club, de, lunéville, secl...   \n",
       "2  [en, tout, cas, du, côté, du, ps, dole, qui, r...   \n",
       "3  [le, nombre, des, donneurs, était, en, légère,...   \n",
       "4  [eloyes, _, les, ramoncenais, n, auront, tenu,...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [andré, bauer, bonhomme, st-dié, alain, dagost...  \n",
       "1  [smash, entente, club, lunéville, secl, a, rep...  \n",
       "2  [tout, cas, côté, ps, dole, reste, course, acc...  \n",
       "3  [nombre, donneurs, légère, baisse, lors, derni...  \n",
       "4  [eloyes, _, ramoncenais, tenu, heure, face, ré...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords (PARTIE 3)\n",
    "pd.options.mode.chained_assignment = None\n",
    "def remove_stopword(tokens):\n",
    "     return [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "# Verifier que ca marche\n",
    "#remove_stopword(tokens)\n",
    "\n",
    "# appliquer a la dataframe\n",
    "\n",
    "df['tokens'] = df.tokens_all.apply(\n",
    "    lambda tks : remove_stopword(tks) \n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"Heureusement, l'affaire ne relève en rien du trafic. L'avocat de la défense fait remarquer qu'il s'agit en tout et pour tout de trente kilos de viande, d'ailleurs isolés du reste du stock, soit 6 pâtés de canard en croûtes pâtissiers, 12 taboulés et 3 mousses de canard. Juste un casse-croûte, quand la société en est à 2000 livraisons et trente tonnes vendues chaque semaine, pour un chiffre d'affaires de 30 MF annuels. « Les dates de péremption sont calculées au plus juste. Or, les produits incriminés étaient périmés d'un jour, ou d'une semaine maximum. Ils étaient encore tout à fait consommables. Et les congeler juste avant la date de péremption, c'est ce que font toutes les ménagères en gérant leur frigo ! »\",\n",
       "       'Heureusement  l affaire ne relève en rien du trafic  L avocat de la défense fait remarquer qu il s agit en tout et pour tout de trente kilos de viande  d ailleurs isolés du reste du stock  soit 6 pâtés de canard en croûtes pâtissiers  12 taboulés et 3 mousses de canard  Juste un casse-croûte  quand la société en est à 2000 livraisons et trente tonnes vendues chaque semaine  pour un chiffre d affaires de 30 MF annuels    Les dates de péremption sont calculées au plus juste  Or  les produits incriminés étaient périmés d un jour  ou d une semaine maximum  Ils étaient encore tout à fait consommables  Et les congeler juste avant la date de péremption  c est ce que font toutes les ménagères en gérant leur frigo    ',\n",
       "       list(['heureusement', 'l', 'affaire', 'ne', 'relève', 'en', 'rien', 'du', 'trafic', 'l', 'avocat', 'de', 'la', 'défense', 'fait', 'remarquer', 'qu', 'il', 's', 'agit', 'en', 'tout', 'et', 'pour', 'tout', 'de', 'trente', 'kilos', 'de', 'viande', 'd', 'ailleurs', 'isolés', 'du', 'reste', 'du', 'stock', 'soit', '6', 'pâtés', 'de', 'canard', 'en', 'croûtes', 'pâtissiers', '12', 'taboulés', 'et', '3', 'mousses', 'de', 'canard', 'juste', 'un', 'casse-croûte', 'quand', 'la', 'société', 'en', 'est', 'à', '2000', 'livraisons', 'et', 'trente', 'tonnes', 'vendues', 'chaque', 'semaine', 'pour', 'un', 'chiffre', 'd', 'affaires', 'de', '30', 'mf', 'annuels', 'les', 'dates', 'de', 'péremption', 'sont', 'calculées', 'au', 'plus', 'juste', 'or', 'les', 'produits', 'incriminés', 'étaient', 'périmés', 'd', 'un', 'jour', 'ou', 'd', 'une', 'semaine', 'maximum', 'ils', 'étaient', 'encore', 'tout', 'à', 'fait', 'consommables', 'et', 'les', 'congeler', 'juste', 'avant', 'la', 'date', 'de', 'péremption', 'c', 'est', 'ce', 'que', 'font', 'toutes', 'les', 'ménagères', 'en', 'gérant', 'leur', 'frigo']),\n",
       "       list(['heureusement', 'affaire', 'relève', 'rien', 'trafic', 'avocat', 'défense', 'fait', 'remarquer', 'agit', 'tout', 'tout', 'trente', 'kilos', 'viande', 'ailleurs', 'isolés', 'reste', 'stock', '6', 'pâtés', 'canard', 'croûtes', 'pâtissiers', '12', 'taboulés', '3', 'mousses', 'canard', 'juste', 'casse-croûte', 'quand', 'société', '2000', 'livraisons', 'trente', 'tonnes', 'vendues', 'chaque', 'semaine', 'chiffre', 'affaires', '30', 'mf', 'annuels', 'dates', 'péremption', 'calculées', 'plus', 'juste', 'or', 'produits', 'incriminés', 'périmés', 'jour', 'semaine', 'maximum', 'ils', 'encore', 'tout', 'fait', 'consommables', 'congeler', 'juste', 'avant', 'date', 'péremption', 'font', 'toutes', 'ménagères', 'gérant', 'frigo'])], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5].values # m> on essaie d'appliquer sur un paragraphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'text_no_punctuation', 'tokens_all', 'tokens'], dtype='object')\n",
      "Index(['text', 'tokens'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#APREM 18/12/2017\n",
    "# enlever les colonnes intermediaires\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df = df[['text', 'tokens']]\n",
    "\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
      "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
      "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
      "3  « Le nombre des donneurs était en légère baiss...   \n",
      "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
      "\n",
      "                                              tokens  token_count  \n",
      "0  [andré, bauer, bonhomme, st-dié, alain, dagost...          124  \n",
      "1  [smash, entente, club, lunéville, secl, a, rep...           73  \n",
      "2  [tout, cas, côté, ps, dole, reste, course, acc...           66  \n",
      "3  [nombre, donneurs, légère, baisse, lors, derni...           69  \n",
      "4  [eloyes, _, ramoncenais, tenu, heure, face, ré...           71  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean       79.320000\n",
       "std        36.079847\n",
       "min        44.000000\n",
       "25%        63.000000\n",
       "50%        71.000000\n",
       "75%        85.000000\n",
       "max       916.000000\n",
       "Name: token_count, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nombre de tokens par rangée\n",
    "pd.options.mode.chained_assignment = None\n",
    "df['token_count'] = df.tokens.apply( lambda r : len(r) )\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# repartition du nombre de tokens\n",
    "df.token_count.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \"161. Brisset (CSA BA 133) ; 162. Tascone ; 163. Leroy (SNCF) ; 164. Cassin (Tomblaine) ; 165. Aubert (Frouard) ; 166. Pascaot ; 167. Lagrange (Messein) ; 168. Vermandé (Messein) ; 169. Rouyer (Commercy) ; 170. Chrisment (Nancy) ; 171. Lepeltier (Ecrouves) ; 172. Jacquot ; 173. Delery ; 174. Billy ; 175. Lamy ; 176. Charpentier (Ludres) ; 177. Tousch (Gondrecourt) ; 178. Carbillet (La Poste) ; 179. Brunot (Philips) ; 180. Benyoussef ; 181. Starosse (Allamps) ; 182. Seiler (Vandoeuvre) ; 183.Discristofano ; 184. Fossard (Vandoeuvre) ; 185. Boyer ; 186. Leroy ; 187. Carreiras (Golbey) ; 188. Evrard ; 189. Vagne (CSA BA 133) ; 190. Georges (Tomblaine) ; 191. Aubert (Maidières) ; 192. Leroux ; 193. Geoffroy ; 194. Clausse (Chaudenay) ; 195. Deuze (Villers) ; 196. P. Knapek (Ecrouves) ; 197. F. Kanpek (Toul) ; 198. Noël ; 199. Maire (Varangéville) ; 200. Mercky ; 201. Gaumer ; 202. Vincent (Sommerviller) ; 203. Socha (Aingeray) ; 204. Boulot (Blénod) ; 205. Houin (Amicale CHU) ; 206. Borgniet (Jarville) ; 207. Cazzulani (Bulligny) ; 208. Parisot (Lenoncourt) ; 209. Baron (Gondreville) ; 210. Marchal . 211. Ingret (Essey-lès-Nancy) ; 212. Leonardi (Saizerais) ; 213. Bar (St-Max) ; 214. Labarbe (Nancy) ; 215. Picart (Villers) ; 216. Pageot (Essey-lès-Nancy) ; 217. Guillou (Art-sur-Meurthe) ; 218. Bourgeois (Gondrecourt) ; 219. Sutter (Maxéville) ; 220. Marchi (Vandoeuvre) ; 221. Gonthier ; 222. Girardi ; 223. Heckel (Maxéville) ; 224. Lee (Philips) ; 225. Compagnon (Richardménil) ; 226. Obeltz (Gars Val Cross) ; 227. Othelet (Tantonville) ; 228. Kovac (Pexonne) ; 229. T. Godfroy ; 230. Barin . 231. Payeur (Chaudenay) ; 232. Baraud (Thierville) ; 233. Pochit ; 234. Antoine ; 235. Peiffer (Chaudenay) ; 236. R. Deloy (Nomexy) ; 237. G. Deloy ; 238. Milbach ; 239. Andre (Benney) ; 240. Petitjean ; 241. Perrin (Lunéville) ; 242. Pernel (Blénod) ; 243. Ferreira ; 244. Alfieri (Laneuveville) ; 245. Hutinet ; 246. Defente (Blénod) ; 247. Villaume ; 248. Gérard (Nves-Maisons) ; 249. Payeur (Commercy) ; 250. Bournon ; 251. Collet (Philips) ; 252. Ludmann (Hériménil) ; 253. Vallance (Goviller) ; 254. Belin (Vandoeuvre) ; 255. Verd ; 256. Renard . 257. Bastien ; 258. Hincelin ; 259. Roger (Essey-lès-Nancy) ; 260. Rakotoarison (St-Max) ; 261. Deshayes (CSA BA 133) ; 262. Debias ; 263. Mouillé (Tomblaine) ; 264. Remy (Nancy) ; 265. Baechler (Dombasle) ; 266. Maillet ; 267. Saint Jours ; 268. Pierre (Blénod) ; 269. Morisot . 270. Serveur (Essey-lès-Nancy) ; 271. Fritz ; 272. Mangenot ; 273. Butin (Toul) ; 274. J. Etienne ; 275. D. Etienne ; 276. Mathis ; 277. Boyer (Gondrecourt) ; 278. Mandra (Gars Val Cross) ; 279. Belhote (Pulnoy) ; 280. Alison (Nimes) ; 281. Liegey (Varangéville) ; 282. Pierrat (Spiridon Club) ; 283. Humbert (Seichamps) ; 284. Back (Bouzonville) ; 285. Dewitte ; 286. Philippe (Thiébauménil) ; 287. Boyer ; 288. Durand (Houdemont) ; 289. Thieblemont ; 290. Baumann (Boncourt) ; 291. Tabouret (Toul) ; 292. Husson (Blénod) ; 293. Emmanuel (Damelevieres) ; 294. Pernot ; 295. Schwoerer (Nancy) ; 296. Sitz (Dieulouard) ; 297. Chuste ; 298. Laithier (Nancy) ; 299. Abscheidt ; 300. Pagliarela (Philips) ; 301. Tardy (Golbey) ; 302. Koffolt (Vincey) ; 303. Chassatte (Dombasle) ; 304. Vaxelaire ; 305. Robert (Nancy) ; 306. Chretien (Villers) ; 307. Paulin (Belleville) ; 308. Morville (Villers) ; 309. Ziegler (Bataville) ; 310. Morville (Gondrecourt) ; 311. Nowakowski (SNCF) ; 312. Fourar (Vandoeuvre) ; 313. Osiewiez (Energ.Pavois) ; 314. Dominiak (Vandoeuvre) ; 315. Ricatte (Lunéville) ; 316. Faltot (Lunéville) ; 317. Candat ; 318. Bardot (Lunéville) ; 319. Martin . 320. Ferry ; 321. Mathieu (Nancy) ; 322. Chenin ; 323. Geoffroy (AFPA) ; 324. Filliot (AFPA) ; 325. Grapinet ; 326. Pelon (Laquenexy) ; 327. Tournoy (Laneuveville) ; 328. Picazo ; 329. Poirot (Velaine-en-Haye) ; 330. Lostetier ; 331. Zamboni (Varangéville) ; 332. Denat ; 333. Charpentier (Ludres) ; 334. Michel ; 335. Dolveck (Blénod) ; 336. Buchi ; 337. Berteaux ; 338. Legast (Chaudenay) ; 339. Pawlowski (Dieulouard) ; 340. Reinhard (Langley) ; 341. Moine (Heillecourt) ; 342. Carpentier (SNCF) ; 343. Liegeois (Ars-sur-Moselle) ; 344. Gousse (Villers) ; 345. Poinsard (Lunéville) ; 346. Dimarcq ; 347. Blaise (Blénod) ; 348. Desgranges (Gondreville) ; 349. Falchetto ; 350. Flament (Méréville) ; 351. Heloir (Messein) ; 352. Arson (Gars Val Cross) ; 353. Richard (Einvaux) ; 354. Mercier (Rosières-aux-Salines) ; 355. Pernossi (Alstom Moteurs) ; 356. Poncet (Vincey) ; 357. Wolfarth (Malleloy) ; 358. Toldre (Laneuveville) ; 359. Touillet (Nancy) ; 360. Brice (Spiridon Club) ; 361. Seyer (Metz) ; 362. Mauclair (Lunéville) ; 363. Bourtembourg (Saulxures-lès-Nancy) ; 364. Gallois ; 365. Leroy (Blénod) ; 366. Schneider (Pagny-sur-Moselle) ; 367. Pierre (Mont-sur-Meurthe) ; 368. Vachon . 369. Lorphelin (Laneuville) ; 370. Thil ; 371. Lafont (Nancy) ; 372. Lunéville) ; 373. Martinez (La Poste) ; 374. Drahon (Toul) ; 375. Cuny (Gars Val Cross) ; 376. Rousseau (Malzéville) ; 377. Vernay (PAM) ; 378. Borgniet (Jarville) ; 379. Caballero (Ecrouves) ; 380. Hacquard (Champigneulles) ; 381. Remy (Neuves-Maisons) ; 382. Ingret (Essey-lès-Nancy) ; 383. Liegeois (Ars-sur-Moselle) ; 384. Thivet . 385. Deutzer (Philips) ; 386. Zminka (Toul) ; 387. François (Energ.Pavois) ; 388. Charrue (Paris) ; 389. Mercier (Bouxières) ; 390. Leclere ; 391. Bourguignon (Blainville) ; 392. Durand (Gars Val Cross) ; 393. Maron (Custines) ; 394. Huttier ; 395. Hesse (Bicquelet) ; 396. Moine (Heillecourt) ; 397. Boul ; 398. Evezard ; 399. Charpentier (Ludres) ; 400. Melich ; 401. Allard ; 402. Richard ; 403. Steck (Lucey) ; 404. Flament (Richardménil) ; 405. Genot (Heillecourt) ; 406. Mougel (Ludres) ; 407. Richard (Sports Loisirs) ; 408. Baillot ; 409. S. Bour (Heillecourt) ; 410. A. Bour (Heillecourt) ; 411. Lance ; 412. Melchior (Laxou) ; 413. Haut (Metz) ; 414. Bruant (Tomblaine) ; 415. Barbette ; 416. Chassatte (Dombasle) ; 417. Joublin ; 418. Liegey (Varangéville) ; 419. Marchal ; 420. Henard ; 421. Mouchet (Sport Loisirs) ; 422. Stephan (Pulnoy) ; 423. Gauchey (Dommartin-lès-Toul) ; 424. Geoffroy ; 425. Gallois (Maxéville) ; 426. Chapelle (Golbey) ; 427. Serveur (Essey-lès-Ncy) ; 428. Piazuelo (Toul) ; 429. Arson (Gars Val Cross) ; 430. Sitz (Dieulouard) ; 431. Poinsignon (Seichamps) ; 432. Thirion ; 433. Larbaletrier (Malzéville) ; 434. Desloges (Blénod) ; 435. Pelte (Réméréville) ; 436. Simon ; 437. Keniche (Nancy) ; 438. Othelet ; 439. Marchal ; 440. Belin (Vandoeuvre) ; 441. Carpentier (SNCF) ; 442. Heloir (Messein) ; 443. Borgniet (Laneuveville) ; 444. L'Huillier ; 445. Boes (Menaucourt) ; 446. Morlon (Jarville) ; 447. Dieudonne (Varangéville) ; 448. Prignot (Villers-lès-Ncy) ; 449. Ihry (Neuves-Maisons) ; 450. Lottin (Laferte-sur-Am.) ; 451. Lotto ; 452. Durand (Gars Val Cross) ; 453. C. Larbaletrier (CHU Nancy) ; 454. Mathis (Rosières-aux-Salines) ; 455. Schatzbe ; 456. Dannequin ; 457. Cael ; 458. Genelot ; 459. Rahnena (Ludres) ; 460. Montovani ; 461. Philippot ; 462. Schertz ; 463. Barozzi (Nancy) ; 464. Simon (Seichamps) ; 465. Heinrich ; 466. Deny (Neuves-Maisons) ; 467. Varnerot (Dommartin-lès-Toul) ; 468. Deveze ; 469. Bicquelet ; 470. Haut (Metz) ; 471. Rouyer (AFPA) ; 472. Pierot (Sport Loisir) ; 473. Legroux (AFPA) ; 474. Voitot (Toul) ; 475. Fourcaulx (Fléville) ; 476. Dubail (Neugartheim) ; 477. G. Dannequin ; 478. D. Bicquelet ; 479. Martin ; 480. Durand (Gars Val Cross) ; 481. Tekieli (Rosières-aux-Salines) ; 482. Marc (Ars-sur-Moselle) ; 483. Thivet ; 484. Gauchey (Dommartin-lès-Toul) ; 485. Genelot ; 486. Picot (Jarville) ; 487. Moriset.\"]\n"
     ]
    }
   ],
   "source": [
    "# quel document a 916 tokens?\n",
    "\n",
    "condition = (df.token_count == 916)\n",
    "print(df[condition].text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20264 unique tokens: ['andré', 'bauer', 'bonhomme', 'st-dié', 'alain']...)\n"
     ]
    }
   ],
   "source": [
    "# Dictionnaire de tous les tokens\n",
    "from gensim import corpora, models\n",
    "dictionary  = corpora.Dictionary(df.tokens)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ \"André Bauer, le Bonhomme de St-Dié ; Alain Dagosto, Bières de Vézelize (54) ; Paulette Gay, Pieds de cochons de Dommartin ; Corinne Dexemple, Tourte néocastrienne ; Bernadette Paulin, Pâté lorrain de Châtenois ; Andrée Labrux, Pétrou de Senones ; Marie-Thérèse Muller, Cochonneux de la Seille de Sillegny (57) ; Ginette Laporte, Eau de Contrexéville ; Danièle Richard, Framboise saulxuronne ; Josette Pouchucq, Cuisses de grenouilles de Vittel ; Daniel Léonard, Nostre damme de Chiney (Belgique) ; Sophie Valdenaire, Pissenlit de Xertigny ; Simone Leick, Cochon d'autrefois de Sierck les Bains (57) ; Yves Lievens, Kuulkappers de St-Gilles (Belgique) ; Brigitte Lievens, Miel de montagne de Plombières ; Louise Fallot, Madeleine de Commercy ; Claude Himbert, Macaron et bergamote de Nancy ; Robert Fuchs, Marmite d'or ; Jean-Pierre Ruspini, Saumon de Salm ; Jean-Marie Mougel, Andouille du Val d'Ajol ; Jean-François Ancel, Rognons blancs de Raon aux Bois ; Evelyne Bailleux, Fine coquille de Poseïdon de Recey sur Ource (21) ; Dominique Thaller, Tête de veau de Rambervillers ; Nicole Dubois, Boudin noir de Soissons (02) ; Martine Pattey, Image d'Epinal.\"\n",
      "  list([(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 2), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 2), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 2), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1)])]\n",
      " [ \"Le Smash Entente Club de Lunéville (SECL) a repris ses activités au complexe Berte. Les séances d'entraînement du mercredi soir, de 20 h à 22 h, sont ouvertes à toute personne adulte désirant pratiquer le volley-ball dans un esprit de loisir et de détente. Pour ceux ou celles qui souhaitent participer au championnat loisir, un entraînement spécifique leur est proposé le lundi soir, aux mêmes horaires. L'assemblée générale du club aura lieu le mercredi 29 septembre, à 20 h, dans la salle de réunion de la maison des sports. Tous les membres du club y sont conviés ainsi que toutes les personnes intéressées. Pour tout renseignement complémentaire, s'adresser au président J. Ravaine (03.83.74.12.28).\"\n",
      "  list([(121, 1), (122, 1), (123, 3), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 2), (133, 2), (134, 2), (135, 2), (136, 3), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 2), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1)])]\n",
      " [ \"En tout cas, du côté du PS Dole qui reste en course pour l'accession parmi l'élite régionale après un nouveau et net succès contre Devecey (4-1), on espère bien mettre à profit ces paramètres pour une qualification dans le dernier carré : « nous aborderons cette rencontre sans aucun complexe » précisait hier le président jurassien Alain Viennot. « Nous disposons d'un groupe homogène capable de se transcender dans les grandes occasions ». Celle-ci, surtout à domicile, en est une et les Dolois comptent bien tout mettre en oeuvre au côté de son buteur maison, Euvrard (17 buts et leader des buteurs de PH-B) pour parvenir à leurs fins.\"\n",
      "  list([(4, 1), (129, 1), (165, 1), (174, 2), (178, 1), (185, 1), (186, 2), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 2), (204, 2), (205, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1)])]\n",
      " [ \"« Le nombre des donneurs était en légère baisse lors de la dernière collecte à Neuves-Maisons. C'est inquiétant car les besoins sont énormes. Au niveau départemental, les stocks sont très faibles, et ne permettent de faire face à la demande que pour une période de cinq jours au grand maximum. Que se passerait-il si un sinistre d'une gravité exceptionnelle se produisait ? Il serait sans doute impossible de répondre à la demande. C'est pourquoi, nous invitons les habitants de Neuves-Maisons à participer massivement à la prochaine collecte prévue le dimanche 5 septembre, de 8 h 30 à 12 h, au centre socio culturel Jean-L'Hôte...» explique Jean Marie Greiner, président de l'association des donneurs de sang du secteur de Neuves-Maisons.\"\n",
      "  list([(136, 2), (151, 1), (162, 1), (178, 1), (183, 1), (213, 1), (242, 1), (243, 2), (244, 1), (245, 1), (246, 1), (247, 1), (248, 2), (249, 3), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 2), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1)])]\n",
      " [ \"ELOYES._ Les Ramoncenais n'auront tenu qu'une heure face aux réservistes loyas, incontestablement supérieurs techniquement et au niveau de la condition physique. Hajaji allait se montrer l'attaquant le plus remuant d'entrée de jeu (6') ou avec son compère El Khala (13') permettant à Paillard de sauver les meubles. Cela tenait, jusqu'à une belle combinaison entre les deux, plus Abou en relayeur, et Hajaji trouvait les filets (17'). On retrouvait les mêmes par la suite : El Khala (20'), Hajaji (30', 37' et 44'), souvent bien servis par le jeune Claudel. Les visiteurs s'étaient néanmoins montrés dangereux par une action Peduzzi-Blouin (10') et les raids de Martins (18' et 32').\"\n",
      "  list([(135, 1), (156, 1), (203, 1), (234, 1), (254, 1), (261, 1), (286, 1), (299, 1), (300, 1), (301, 1), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 3), (312, 1), (313, 1), (314, 1), (315, 2), (316, 1), (317, 1), (318, 1), (319, 1), (320, 1), (321, 2), (322, 2), (323, 1), (324, 1), (325, 1), (326, 1), (327, 1), (328, 1), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 1), (341, 1), (342, 1), (343, 1), (344, 1), (345, 1), (346, 1), (347, 1), (348, 1), (349, 1), (350, 1), (351, 1), (352, 1), (353, 1), (354, 1), (355, 1), (356, 1), (357, 1)])]]\n"
     ]
    }
   ],
   "source": [
    "# corpus_gensim\n",
    "\n",
    "df['corpus_gensim'] = df.tokens.apply(lambda d : dictionary.doc2bow(d))\n",
    "\n",
    "print(df[['text','corpus_gensim']] .head().values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_gensim = [c for c in df.corpus_gensim ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics= 100\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus_gensim,\n",
    "    id2word      = dictionary,\n",
    "    num_topics   = num_topics,\n",
    "    alpha        = 'asymmetric',\n",
    "    eta          = 'auto',\n",
    "    passes       = 2,\n",
    "    iterations   = 20 \n",
    ")    #iteration = nombre de pass \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== topic #0\n",
      "0.011: a,  0.006: cette,  0.004: tout,  0.004: bien,  0.004: être,  0.003: f,  0.003: ans,  0.003: conseil,  0.003: h,  0.003: rue,  0.003: école,  0.003: deux,  0.003: depuis,  0.003: travaux,  0.003: ville,  0.002: plus,  0.002: -,  0.002: adjoint,  0.002: passe,  0.002: salle\n",
      "\n",
      "=== topic #1\n",
      "0.018: a,  0.007: plus,  0.005: fait,  0.005: ils,  0.004: tout,  0.004: deux,  0.004: très,  0.003: faire,  0.003: tour,  0.003: depuis,  0.003: sans,  0.003: après,  0.003: ans,  0.003: président,  0.003: cette,  0.003: france,  0.003: encore,  0.003: cours,  0.003: rues,  0.003: leurs\n",
      "\n",
      "=== topic #2\n",
      "0.007: a,  0.006: ans,  0.003: eau,  0.003: h,  0.003: après,  0.003: lac,  0.003: grande,  0.003: cette,  0.003: deux,  0.003: théâtre,  0.003: rôle,  0.002: peinture,  0.002: pont-à-mousson,  0.002: 30,  0.002: faire,  0.002: jeunes,  0.002: 1,  0.002: hier,  0.002: 5,  0.002: 6\n",
      "\n",
      "=== topic #3\n",
      "0.018: a,  0.005: deux,  0.004: alors,  0.004: ans,  0.004: aussi,  0.003: plus,  0.003: hier,  0.003: où,  0.003: peu,  0.003: ils,  0.003: enfants,  0.003: soir,  0.003: avoir,  0.003: mois,  0.002: tout,  0.002: 2000,  0.002: depuis,  0.002: si,  0.002: école,  0.002: michel\n",
      "\n",
      "=== topic #4\n",
      "0.015: a,  0.006: fait,  0.006: f,  0.005: plus,  0.005: deux,  0.005: si,  0.004: bien,  0.004: ils,  0.003: répond,  0.003: être,  0.003: là,  0.003: rentrée,  0.003: comment,  0.003: toujours,  0.003: met,  0.003: 20,  0.003: mois,  0.003: ensemble,  0.003: _,  0.003: quand\n",
      "\n",
      "=== topic #5\n",
      "0.011: a,  0.006: cette,  0.005: tout,  0.005: air,  0.005: jusqu,  0.004: environnement,  0.004: plus,  0.004: bien,  0.004: avant,  0.003: enfants,  0.003: ans,  0.003: 4,  0.003: année,  0.003: depuis,  0.003: sans,  0.003: si,  0.003: petits,  0.003: alors,  0.003: contrexéville,  0.003: autres\n",
      "\n",
      "=== topic #6\n",
      "0.010: a,  0.007: cette,  0.006: 15,  0.005: plus,  0.004: club,  0.004: depuis,  0.004: très,  0.004: deux,  0.003: place,  0.003: france,  0.003: peu,  0.003: monde,  0.003: 14,  0.003: ans,  0.003: tout,  0.003: dernière,  0.003: 16,  0.002: municipale,  0.002: aussi,  0.002: avoir\n",
      "\n",
      "=== topic #7\n",
      "0.009: plus,  0.009: a,  0.004: h,  0.004: but,  0.004: deux,  0.004: ils,  0.003: années,  0.003: comme,  0.003: 1,  0.003: ans,  0.003: jeunes,  0.003: rien,  0.002: faire,  0.002: art,  0.002: lors,  0.002: trois,  0.002: longtemps,  0.002: depuis,  0.002: moment,  0.002: cette\n",
      "\n",
      "=== topic #8\n",
      "0.012: a,  0.007: pts,  0.006: plus,  0.006: ans,  0.003: p,  0.003: 000,  0.003: 5,  0.003: cette,  0.003: 1,  0.003: vosgien,  0.003: 3,  0.003: euro,  0.003: mètres,  0.003: jeunes,  0.003: f,  0.002: leurs,  0.002: général,  0.002: présenté,  0.002: 7,  0.002: club\n",
      "\n",
      "=== topic #9\n",
      "0.009: a,  0.007: comme,  0.005: deux,  0.005: trois,  0.005: être,  0.004: travaux,  0.004: travail,  0.003: école,  0.003: beauté,  0.003: f,  0.003: première,  0.003: cela,  0.003: écoles,  0.002: discuter,  0.002: déjà,  0.002: département,  0.002: autres,  0.002: enfants,  0.002: public,  0.002: 2000\n",
      "\n",
      "=== topic #10\n",
      "0.055: h,  0.017: 30,  0.015: a,  0.010: 14,  0.007: 15,  0.007: 16,  0.007: rue,  0.005: plus,  0.005: 17,  0.005: 13,  0.004: 11,  0.004: après,  0.003: cette,  0.003: tout,  0.003: f,  0.003: 10,  0.003: 18,  0.003: course,  0.003: 19,  0.003: bien\n",
      "\n",
      "=== topic #11\n",
      "0.006: a,  0.005: si,  0.004: h,  0.004: hier,  0.004: cette,  0.004: association,  0.003: dimanche,  0.003: être,  0.003: 6,  0.003: deux,  0.003: jeunes,  0.003: ventes,  0.003: soir,  0.003: commune,  0.003: plus,  0.003: lors,  0.003: quelques,  0.002: nouveau,  0.002: donc,  0.002: maire\n",
      "\n",
      "=== topic #12\n",
      "0.021: a,  0.008: h,  0.005: bien,  0.005: plus,  0.004: tout,  0.004: 30,  0.004: jeunes,  0.004: enfants,  0.003: fait,  0.003: 40,  0.003: deux,  0.003: ville,  0.003: président,  0.003: temps,  0.003: comme,  0.003: sans,  0.003: alors,  0.003: culture,  0.003: faire,  0.002: pierre\n",
      "\n",
      "=== topic #13\n",
      "0.020: a,  0.014: née,  0.010: ans,  0.004: plus,  0.004: tout,  0.004: aussi,  0.003: toutes,  0.003: enfants,  0.003: deux,  0.003: travail,  0.003: temps,  0.003: après,  0.003: année,  0.003: médailles,  0.003: h,  0.003: très,  0.003: centre,  0.002: école,  0.002: fois,  0.002: avoir\n",
      "\n",
      "=== topic #14\n",
      "0.016: a,  0.005: fait,  0.004: plus,  0.004: varangeville,  0.003: deux,  0.003: f,  0.003: aussi,  0.003: h,  0.003: depuis,  0.003: année,  0.003: cette,  0.003: école,  0.003: cours,  0.003: peu,  0.003: sans,  0.003: club,  0.003: gaby,  0.003: être,  0.002: là,  0.002: enfants\n",
      "\n",
      "=== topic #15\n",
      "0.008: rue,  0.006: nancy,  0.005: 10,  0.004: surtout,  0.004: être,  0.004: année,  0.004: a,  0.003: château,  0.003: tout,  0.003: résidence,  0.003: plus,  0.003: vacances,  0.003: jamais,  0.003: kleinmann,  0.003: domicilié,  0.003: claude,  0.003: nouveau,  0.002: après,  0.002: cette,  0.002: ils\n",
      "\n",
      "=== topic #16\n",
      "0.013: ans,  0.012: a,  0.007: mardi,  0.005: 18,  0.005: h,  0.005: ferme,  0.004: initiation,  0.004: ihsan,  0.003: catégorie,  0.003: ados,  0.003: f,  0.003: lahmar,  0.003: deux,  0.003: 5,  0.003: enfants,  0.003: spinalien,  0.003: drogue,  0.003: kg,  0.003: julien,  0.003: cette\n",
      "\n",
      "=== topic #17\n",
      "0.013: a,  0.005: ans,  0.004: deux,  0.004: ensuite,  0.003: avoir,  0.003: beaucoup,  0.003: cette,  0.003: après,  0.003: h,  0.003: village,  0.003: fait,  0.003: où,  0.002: aussi,  0.002: leurs,  0.002: poste,  0.002: grande,  0.002: adversaires,  0.002: puis,  0.002: sans,  0.002: jeu\n",
      "\n",
      "=== topic #18\n",
      "0.020: ans,  0.009: h,  0.007: 1,  0.006: tout,  0.006: a,  0.005: plus,  0.004: après,  0.004: comme,  0.004: reims,  0.004: mme,  0.004: cette,  0.003: gérardmer,  0.003: fait,  0.003: 3,  0.003: veuve,  0.003: 30,  0.003: chemin,  0.003: année,  0.003: clémenceau,  0.003: 5\n",
      "\n",
      "=== topic #19\n",
      "0.010: a,  0.006: f,  0.006: plus,  0.004: -,  0.004: aussi,  0.004: club,  0.003: ans,  0.003: si,  0.003: sans,  0.003: cette,  0.003: fait,  0.003: membres,  0.003: bonnes,  0.003: rond-point,  0.003: comme,  0.002: peu,  0.002: côté,  0.002: deux,  0.002: enfin,  0.002: photo\n",
      "\n",
      "=== topic #20\n",
      "0.022: a,  0.006: ans,  0.004: h,  0.004: deux,  0.004: ils,  0.004: premier,  0.003: plus,  0.003: hier,  0.003: fait,  0.003: peu,  0.003: terrain,  0.003: france,  0.003: faut,  0.003: autre,  0.002: aussi,  0.002: conseil,  0.002: président,  0.002: avant,  0.002: 30,  0.002: mieux\n",
      "\n",
      "=== topic #21\n",
      "0.028: h,  0.009: a,  0.007: -,  0.006: ans,  0.006: 15,  0.005: rue,  0.005: cette,  0.004: 21,  0.004: 45,  0.004: ville,  0.004: 2,  0.004: deux,  0.004: 16,  0.004: 14,  0.004: 10,  0.004: 19,  0.004: jouent,  0.003: contre,  0.003: tous,  0.003: plus\n",
      "\n",
      "=== topic #22\n",
      "0.060: h,  0.021: 30,  0.013: r,  0.011: 13,  0.009: a,  0.009: 15,  0.008: rue,  0.008: 14,  0.007: 20,  0.007: 12,  0.006: 9,  0.005: 19,  0.005: 17,  0.005: 22,  0.005: 21,  0.005: rv,  0.005: match,  0.005: dise,  0.005: 18,  0.005: place\n",
      "\n",
      "=== topic #23\n",
      "0.007: h,  0.005: association,  0.005: 13,  0.005: rdv,  0.004: 30,  0.004: p,  0.004: stade,  0.003: a,  0.003: cette,  0.003: peu,  0.003: ans,  0.003: 2,  0.003: président,  0.003: plus,  0.003: 1,  0.003: moins,  0.003: 15,  0.003: michel,  0.003: rue,  0.003: instituteurs\n",
      "\n",
      "=== topic #24\n",
      "0.005: h,  0.005: a,  0.004: après,  0.003: maires,  0.003: lundi,  0.003: juillet,  0.003: f,  0.003: quatre,  0.003: très,  0.003: depuis,  0.003: dès,  0.003: -,  0.003: maire,  0.003: atton,  0.003: projet,  0.003: corps,  0.002: général,  0.002: préfecture,  0.002: toul,  0.002: advmm\n",
      "\n",
      "=== topic #25\n",
      "0.014: a,  0.006: plus,  0.005: cette,  0.004: ville,  0.004: faire,  0.004: beaucoup,  0.004: bien,  0.004: h,  0.004: équipe,  0.004: 4,  0.004: leurs,  0.003: travail,  0.003: jours,  0.003: da,  0.003: temps,  0.003: projet,  0.003: tous,  0.003: être,  0.003: place,  0.003: peut\n",
      "\n",
      "=== topic #26\n",
      "0.014: a,  0.008: rue,  0.006: fait,  0.006: deux,  0.006: partie,  0.004: sens,  0.004: entre,  0.004: peu,  0.004: h,  0.004: vu,  0.003: après,  0.003: très,  0.003: ils,  0.003: nancy,  0.003: là,  0.003: avenue,  0.003: collège,  0.003: plus,  0.003: avoir,  0.003: comprise\n",
      "\n",
      "=== topic #27\n",
      "0.011: a,  0.010: ab,  0.007: pts,  0.005: b,  0.005: plus,  0.004: comme,  0.004: cette,  0.004: deux,  0.003: sans,  0.003: bien,  0.003: tout,  0.003: fait,  0.003: h,  0.003: france,  0.002: montbéliard,  0.002: décision,  0.002: aussi,  0.002: niveau,  0.002: nicolas,  0.002: fin\n",
      "\n",
      "=== topic #28\n",
      "0.010: a,  0.005: plus,  0.005: jour,  0.004: h,  0.004: week-end,  0.004: mois,  0.004: liberté,  0.004: air,  0.004: pilotes,  0.004: 5,  0.003: ils,  0.003: ans,  0.003: toujours,  0.003: année,  0.003: très,  0.003: selon,  0.003: direction,  0.003: syndicats,  0.003: 00,  0.003: encore\n",
      "\n",
      "=== topic #29\n",
      "0.008: a,  0.008: rue,  0.004: cette,  0.004: plus,  0.004: sans,  0.004: jusqu,  0.003: bien,  0.003: tous,  0.003: vente,  0.003: lucienne,  0.003: mitterrand,  0.003: aussi,  0.003: place,  0.003: tout,  0.003: élus,  0.003: andré,  0.003: comme,  0.003: ville,  0.003: bon,  0.003: puis\n",
      "\n",
      "=== topic #30\n",
      "0.006: bien,  0.005: a,  0.005: h,  0.004: cette,  0.004: ans,  0.003: entre,  0.003: toute,  0.003: football,  0.003: professeur,  0.003: 1994,  0.003: comme,  0.003: point,  0.003: aucun,  0.003: ainsi,  0.002: si,  0.002: plus,  0.002: pièces,  0.002: tous,  0.002: également,  0.002: 3\n",
      "\n",
      "=== topic #31\n",
      "0.021: a,  0.005: plus,  0.004: vandoeuvre,  0.004: cette,  0.004: comme,  0.003: eau,  0.003: puis,  0.003: enfants,  0.003: marly,  0.002: partie,  0.002: donner,  0.002: gauche,  0.002: jeunes,  0.002: public,  0.002: si,  0.002: 20,  0.002: fait,  0.002: sans,  0.002: très,  0.002: deux\n",
      "\n",
      "=== topic #32\n",
      "0.005: restaurant,  0.004: h,  0.004: michèle,  0.004: jour,  0.003: deux,  0.003: mois,  0.003: 20,  0.003: minutes,  0.003: peut,  0.002: gare,  0.002: plus,  0.002: nancy,  0.002: décibels,  0.002: 15,  0.002: voies,  0.002: frais,  0.002: a,  0.002: premiers,  0.002: avant,  0.002: derrière\n",
      "\n",
      "=== topic #33\n",
      "0.010: a,  0.007: plus,  0.004: ville,  0.004: très,  0.003: woippy,  0.003: fait,  0.003: compte,  0.003: ainsi,  0.003: ni,  0.003: deux,  0.003: bien,  0.003: quelques,  0.003: tout,  0.003: centre,  0.003: -,  0.003: toute,  0.002: tête,  0.002: autres,  0.002: ils,  0.002: leurs\n",
      "\n",
      "=== topic #34\n",
      "0.014: a,  0.007: beaucoup,  0.006: plus,  0.005: équipe,  0.004: depuis,  0.004: ministre,  0.004: toujours,  0.004: président,  0.004: comme,  0.003: première,  0.003: où,  0.003: fait,  0.003: deux,  0.003: lyonnais,  0.003: actuellement,  0.003: après,  0.002: faire,  0.002: entendre,  0.002: reconnaît,  0.002: raonnais\n",
      "\n",
      "=== topic #35\n",
      "0.006: a,  0.004: h,  0.004: tout,  0.004: produits,  0.003: aussi,  0.003: plus,  0.003: association,  0.003: 4,  0.003: valeur,  0.003: eau,  0.003: juin,  0.002: qualité,  0.002: chefs,  0.002: fête,  0.002: aide,  0.002: ville,  0.002: forêt,  0.002: avancement,  0.002: jour,  0.002: regarder\n",
      "\n",
      "=== topic #36\n",
      "0.007: a,  0.005: plus,  0.005: après,  0.004: 0,  0.004: deux,  0.004: avoir,  0.003: communes,  0.003: enfants,  0.003: lorraine,  0.003: 4,  0.003: être,  0.003: 00,  0.003: ville,  0.003: développement,  0.003: michel,  0.003: fait,  0.002: ans,  0.002: formation,  0.002: encore,  0.002: sébastien\n",
      "\n",
      "=== topic #37\n",
      "0.028: a,  0.008: f,  0.008: deux,  0.006: ans,  0.006: plus,  0.006: tous,  0.006: école,  0.005: comme,  0.005: ils,  0.004: enfants,  0.004: h,  0.003: prévu,  0.003: quatre,  0.003: autres,  0.003: cela,  0.002: puis,  0.002: presque,  0.002: bon,  0.002: picot,  0.002: joueurs\n",
      "\n",
      "=== topic #38\n",
      "0.010: ind,  0.010: plus,  0.010: a,  0.005: aussi,  0.005: bien,  0.005: cette,  0.004: tous,  0.004: santé,  0.004: tout,  0.004: montbéliard,  0.004: f,  0.003: deux,  0.003: pays,  0.003: comme,  0.003: autres,  0.003: courir,  0.003: leurs,  0.003: élèves,  0.003: dtat,  0.003: cgt\n",
      "\n",
      "=== topic #39\n",
      "0.011: a,  0.006: saison,  0.005: plus,  0.005: faire,  0.005: cette,  0.005: h,  0.004: tout,  0.004: -,  0.003: club,  0.003: 30,  0.003: président,  0.003: école,  0.003: dont,  0.003: septembre,  0.003: 12,  0.003: ans,  0.003: fin,  0.003: deux,  0.003: visite,  0.003: tête\n",
      "\n",
      "=== topic #40\n",
      "0.008: cette,  0.008: a,  0.007: plus,  0.007: deux,  0.005: ils,  0.004: bien,  0.004: entre,  0.004: enfants,  0.003: faire,  0.003: trois,  0.003: après,  0.003: ans,  0.003: salle,  0.003: aujourd,  0.003: alors,  0.003: voix,  0.003: h,  0.002: aussi,  0.002: si,  0.002: hui\n",
      "\n",
      "=== topic #41\n",
      "0.010: a,  0.006: très,  0.005: ils,  0.005: -,  0.003: commune,  0.003: déjà,  0.003: rue,  0.003: public,  0.003: police,  0.003: tout,  0.003: dieulouard,  0.003: place,  0.003: toul,  0.003: blénod,  0.003: vont,  0.003: assurance,  0.002: direction,  0.002: leurs,  0.002: avenants,  0.002: conservatoire\n",
      "\n",
      "=== topic #42\n",
      "0.013: h,  0.005: a,  0.005: plus,  0.004: dimanche,  0.004: tireurs,  0.004: parcours,  0.004: heures,  0.003: 18,  0.003: fait,  0.003: deux,  0.003: ans,  0.003: 6,  0.003: ils,  0.003: 30,  0.003: 17,  0.003: départ,  0.003: 9,  0.003: 20,  0.003: depuis,  0.003: 10\n",
      "\n",
      "=== topic #43\n",
      "0.034: h,  0.014: ans,  0.013: 30,  0.008: jury,  0.008: n°,  0.007: 1,  0.007: 15,  0.006: 3,  0.006: 11,  0.006: juments,  0.006: 9,  0.005: enfants,  0.005: a,  0.005: 10,  0.005: 17,  0.004: suitées,  0.004: 2,  0.004: plus,  0.003: 8,  0.003: 4\n",
      "\n",
      "=== topic #44\n",
      "0.007: a,  0.005: h,  0.005: deux,  0.004: comme,  0.004: plus,  0.004: ans,  0.004: aussi,  0.003: encore,  0.003: bien,  0.003: ecole,  0.003: là,  0.003: 3,  0.003: ils,  0.003: temps,  0.003: 45,  0.003: peut,  0.003: place,  0.003: 2,  0.003: hier,  0.003: groupe\n",
      "\n",
      "=== topic #45\n",
      "0.011: a,  0.007: ils,  0.005: fois,  0.005: très,  0.005: bien,  0.005: ligne,  0.005: plus,  0.004: sans,  0.004: équipe,  0.003: fait,  0.003: première,  0.003: leurs,  0.003: deux,  0.003: château,  0.003: depuis,  0.003: cette,  0.003: france,  0.003: salm,  0.003: curieusement,  0.003: où\n",
      "\n",
      "=== topic #46\n",
      "0.008: a,  0.006: 000,  0.004: cette,  0.004: commune,  0.004: villerot,  0.004: projet,  0.003: comme,  0.003: presbytère,  0.003: mission,  0.003: trois,  0.003: plus,  0.003: 7,  0.003: si,  0.003: septembre,  0.003: fait,  0.003: henri,  0.003: 2,  0.003: 3,  0.003: faut,  0.003: opération\n",
      "\n",
      "=== topic #47\n",
      "0.010: 2,  0.010: h,  0.008: a,  0.006: 1,  0.006: classes,  0.005: septembre,  0.005: bp,  0.004: rentrée,  0.004: tout,  0.004: 6,  0.004: aussi,  0.003: sans,  0.003: celui,  0.003: 7,  0.003: 9,  0.003: cette,  0.003: lundi,  0.003: 30,  0.003: jeune,  0.002: deux\n",
      "\n",
      "=== topic #48\n",
      "0.015: a,  0.007: plus,  0.005: rue,  0.005: comme,  0.005: ans,  0.004: trois,  0.004: cette,  0.004: mme,  0.003: après,  0.003: aussi,  0.003: premier,  0.003: première,  0.003: soeurs,  0.003: km,  0.003: tous,  0.003: dont,  0.002: temps,  0.002: deux,  0.002: stationnement,  0.002: -\n",
      "\n",
      "=== topic #49\n",
      "0.011: a,  0.008: h,  0.006: dimanche,  0.005: ans,  0.005: décembre,  0.005: plus,  0.004: samedi,  0.003: comme,  0.003: faire,  0.003: octobre,  0.003: francs,  0.003: marché,  0.003: cette,  0.003: depuis,  0.003: noël,  0.002: nom,  0.002: vie,  0.002: certains,  0.002: deux,  0.002: 12\n",
      "\n",
      "=== topic #50\n",
      "0.025: -,  0.019: a,  0.007: plus,  0.006: direction,  0.006: cette,  0.004: deux,  0.004: rue,  0.004: coupe,  0.004: public,  0.004: tous,  0.003: kg,  0.003: centre,  0.003: 3,  0.003: vers,  0.003: ans,  0.003: fait,  0.003: 6,  0.003: cs,  0.003: dernier,  0.002: bien\n",
      "\n",
      "=== topic #51\n",
      "0.010: a,  0.007: née,  0.005: plus,  0.005: h,  0.005: car,  0.004: f,  0.004: plan,  0.004: polaincourt,  0.003: chasse,  0.003: teddy,  0.003: après,  0.003: cette,  0.003: deux,  0.003: centre,  0.003: jeunes,  0.002: prix,  0.002: entre,  0.002: septembre,  0.002: sangliers,  0.002: an\n",
      "\n",
      "=== topic #52\n",
      "0.011: a,  0.008: f,  0.005: comme,  0.005: lorraine,  0.004: ils,  0.004: aussi,  0.004: cette,  0.004: où,  0.003: sport,  0.003: bien,  0.003: entreprise,  0.003: ans,  0.003: secrétaire,  0.003: très,  0.003: si,  0.003: ligue,  0.003: peut,  0.003: période,  0.003: france,  0.002: fait\n",
      "\n",
      "=== topic #53\n",
      "0.045: h,  0.014: 1,  0.010: a,  0.007: rue,  0.006: 30,  0.006: 40,  0.006: 14,  0.005: place,  0.005: 13,  0.005: 25,  0.005: 19,  0.004: tous,  0.004: 22,  0.004: deux,  0.004: 35,  0.004: filles,  0.004: 15,  0.003: 18,  0.003: ans,  0.003: 27\n",
      "\n",
      "=== topic #54\n",
      "0.015: h,  0.009: 30,  0.006: 1,  0.005: a,  0.005: remiremont,  0.004: 18,  0.004: 15,  0.004: rosières-aux-salines,  0.004: gym,  0.004: chevaux,  0.004: dimanche,  0.003: temps,  0.003: 9,  0.003: ans,  0.003: deux,  0.003: 3,  0.003: 20,  0.003: plus,  0.003: moindre,  0.003: bien\n",
      "\n",
      "=== topic #55\n",
      "0.022: a,  0.007: h,  0.007: ans,  0.007: plus,  0.006: fait,  0.004: où,  0.004: sous,  0.003: comme,  0.003: déjà,  0.003: -,  0.003: depuis,  0.003: association,  0.003: 30,  0.003: jusqu,  0.003: temps,  0.003: toute,  0.003: samedi,  0.002: strasbourg,  0.002: autres,  0.002: foyer\n",
      "\n",
      "=== topic #56\n",
      "0.009: a,  0.007: plus,  0.007: ils,  0.004: si,  0.004: cette,  0.004: h,  0.004: très,  0.003: tout,  0.003: peu,  0.003: saône,  0.003: où,  0.002: 13,  0.002: faut,  0.002: souvent,  0.002: hier,  0.002: peut,  0.002: fait,  0.002: municipale,  0.002: -,  0.002: autres\n",
      "\n",
      "=== topic #57\n",
      "0.033: a,  0.006: plus,  0.004: après,  0.004: tout,  0.003: aussi,  0.003: deux,  0.003: cette,  0.003: concours,  0.003: sans,  0.003: bien,  0.003: maire,  0.003: fait,  0.003: être,  0.003: jeunes,  0.002: comme,  0.002: souligne,  0.002: fois,  0.002: encore,  0.002: jean,  0.002: dernier\n",
      "\n",
      "=== topic #58\n",
      "0.009: a,  0.005: plus,  0.003: comme,  0.003: travail,  0.003: cela,  0.003: chien,  0.003: norman,  0.003: fait,  0.003: gendarmerie,  0.003: brigade,  0.003: école,  0.003: aussi,  0.003: entre,  0.002: cette,  0.002: encore,  0.002: pourtant,  0.002: procureur,  0.002: si,  0.002: aucune,  0.002: stups\n",
      "\n",
      "=== topic #59\n",
      "0.010: a,  0.009: plus,  0.008: f,  0.004: boues,  0.004: fait,  0.004: deux,  0.004: tout,  0.004: 5,  0.003: après,  0.003: ttc,  0.003: va,  0.003: autres,  0.003: ans,  0.003: place,  0.003: gauche,  0.003: route,  0.003: simone,  0.003: entre,  0.003: septembre,  0.003: station\n",
      "\n",
      "=== topic #60\n",
      "0.013: a,  0.009: plus,  0.006: rue,  0.005: place,  0.005: ils,  0.004: moins,  0.003: entre,  0.003: victoire,  0.003: 3,  0.003: communauté,  0.003: moyenmoutier,  0.003: coup,  0.003: prix,  0.003: marché,  0.003: h,  0.003: encore,  0.002: habitants,  0.002: déjà,  0.002: 4,  0.002: avenue\n",
      "\n",
      "=== topic #61\n",
      "0.022: ind,  0.013: a,  0.006: f,  0.004: p,  0.003: parc,  0.003: agriculture,  0.003: h,  0.003: bio,  0.002: 000,  0.002: animations,  0.002: requiert,  0.002: policier,  0.002: asptt,  0.002: substitut,  0.002: ans,  0.002: bien,  0.002: vins,  0.002: partie,  0.002: pts,  0.002: deux\n",
      "\n",
      "=== topic #62\n",
      "0.008: a,  0.006: ans,  0.005: 2,  0.004: juillet,  0.004: août,  0.004: mathieu,  0.004: date,  0.004: plus,  0.003: vendredi,  0.003: tout,  0.003: limite,  0.003: 23,  0.003: cette,  0.003: emilie,  0.003: julien,  0.003: comme,  0.003: 6,  0.003: station,  0.003: jusqu,  0.003: h\n",
      "\n",
      "=== topic #63\n",
      "0.018: a,  0.009: ind,  0.007: lorraine,  0.006: rue,  0.005: fait,  0.005: 00,  0.005: plus,  0.004: 0,  0.004: où,  0.003: bien,  0.003: 4,  0.003: boucherie,  0.003: place,  0.003: pierrat,  0.002: faut,  0.002: quatre,  0.002: public,  0.002: dont,  0.002: vtt,  0.002: puis\n",
      "\n",
      "=== topic #64\n",
      "0.010: h,  0.009: ans,  0.007: a,  0.005: deux,  0.005: centre,  0.004: plus,  0.004: enfants,  0.004: leurs,  0.003: cette,  0.003: ils,  0.003: public,  0.003: 18,  0.003: si,  0.003: sous,  0.003: 30,  0.003: autres,  0.003: après,  0.003: 20,  0.003: tous,  0.003: club\n",
      "\n",
      "=== topic #65\n",
      "0.013: f,  0.013: mp,  0.012: a,  0.006: cette,  0.005: plus,  0.005: comme,  0.004: ça,  0.004: après,  0.004: très,  0.003: cet,  0.003: tous,  0.003: ans,  0.003: jours,  0.002: également,  0.002: km,  0.002: tout,  0.002: quelques,  0.002: vers,  0.002: ensemble,  0.002: si\n",
      "\n",
      "=== topic #66\n",
      "0.013: a,  0.006: plus,  0.004: tout,  0.004: deux,  0.003: france,  0.003: fait,  0.003: beaucoup,  0.003: elles,  0.003: ans,  0.003: comme,  0.003: leurs,  0.003: 000,  0.003: année,  0.003: autorisation,  0.003: ils,  0.003: rue,  0.002: fin,  0.002: 5,  0.002: déjà,  0.002: également\n",
      "\n",
      "=== topic #67\n",
      "0.012: bat,  0.011: 6-3,  0.009: a,  0.006: tout,  0.006: plus,  0.005: 6-2,  0.005: usa,  0.005: 6-4,  0.005: f,  0.004: prix,  0.004: fait,  0.004: 14,  0.004: président,  0.004: fra,  0.004: ans,  0.003: bien,  0.003: 7,  0.003: club,  0.003: équipe,  0.003: tous\n",
      "\n",
      "=== topic #68\n",
      "0.007: a,  0.006: deux,  0.005: nancy,  0.005: cette,  0.005: ans,  0.004: ils,  0.003: plus,  0.003: val,  0.003: gars,  0.003: cross,  0.003: aussi,  0.003: trois,  0.002: après,  0.002: tout,  0.002: laneuveville,  0.002: blénod,  0.002: case,  0.002: visite,  0.002: déjà,  0.002: essey-lès-nancy\n",
      "\n",
      "=== topic #69\n",
      "0.012: a,  0.011: maîche,  0.007: charquemont,  0.007: plus,  0.005: année,  0.004: deux,  0.004: élèves,  0.003: école,  0.003: scolaire,  0.003: cette,  0.003: également,  0.003: municipalité,  0.003: locaux,  0.003: robert,  0.003: comme,  0.003: jean,  0.003: leurs,  0.003: maire,  0.003: création,  0.003: trévillers\n",
      "\n",
      "=== topic #70\n",
      "0.008: a,  0.004: bazaud,  0.004: frantz,  0.004: encore,  0.004: place,  0.003: devant,  0.003: être,  0.003: quadrette,  0.003: boccard,  0.003: nouvelle,  0.003: chamberod,  0.003: équipe,  0.003: guignol,  0.003: 3,  0.003: hier,  0.003: saison,  0.003: plus,  0.002: général,  0.002: ils,  0.002: vittelloise\n",
      "\n",
      "=== topic #71\n",
      "0.014: a,  0.004: fait,  0.004: plus,  0.004: si,  0.004: dopage,  0.003: deux,  0.003: comme,  0.003: leblanc,  0.003: brc,  0.003: premier,  0.003: francs,  0.003: faut,  0.003: lieu,  0.003: avant,  0.003: 000,  0.003: france,  0.003: faire,  0.002: voitures,  0.002: emplacement,  0.002: large\n",
      "\n",
      "=== topic #72\n",
      "0.012: a,  0.006: rue,  0.004: comme,  0.004: tout,  0.003: école,  0.003: ans,  0.003: enfants,  0.003: fait,  0.003: route,  0.003: sans,  0.003: plus,  0.002: vie,  0.002: direction,  0.002: agent,  0.002: club,  0.002: michèle,  0.002: où,  0.002: service,  0.002: bien,  0.002: depuis\n",
      "\n",
      "=== topic #73\n",
      "0.016: h,  0.011: a,  0.006: deux,  0.006: 30,  0.004: 14,  0.004: personnes,  0.004: ans,  0.003: plus,  0.003: 2,  0.003: 17,  0.003: trois,  0.003: vacances,  0.003: travail,  0.003: sans,  0.003: leurs,  0.003: hier,  0.003: lieu,  0.002: après,  0.002: aussi,  0.002: malade\n",
      "\n",
      "=== topic #74\n",
      "0.039: h,  0.020: 4,  0.013: 9,  0.011: 7,  0.010: 11,  0.009: 8,  0.008: a,  0.007: 14,  0.007: 2,  0.007: 10,  0.006: 1er,  0.006: 13,  0.006: 1,  0.006: 30,  0.006: r,  0.006: 6,  0.005: 31,  0.005: 15,  0.005: 17,  0.004: 5\n",
      "\n",
      "=== topic #75\n",
      "0.011: a,  0.004: être,  0.004: tout,  0.003: ils,  0.003: perrin,  0.003: face,  0.003: deux,  0.003: plus,  0.003: projet,  0.003: françois,  0.003: autres,  0.003: notamment,  0.002: trois,  0.002: première,  0.002: moins,  0.002: venus,  0.002: si,  0.002: comtois,  0.002: lure,  0.002: comme\n",
      "\n",
      "=== topic #76\n",
      "0.013: a,  0.007: plus,  0.005: comme,  0.005: jeunes,  0.004: lieu,  0.004: cette,  0.004: pris,  0.004: tous,  0.003: ans,  0.003: sans,  0.003: leurs,  0.003: h,  0.003: fait,  0.003: 30,  0.003: cet,  0.003: farce,  0.003: escadron,  0.003: seulement,  0.003: deux,  0.003: vraie\n",
      "\n",
      "=== topic #77\n",
      "0.032: a,  0.008: plus,  0.006: cette,  0.004: bien,  0.004: premier,  0.004: leurs,  0.004: ans,  0.003: trois,  0.003: semaine,  0.003: tout,  0.003: encore,  0.003: après,  0.003: année,  0.003: jeune,  0.003: entre,  0.003: autres,  0.003: va,  0.003: tête,  0.003: fait,  0.003: deux\n",
      "\n",
      "=== topic #78\n",
      "0.012: a,  0.007: asptt,  0.007: 3,  0.005: 13,  0.005: 02,  0.005: encore,  0.005: nancy,  0.005: depuis,  0.004: cette,  0.004: plus,  0.004: première,  0.004: aussi,  0.003: bien,  0.003: ans,  0.003: sc,  0.003: sarreguemines,  0.003: 5,  0.003: vc,  0.003: deux,  0.003: enfants\n",
      "\n",
      "=== topic #79\n",
      "0.007: 5,  0.004: dan,  0.004: pendant,  0.004: ans,  0.004: a,  0.004: puis,  0.003: sans,  0.003: orchestre,  0.003: bals,  0.003: plus,  0.003: gil,  0.003: 15,  0.003: chante,  0.003: millions,  0.003: organisateurs,  0.003: 6,  0.003: trois,  0.003: villers,  0.003: dimanche,  0.002: bat\n",
      "\n",
      "=== topic #80\n",
      "0.021: f,  0.013: 500,  0.011: a,  0.007: 4,  0.007: 000,  0.005: 1,  0.004: 3,  0.004: deux,  0.004: 5,  0.003: hubsch,  0.003: vezouze,  0.003: ils,  0.003: h,  0.003: amicale,  0.002: autres,  0.002: soir,  0.002: sous,  0.002: plus,  0.002: ind,  0.002: sans\n",
      "\n",
      "=== topic #81\n",
      "0.010: a,  0.009: 7,  0.007: vc,  0.006: inscription,  0.005: stéphanois,  0.005: ec,  0.005: vendredi,  0.005: limite,  0.005: g,  0.004: 9,  0.004: date,  0.004: plus,  0.004: 8,  0.004: être,  0.003: jec,  0.003: asge,  0.003: spinalien,  0.003: st-dié,  0.003: 13,  0.003: 16\n",
      "\n",
      "=== topic #82\n",
      "0.014: a,  0.004: robert,  0.004: où,  0.003: fabrer,  0.003: tout,  0.003: encore,  0.003: monde,  0.003: depuis,  0.003: niveau,  0.003: guerre,  0.003: deux,  0.003: est-il,  0.003: plus,  0.003: après,  0.003: ils,  0.003: président,  0.002: autre,  0.002: nuit,  0.002: avant,  0.002: ans\n",
      "\n",
      "=== topic #83\n",
      "0.011: a,  0.005: ans,  0.004: blés,  0.004: deux,  0.004: origine,  0.004: faire,  0.003: connu,  0.003: très,  0.003: pouliche,  0.003: chevaux,  0.003: entre,  0.003: haras,  0.003: jury,  0.003: ainsi,  0.003: quatre,  0.003: cirey-sur-vezouze,  0.003: millions,  0.003: saut,  0.002: vélo,  0.002: francs\n",
      "\n",
      "=== topic #84\n",
      "0.026: a,  0.005: plus,  0.005: tout,  0.004: deux,  0.004: depuis,  0.004: h,  0.004: cette,  0.003: ans,  0.003: fait,  0.003: jours,  0.003: quelques,  0.003: club,  0.003: place,  0.003: premier,  0.003: tous,  0.002: ainsi,  0.002: bien,  0.002: après,  0.002: local,  0.002: saison\n",
      "\n",
      "=== topic #85\n",
      "0.018: a,  0.005: plus,  0.005: depuis,  0.005: fait,  0.004: conseil,  0.004: cours,  0.004: ceux,  0.004: encore,  0.003: demande,  0.003: donc,  0.003: bruyères,  0.003: leurs,  0.003: travaux,  0.003: élus,  0.003: deux,  0.003: plage,  0.002: tout,  0.002: logement,  0.002: côté,  0.002: auprès\n",
      "\n",
      "=== topic #86\n",
      "0.014: a,  0.005: comme,  0.005: tout,  0.005: être,  0.004: deux,  0.004: cette,  0.004: encore,  0.004: bien,  0.003: sans,  0.003: avant,  0.003: fait,  0.003: espace,  0.002: abord,  0.002: après,  0.002: sécurité,  0.002: non,  0.002: ans,  0.002: toujours,  0.002: plus,  0.002: monde\n",
      "\n",
      "=== topic #87\n",
      "0.012: rue,  0.007: a,  0.005: h,  0.004: activités,  0.004: tout,  0.003: plus,  0.003: entre,  0.003: place,  0.003: philippe,  0.003: durant,  0.002: voie,  0.002: 6,  0.002: aussi,  0.002: question,  0.002: carte,  0.002: deux,  0.002: 2,  0.002: alstom,  0.002: ils,  0.002: ans\n",
      "\n",
      "=== topic #88\n",
      "0.014: a,  0.007: plus,  0.004: ans,  0.004: jeunes,  0.003: belfort,  0.003: aussi,  0.003: bien,  0.003: sous,  0.003: p,  0.003: deux,  0.003: cette,  0.003: encore,  0.002: club,  0.002: nature,  0.002: heillecourt,  0.002: suite,  0.002: cité,  0.002: forme,  0.002: nancy,  0.002: peu\n",
      "\n",
      "=== topic #89\n",
      "0.007: cette,  0.007: a,  0.005: très,  0.004: comme,  0.003: où,  0.003: faire,  0.003: grande,  0.003: toutes,  0.003: pourquoi,  0.003: an,  0.002: ordures,  0.002: semaine,  0.002: pays,  0.002: être,  0.002: vie,  0.002: peut,  0.002: châteaux,  0.002: tout,  0.002: chapiteau,  0.002: foucherans\n",
      "\n",
      "=== topic #90\n",
      "0.011: a,  0.007: 6,  0.006: plus,  0.006: tout,  0.006: h,  0.005: ans,  0.005: où,  0.005: 5,  0.004: 15,  0.004: deux,  0.003: encore,  0.003: bat,  0.003: ludres,  0.003: depuis,  0.003: fait,  0.003: si,  0.003: retraite,  0.002: moulin,  0.002: car,  0.002: là\n",
      "\n",
      "=== topic #91\n",
      "0.009: a,  0.005: professeur,  0.005: grand,  0.004: fait,  0.004: également,  0.004: école,  0.004: peut,  0.004: nancy,  0.004: livre,  0.003: réussite,  0.003: deux,  0.003: tout,  0.003: plus,  0.003: jean,  0.003: -,  0.003: ans,  0.003: docteur,  0.003: h,  0.003: médecins,  0.003: schmitt\n",
      "\n",
      "=== topic #92\n",
      "0.010: a,  0.005: comme,  0.005: cette,  0.005: centre,  0.004: née,  0.004: francas,  0.004: plus,  0.003: très,  0.003: faire,  0.003: personnes,  0.003: pu,  0.003: équipe,  0.003: neuves-maisons,  0.003: si,  0.002: deux,  0.002: chez,  0.002: être,  0.002: fait,  0.002: france,  0.002: septembre\n",
      "\n",
      "=== topic #93\n",
      "0.021: ab,  0.010: a,  0.007: b,  0.004: sébastien,  0.004: elles,  0.004: cédric,  0.004: être,  0.004: bien,  0.003: centre,  0.003: plus,  0.003: romain,  0.003: ans,  0.003: grande,  0.003: ça,  0.002: -,  0.002: damien,  0.002: 2,  0.002: surtout,  0.002: dauphines,  0.002: commission\n",
      "\n",
      "=== topic #94\n",
      "0.011: a,  0.008: h,  0.006: f,  0.005: club,  0.004: ils,  0.004: président,  0.004: année,  0.004: ans,  0.004: organisation,  0.003: an,  0.003: années,  0.003: cercle,  0.003: entraînements,  0.003: tout,  0.003: 8,  0.003: très,  0.002: voiture,  0.002: 2,  0.002: parcours,  0.002: équipes\n",
      "\n",
      "=== topic #95\n",
      "0.025: h,  0.012: -,  0.010: a,  0.009: samedi,  0.008: 18,  0.008: dimanche,  0.007: 14,  0.006: 10,  0.004: jeunes,  0.004: ouv,  0.004: mh,  0.004: vg,  0.004: jarville,  0.003: oe,  0.003: xvie,  0.003: tous,  0.003: xviiie,  0.003: club,  0.003: association,  0.003: art\n",
      "\n",
      "=== topic #96\n",
      "0.010: a,  0.007: plus,  0.003: équipe,  0.003: deux,  0.003: el,  0.003: entre,  0.003: jeunes,  0.003: sans,  0.003: -,  0.003: hier,  0.003: petits,  0.003: tous,  0.003: e,  0.002: nuit,  0.002: comme,  0.002: co,  0.002: h,  0.002: hassani,  0.002: depuis,  0.002: ans\n",
      "\n",
      "=== topic #97\n",
      "0.007: a,  0.005: ans,  0.004: niveau,  0.004: daniel,  0.004: diplôme,  0.004: pierre,  0.004: prix,  0.004: paul,  0.003: professionnelle,  0.003: titre,  0.003: 133,  0.003: travail,  0.003: mini-camp,  0.003: troisième,  0.003: robert,  0.002: années,  0.002: juillet,  0.002: 168,  0.002: titulaire,  0.002: licence\n",
      "\n",
      "=== topic #98\n",
      "0.016: h,  0.009: a,  0.006: cette,  0.004: bien,  0.004: 30,  0.004: faire,  0.004: tous,  0.004: 15,  0.003: 17,  0.003: dimanche,  0.003: jours,  0.003: 10,  0.003: tout,  0.003: aînés,  0.003: grâce,  0.002: ils,  0.002: deux,  0.002: -,  0.002: 14,  0.002: jour\n",
      "\n",
      "=== topic #99\n",
      "0.018: a,  0.007: plus,  0.004: ans,  0.004: cette,  0.004: comme,  0.003: tout,  0.003: ils,  0.003: dont,  0.003: beaucoup,  0.003: faut,  0.003: mairie,  0.002: salle,  0.002: jours,  0.002: trois,  0.002: club,  0.002: si,  0.002: h,  0.002: très,  0.002: quelques,  0.002: tour\n"
     ]
    }
   ],
   "source": [
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False, num_words=20):\n",
    "    print(\"\\n=== topic #{}\".format(t[0]))\n",
    "    print(t[1].replace('*', ': ').replace(' +',', ').replace('\"',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "?lda.show_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
