{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "a=2+2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "test = 1\n",
    "test2 = 2\n",
    "print(test+test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'panda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f74b2037b3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpanda\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'estrepublicain_annee_1999.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'panda'"
     ]
    }
   ],
   "source": [
    "import panda as pd\n",
    "DATA_PATH='../data'\n",
    "filename= 'estrepublicain_annee_1999.csv'\n",
    "df=pd.read_csv(DATA_PATH+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'panda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f74b2037b3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpanda\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'estrepublicain_annee_1999.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'panda'"
     ]
    }
   ],
   "source": [
    "import panda as pd\n",
    "DATA_PATH='../data'\n",
    "filename= 'estrepublicain_annee_1999.csv'\n",
    "df=pd.read_csv(DATA_PATH+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../dataestrepublicain_annee_1999.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8739a92c4b93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'estrepublicain_annee_1999.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../dataestrepublicain_annee_1999.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATA_PATH='../data'\n",
    "filename= 'estrepublicain_annee_1999.csv'\n",
    "df=pd.read_csv(DATA_PATH+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../data/estrepublicain_annee_1999.csvestrepublicain_annee_1999.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2ea3c88bc997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/estrepublicain_annee_1999.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'estrepublicain_annee_1999.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../data/estrepublicain_annee_1999.csvestrepublicain_annee_1999.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATA_PATH='../data/estrepublicain_annee_1999.csv'\n",
    "filename= 'estrepublicain_annee_1999.csv'\n",
    "df=pd.read_csv(DATA_PATH+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATA_PATH='../data/'\n",
    "filename= 'estrepublicain_annee_1999.csv'\n",
    "df=pd.read_csv(DATA_PATH+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>André Bauer, le Bonhomme de St-Dié ; Alain Dag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Smash Entente Club de Lunéville (SECL) a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En tout cas, du côté du PS Dole qui reste en c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>« Le nombre des donneurs était en légère baiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELOYES._ Les Ramoncenais n'auront tenu qu'une ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Heureusement, l'affaire ne relève en rien du t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Plus d'un Romarimontain sur deux, sans doute, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BESANÇON._ « C'est non ». La voix de Pierre-Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Une cinquantaine d'enfants de CE2, CM1 et CM2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Premiers à se lancer dans la course, les vétér...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L'assemblée générale de Tricot Couture Service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Aux HLM, en revanche, on a le sourire. Les prê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Le début du mois a vu le départ d'un groupe d'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Les risques ? « Comme tout ce qui vole, les mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>« Je comprends l'impatience des supporters qui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Annie Humbert a remercié l'ensemble des associ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HDL est une association au service des collect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Pendant que les diplomates s'écharpent en Macé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>L'équipe B disputera un match amical, le mardi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>En 1990, un programme expérimental d'hygiène b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>« Cette saison 98-99 a été l'année des jeunes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>OPPOSITION. Pour l'instant, le SLUC n'a rencon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Désireux de prendre sa revanche avec Dame coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>« Cette opération au Kosovo n'est pas une acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Le contact se fait tout naturellement. Assis e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Dans notre édition de lundi, il était fait éta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>- Je ne sais pas mais c'est quelqu'un qui a un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Avec l'Office du tourisme. De 11 h à 19 h, exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Tournier en vitrine. Les collégiens qui passen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Le Franch Country festival a commencé sous d'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30211</th>\n",
       "      <td>« Un jour, je suis monté dans mon grenier et j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30212</th>\n",
       "      <td>Ce n'est pas l'art qui imite la nature mais to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30213</th>\n",
       "      <td>Seule civette de Meuse, une des rares en Lorra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30214</th>\n",
       "      <td>La rétrospective s'ouvre par une série de tabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30215</th>\n",
       "      <td>Paradoxalement, les meilleures conditions de v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30216</th>\n",
       "      <td>S3 : 1. P. Billod (TCVB) 42 pts ; 2. R. Peter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30217</th>\n",
       "      <td>Ressuscitées depuis peu par le Kiwanis club de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30218</th>\n",
       "      <td>La première touche de la palette a été symboli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30219</th>\n",
       "      <td>L'association sportive des sourds de Sochaux-M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30220</th>\n",
       "      <td>En d'autres temps, semblable flambée des cours...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30221</th>\n",
       "      <td>Eddy Merckx, qui a inauguré mercredi soir le v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30222</th>\n",
       "      <td>En classe, il y avait étalés sur les tables le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30223</th>\n",
       "      <td>Raon passait alors un sale quart d'heure et on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30224</th>\n",
       "      <td>Le classement : 1. A. Dolguikh (SC Sarreguemin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30225</th>\n",
       "      <td>Gugnécourt, 16 h, hier après-midi : « T'as tou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30226</th>\n",
       "      <td>Hier matin, Guy Souhait, accompagné par Evelyn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30227</th>\n",
       "      <td>2481. Rigollot j. (Ind) ; 2482. Martinez y. (I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30228</th>\n",
       "      <td>Avant le grand départ pour les vacances, les q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30229</th>\n",
       "      <td>Avec un taux de participation de 49,42 % au sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30230</th>\n",
       "      <td>Des chiffres : les fêtes artisanales et artist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30231</th>\n",
       "      <td>Les disciples d'Asllepios, dieu mythologique g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30232</th>\n",
       "      <td>Ils ont venus avec leurs médailles, leurs drap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30233</th>\n",
       "      <td>Celui du mois d'août a connu un certain privil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30234</th>\n",
       "      <td>Punch Nancy bat Saint-Mihiel 15-12, 15-7 : Les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30235</th>\n",
       "      <td>La plupart des partants, pompiers professionne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30236</th>\n",
       "      <td>Et le rêve américain ne l'a plus quitté. « A m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30237</th>\n",
       "      <td>Deux mois après l'incendie du tunnel du Mont-B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30238</th>\n",
       "      <td>Cartons rouges aux installations sportives.- L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30239</th>\n",
       "      <td>81. Knapek (UST ADEGEM) 56'10 ; 82. Filliot (A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30240</th>\n",
       "      <td>Né à Eloyes, en 1908, issu d'une famille impla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30241 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      André Bauer, le Bonhomme de St-Dié ; Alain Dag...\n",
       "1      Le Smash Entente Club de Lunéville (SECL) a re...\n",
       "2      En tout cas, du côté du PS Dole qui reste en c...\n",
       "3      « Le nombre des donneurs était en légère baiss...\n",
       "4      ELOYES._ Les Ramoncenais n'auront tenu qu'une ...\n",
       "5      Heureusement, l'affaire ne relève en rien du t...\n",
       "6      Plus d'un Romarimontain sur deux, sans doute, ...\n",
       "7      BESANÇON._ « C'est non ». La voix de Pierre-Al...\n",
       "8      Une cinquantaine d'enfants de CE2, CM1 et CM2 ...\n",
       "9      Premiers à se lancer dans la course, les vétér...\n",
       "10     L'assemblée générale de Tricot Couture Service...\n",
       "11     Aux HLM, en revanche, on a le sourire. Les prê...\n",
       "12     Le début du mois a vu le départ d'un groupe d'...\n",
       "13     Les risques ? « Comme tout ce qui vole, les mo...\n",
       "14     « Je comprends l'impatience des supporters qui...\n",
       "15     Annie Humbert a remercié l'ensemble des associ...\n",
       "16     HDL est une association au service des collect...\n",
       "17     Pendant que les diplomates s'écharpent en Macé...\n",
       "18     L'équipe B disputera un match amical, le mardi...\n",
       "19     En 1990, un programme expérimental d'hygiène b...\n",
       "20     « Cette saison 98-99 a été l'année des jeunes ...\n",
       "21     OPPOSITION. Pour l'instant, le SLUC n'a rencon...\n",
       "22     Désireux de prendre sa revanche avec Dame coup...\n",
       "23     « Cette opération au Kosovo n'est pas une acti...\n",
       "24     Le contact se fait tout naturellement. Assis e...\n",
       "25     Dans notre édition de lundi, il était fait éta...\n",
       "26     - Je ne sais pas mais c'est quelqu'un qui a un...\n",
       "27     Avec l'Office du tourisme. De 11 h à 19 h, exc...\n",
       "28     Tournier en vitrine. Les collégiens qui passen...\n",
       "29     Le Franch Country festival a commencé sous d'e...\n",
       "...                                                  ...\n",
       "30211  « Un jour, je suis monté dans mon grenier et j...\n",
       "30212  Ce n'est pas l'art qui imite la nature mais to...\n",
       "30213  Seule civette de Meuse, une des rares en Lorra...\n",
       "30214  La rétrospective s'ouvre par une série de tabl...\n",
       "30215  Paradoxalement, les meilleures conditions de v...\n",
       "30216  S3 : 1. P. Billod (TCVB) 42 pts ; 2. R. Peter ...\n",
       "30217  Ressuscitées depuis peu par le Kiwanis club de...\n",
       "30218  La première touche de la palette a été symboli...\n",
       "30219  L'association sportive des sourds de Sochaux-M...\n",
       "30220  En d'autres temps, semblable flambée des cours...\n",
       "30221  Eddy Merckx, qui a inauguré mercredi soir le v...\n",
       "30222  En classe, il y avait étalés sur les tables le...\n",
       "30223  Raon passait alors un sale quart d'heure et on...\n",
       "30224  Le classement : 1. A. Dolguikh (SC Sarreguemin...\n",
       "30225  Gugnécourt, 16 h, hier après-midi : « T'as tou...\n",
       "30226  Hier matin, Guy Souhait, accompagné par Evelyn...\n",
       "30227  2481. Rigollot j. (Ind) ; 2482. Martinez y. (I...\n",
       "30228  Avant le grand départ pour les vacances, les q...\n",
       "30229  Avec un taux de participation de 49,42 % au sc...\n",
       "30230  Des chiffres : les fêtes artisanales et artist...\n",
       "30231  Les disciples d'Asllepios, dieu mythologique g...\n",
       "30232  Ils ont venus avec leurs médailles, leurs drap...\n",
       "30233  Celui du mois d'août a connu un certain privil...\n",
       "30234  Punch Nancy bat Saint-Mihiel 15-12, 15-7 : Les...\n",
       "30235  La plupart des partants, pompiers professionne...\n",
       "30236  Et le rêve américain ne l'a plus quitté. « A m...\n",
       "30237  Deux mois après l'incendie du tunnel du Mont-B...\n",
       "30238  Cartons rouges aux installations sportives.- L...\n",
       "30239  81. Knapek (UST ADEGEM) 56'10 ; 82. Filliot (A...\n",
       "30240  Né à Eloyes, en 1908, issu d'une famille impla...\n",
       "\n",
       "[30241 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30241, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30241, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paragraphe'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.head()\n",
    "df.columns\n",
    "df.columns = ['paragraphe']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paragraphe'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                               paragraphe\n",
       "0      André Bauer, le Bonhomme de St-Dié ; Alain Dag...\n",
       "1      Le Smash Entente Club de Lunéville (SECL) a re...\n",
       "2      En tout cas, du côté du PS Dole qui reste en c...\n",
       "3      « Le nombre des donneurs était en légère baiss...\n",
       "4      ELOYES._ Les Ramoncenais n'auront tenu qu'une ...\n",
       "5      Heureusement, l'affaire ne relève en rien du t...\n",
       "6      Plus d'un Romarimontain sur deux, sans doute, ...\n",
       "7      BESANÇON._ « C'est non ». La voix de Pierre-Al...\n",
       "8      Une cinquantaine d'enfants de CE2, CM1 et CM2 ...\n",
       "9      Premiers à se lancer dans la course, les vétér...\n",
       "10     L'assemblée générale de Tricot Couture Service...\n",
       "11     Aux HLM, en revanche, on a le sourire. Les prê...\n",
       "12     Le début du mois a vu le départ d'un groupe d'...\n",
       "13     Les risques ? « Comme tout ce qui vole, les mo...\n",
       "14     « Je comprends l'impatience des supporters qui...\n",
       "15     Annie Humbert a remercié l'ensemble des associ...\n",
       "16     HDL est une association au service des collect...\n",
       "17     Pendant que les diplomates s'écharpent en Macé...\n",
       "18     L'équipe B disputera un match amical, le mardi...\n",
       "19     En 1990, un programme expérimental d'hygiène b...\n",
       "20     « Cette saison 98-99 a été l'année des jeunes ...\n",
       "21     OPPOSITION. Pour l'instant, le SLUC n'a rencon...\n",
       "22     Désireux de prendre sa revanche avec Dame coup...\n",
       "23     « Cette opération au Kosovo n'est pas une acti...\n",
       "24     Le contact se fait tout naturellement. Assis e...\n",
       "25     Dans notre édition de lundi, il était fait éta...\n",
       "26     - Je ne sais pas mais c'est quelqu'un qui a un...\n",
       "27     Avec l'Office du tourisme. De 11 h à 19 h, exc...\n",
       "28     Tournier en vitrine. Les collégiens qui passen...\n",
       "29     Le Franch Country festival a commencé sous d'e...\n",
       "...                                                  ...\n",
       "30211  « Un jour, je suis monté dans mon grenier et j...\n",
       "30212  Ce n'est pas l'art qui imite la nature mais to...\n",
       "30213  Seule civette de Meuse, une des rares en Lorra...\n",
       "30214  La rétrospective s'ouvre par une série de tabl...\n",
       "30215  Paradoxalement, les meilleures conditions de v...\n",
       "30216  S3 : 1. P. Billod (TCVB) 42 pts ; 2. R. Peter ...\n",
       "30217  Ressuscitées depuis peu par le Kiwanis club de...\n",
       "30218  La première touche de la palette a été symboli...\n",
       "30219  L'association sportive des sourds de Sochaux-M...\n",
       "30220  En d'autres temps, semblable flambée des cours...\n",
       "30221  Eddy Merckx, qui a inauguré mercredi soir le v...\n",
       "30222  En classe, il y avait étalés sur les tables le...\n",
       "30223  Raon passait alors un sale quart d'heure et on...\n",
       "30224  Le classement : 1. A. Dolguikh (SC Sarreguemin...\n",
       "30225  Gugnécourt, 16 h, hier après-midi : « T'as tou...\n",
       "30226  Hier matin, Guy Souhait, accompagné par Evelyn...\n",
       "30227  2481. Rigollot j. (Ind) ; 2482. Martinez y. (I...\n",
       "30228  Avant le grand départ pour les vacances, les q...\n",
       "30229  Avec un taux de participation de 49,42 % au sc...\n",
       "30230  Des chiffres : les fêtes artisanales et artist...\n",
       "30231  Les disciples d'Asllepios, dieu mythologique g...\n",
       "30232  Ils ont venus avec leurs médailles, leurs drap...\n",
       "30233  Celui du mois d'août a connu un certain privil...\n",
       "30234  Punch Nancy bat Saint-Mihiel 15-12, 15-7 : Les...\n",
       "30235  La plupart des partants, pompiers professionne...\n",
       "30236  Et le rêve américain ne l'a plus quitté. « A m...\n",
       "30237  Deux mois après l'incendie du tunnel du Mont-B...\n",
       "30238  Cartons rouges aux installations sportives.- L...\n",
       "30239  81. Knapek (UST ADEGEM) 56'10 ; 82. Filliot (A...\n",
       "30240  Né à Eloyes, en 1908, issu d'une famille impla...\n",
       "\n",
       "[30241 rows x 1 columns]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[0:1000]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                             paragraphe\n",
       "0    André Bauer, le Bonhomme de St-Dié ; Alain Dag...\n",
       "1    Le Smash Entente Club de Lunéville (SECL) a re...\n",
       "2    En tout cas, du côté du PS Dole qui reste en c...\n",
       "3    « Le nombre des donneurs était en légère baiss...\n",
       "4    ELOYES._ Les Ramoncenais n'auront tenu qu'une ...\n",
       "5    Heureusement, l'affaire ne relève en rien du t...\n",
       "6    Plus d'un Romarimontain sur deux, sans doute, ...\n",
       "7    BESANÇON._ « C'est non ». La voix de Pierre-Al...\n",
       "8    Une cinquantaine d'enfants de CE2, CM1 et CM2 ...\n",
       "9    Premiers à se lancer dans la course, les vétér...\n",
       "10   L'assemblée générale de Tricot Couture Service...\n",
       "11   Aux HLM, en revanche, on a le sourire. Les prê...\n",
       "12   Le début du mois a vu le départ d'un groupe d'...\n",
       "13   Les risques ? « Comme tout ce qui vole, les mo...\n",
       "14   « Je comprends l'impatience des supporters qui...\n",
       "15   Annie Humbert a remercié l'ensemble des associ...\n",
       "16   HDL est une association au service des collect...\n",
       "17   Pendant que les diplomates s'écharpent en Macé...\n",
       "18   L'équipe B disputera un match amical, le mardi...\n",
       "19   En 1990, un programme expérimental d'hygiène b...\n",
       "20   « Cette saison 98-99 a été l'année des jeunes ...\n",
       "21   OPPOSITION. Pour l'instant, le SLUC n'a rencon...\n",
       "22   Désireux de prendre sa revanche avec Dame coup...\n",
       "23   « Cette opération au Kosovo n'est pas une acti...\n",
       "24   Le contact se fait tout naturellement. Assis e...\n",
       "25   Dans notre édition de lundi, il était fait éta...\n",
       "26   - Je ne sais pas mais c'est quelqu'un qui a un...\n",
       "27   Avec l'Office du tourisme. De 11 h à 19 h, exc...\n",
       "28   Tournier en vitrine. Les collégiens qui passen...\n",
       "29   Le Franch Country festival a commencé sous d'e...\n",
       "..                                                 ...\n",
       "970  Caractérisées par une composition solide et un...\n",
       "971  L'homme qui a abusé de son innocence n'était a...\n",
       "972  « Ce qui lui est reproché ne saurait donc cons...\n",
       "973  A compter de la semaine prochaine, la sympathi...\n",
       "974  Une réunion du conseil municipal se tiendra le...\n",
       "975  Ce faisant, Jacques Chirac échappe aux reproch...\n",
       "976  La corporation met, également, le doigt sur le...\n",
       "977  Les 15 % de dénivellation recensés sur ces que...\n",
       "978  Jean-Pierre Monier orateur à la précision chir...\n",
       "979  Autant de recherches qui peuvent bénéficier de...\n",
       "980  Les « hostilités » ont démarré quand les comme...\n",
       "981  L'association Vie libre compte une centaine de...\n",
       "982  Egalement accessible aux VTT, ce très beau cir...\n",
       "983  « Notre recrutement basé sur des hommes d'expé...\n",
       "984  Ils sont au nombre de 33 pour ce 33e rendez-vo...\n",
       "985  C'est au début du XIIe siècle que Pierre-Percé...\n",
       "986  Un drôle d'attelage était présent dimanche lor...\n",
       "987  Ce même RPR, lui, se retrouvera du vendredi 27...\n",
       "988  Du côté municipal, on joue la transparence. Le...\n",
       "989  François Covini fait une rétrospective des anc...\n",
       "990  Le classement : 1. Miletto C./Dynamik (Nîmes) ...\n",
       "991  Le gouvernement chinois a annoncé hier une hau...\n",
       "992  Tout juste craint-il qu'on le cherche désormai...\n",
       "993  Les élus, qui se retrouvaient mardi soir au gr...\n",
       "994  6 h : journal-météo (R) ; 9 h : bloc-notes ; 1...\n",
       "995  La MJC organise en juillet et août des mini-ca...\n",
       "996  Dans chaque recoin, les discussions rivalisent...\n",
       "997  Jacques Toupin qui, l'an passé, avait choisi l...\n",
       "998  Quentin Schlosser, 20 ans nous donne son impre...\n",
       "999  La lutte contre les inondations, la protection...\n",
       "\n",
       "[1000 rows x 1 columns]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraphe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>André Bauer, le Bonhomme de St-Dié ; Alain Dag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Smash Entente Club de Lunéville (SECL) a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En tout cas, du côté du PS Dole qui reste en c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>« Le nombre des donneurs était en légère baiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELOYES._ Les Ramoncenais n'auront tenu qu'une ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paragraphe\n",
       "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...\n",
       "1  Le Smash Entente Club de Lunéville (SECL) a re...\n",
       "2  En tout cas, du côté du PS Dole qui reste en c...\n",
       "3  « Le nombre des donneurs était en légère baiss...\n",
       "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avant: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"avant: {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', 'confirment', \"l'absence\", 'de', 'solution', 'parfaites']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste des signes de ponctuation\n",
      "\t!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "{'!': '', '\"': '', '#': '', '$': '', '%': '', '&': '', \"'\": '', '(': '', ')': '', '*': '', '+': '', ',': '', '-': '', '.': '', '/': '', ':': '', ';': '', '<': '', '=': '', '>': '', '?': '', '@': '', '[': '', '\\\\': '', ']': '', '^': '', '_': '', '`': '', '{': '', '|': '', '}': '', '~': ''}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(\"liste des signes de ponctuation\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "d = {}\n",
    "for k in string.punctuation:\n",
    "    d[k]=''\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-deaf1ee1bd99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'translator' is not defined"
     ]
    }
   ],
   "source": [
    "new_sentence=sentence.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== phrase originale \n",
      "\tles médiateurs confirment l'absence de solution parfaites\n",
      "\n",
      "=== sans la ponctuation \n",
      "\tles médiateurs confirment labsence de solution parfaites\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translator = str.maketrans(d)\n",
    "new_sentence=sentence.translate(translator)\n",
    "print()\n",
    "print(\"=== phrase originale \\n\\t{}\\n\".format(sentence))\n",
    "print(\"=== sans la ponctuation \\n\\t{}\\n\".format(new_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== stopwords - français: \n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "print(\"=== stopwords - français: \\n{}\".format(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', 'confirment', 'labsence', 'de', 'solution', 'parfaites']\n",
      "=== stopwords - français: \n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
      "['médiateurs', 'confirment', 'labsence', 'solution', 'parfaites']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(new_sentence)\n",
    "print(tokens)\n",
    "from nltk.corpus import stopwords\n",
    "print(\"=== stopwords - français: \\n{}\".format(stopwords.words('french')))\n",
    "list_stopwords=stopwords.words('french') + ['les', 'de']\n",
    "#tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "print(tokens_sans_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': ' ', '\"': ' ', '#': ' ', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '.': ' ', '/': ' ', ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '`': ' ', '{': ' ', '|': ' ', '}': ' ', '~': ' ', '“': ' ', '”': ' ', '…': ' ', '»': ' ', '«': ' ', '’': ' ', '\\r': ' ', '\\n': ' ', '_': ' '}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-34f196bf871d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m## Equivalent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtokens_sans_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtokens_sans_stopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# maintenant on fait ça sur tout le corpus\n",
    "import string\n",
    "string.punctuation = ''.join([s for s in string.punctuation if s not in ['_', '-']  ]) + '“”…»«’\\r\\n_'\n",
    "dict_ponctuation = {}\n",
    "for k in string.punctuation:\n",
    "    dict_ponctuation[k] = ' '\n",
    "\n",
    "print(dict_ponctuation)\n",
    "# # L'operateur de translation\n",
    "translator = str.maketrans(dict_ponctuation)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# print(\"=== stopwords - français:\")\n",
    "# print(sorted(stopwords.words('french')))\n",
    "\n",
    "list_stopwords = stopwords.words('french') + ['les', 'de']\n",
    "\n",
    "# # equivalent:\n",
    "# stopwords = stopwords.words('french')\n",
    "# stopwords.append('les')\n",
    "# stopwords.append('de')\n",
    "\n",
    "#tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "## Equivalent \n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "\n",
    "df['text_no_ponct']=df.paragraphe.apply(lambda d : (\n",
    "    d.translate(translator)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', ',', 'confirment', \"l'absence\", 'de', 'solution', 'parfaites', '.']\n",
      "liste des signes de ponctuation\n",
      "\t!\"#$%&'()*+,./:;<=>?@[\\]^`{|}~“”…»«’\r\n",
      "“”…»«’\r\n",
      "_\n",
      "{'!': '', '\"': '', '#': '', '$': '', '%': '', '&': '', \"'\": '', '(': '', ')': '', '*': '', '+': '', ',': '', '.': '', '/': '', ':': '', ';': '', '<': '', '=': '', '>': '', '?': '', '@': '', '[': '', '\\\\': '', ']': '', '^': '', '`': '', '{': '', '|': '', '}': '', '~': '', '“': '', '”': '', '…': '', '»': '', '«': '', '’': '', '\\r': '', '\\n': '', '_': ''}\n",
      "\n",
      "=== phrase originale \n",
      "\tles médiateurs, confirment l'absence de solution parfaites.\n",
      "\n",
      "=== sans la ponctuation \n",
      "\tles médiateurs confirment labsence de solution parfaites\n",
      "\n",
      "['les', 'médiateurs', 'confirment', 'labsence', 'de', 'solution', 'parfaites']\n",
      "=== stopwords - français: \n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
      "['médiateurs', 'confirment', 'labsence', 'solution', 'parfaites']\n",
      "{'!': ' ', '\"': ' ', '#': ' ', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '.': ' ', '/': ' ', ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '`': ' ', '{': ' ', '|': ' ', '}': ' ', '~': ' ', '“': ' ', '”': ' ', '…': ' ', '»': ' ', '«': ' ', '’': ' ', '\\r': ' ', '\\n': ' ', '_': ' '}\n",
      "                                          paragraphe  \\\n",
      "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
      "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
      "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
      "3  « Le nombre des donneurs était en légère baiss...   \n",
      "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
      "\n",
      "                                       text_no_ponct  token_count  \n",
      "0  André Bauer  le Bonhomme de St-Dié   Alain Dag...         1156  \n",
      "1  Le Smash Entente Club de Lunéville  SECL  a re...          704  \n",
      "2  En tout cas  du côté du PS Dole qui reste en c...          638  \n",
      "3    Le nombre des donneurs était en légère baiss...          740  \n",
      "4  ELOYES   Les Ramoncenais n auront tenu qu une ...          683  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean      790.159000\n",
       "std       306.593495\n",
       "min       414.000000\n",
       "25%       648.000000\n",
       "50%       718.500000\n",
       "75%       851.000000\n",
       "max      7612.000000\n",
       "Name: token_count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATA_PATH='../data/'\n",
    "filename= 'estrepublicain_annee_1999.csv'\n",
    "df=pd.read_csv(DATA_PATH+filename)\n",
    "\n",
    "df.shape\n",
    "df.head()\n",
    "df.columns=['paragraphe']\n",
    "df.head\n",
    "df=df[0:1000]\n",
    "df.shape\n",
    "#on a pris que les 1000 premières rangées\n",
    "\n",
    "#fonction .split('') permet de découper un texte\n",
    "\n",
    "# comme on est des bbrèles on utilise un package\n",
    "from nltk import word_tokenize\n",
    "sentence = \"les médiateurs, confirment l'absence de solution parfaites.\"\n",
    "tokens=word_tokenize(sentence)\n",
    "print(tokens)\n",
    "#pour avoir tous les signes de ponctuation\n",
    "import string\n",
    "print(\"liste des signes de ponctuation\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "d = {}\n",
    "for k in string.punctuation:\n",
    "    d[k]=''\n",
    "print(d)\n",
    "\n",
    "#on vire donc la ponct\n",
    "translator = str.maketrans(d)\n",
    "new_sentence=sentence.translate(translator)\n",
    "print()\n",
    "print(\"=== phrase originale \\n\\t{}\\n\".format(sentence))\n",
    "print(\"=== sans la ponctuation \\n\\t{}\\n\".format(new_sentence))\n",
    "\n",
    "# virer les mots relous\n",
    "tokens=word_tokenize(new_sentence)\n",
    "print(tokens)\n",
    "from nltk.corpus import stopwords\n",
    "print(\"=== stopwords - français: \\n{}\".format(stopwords.words('french')))\n",
    "list_stopwords=stopwords.words('french') + ['les', 'de']\n",
    "#tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "print(tokens_sans_stopwords)\n",
    "###########################################\n",
    "# maintenant on fait ça sur tout le corpus\n",
    "string.punctuation = ''.join([s for s in string.punctuation if s not in ['_', '-']  ]) + '“”…»«’\\r\\n_'\n",
    "dict_ponctuation = {}\n",
    "for k in string.punctuation:\n",
    "    dict_ponctuation[k] = ' '\n",
    "\n",
    "print(dict_ponctuation)\n",
    "# # L'operateur de translation\n",
    "translator = str.maketrans(dict_ponctuation)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# print(\"=== stopwords - français:\")\n",
    "# print(sorted(stopwords.words('french')))\n",
    "\n",
    "list_stopwords = stopwords.words('french') + ['les', 'de']\n",
    "\n",
    "# # equivalent:\n",
    "# stopwords = stopwords.words('french')\n",
    "# stopwords.append('les')\n",
    "# stopwords.append('de')\n",
    "\n",
    "#tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "## Equivalent \n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "\n",
    "df['text_no_ponct']=df.paragraphe.apply(lambda d : (\n",
    "    d.translate(translator)\n",
    "))\n",
    "df.head()\n",
    "\n",
    "# nombre de tokens par rangée\n",
    "pd.options.mode.chained_assignment = None\n",
    "df['token_count'] = df.paragraphe.apply( lambda r : len(r) )\n",
    "print(df.head())\n",
    "# repartition du nombre de tokens\n",
    "df.token_count.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                  1000\n",
       "unique                                                 1000\n",
       "top       NANCY    Deux défaites d entrée  Villeurbanne ...\n",
       "freq                                                      1\n",
       "Name: text_no_ponct, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text_no_ponct.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tokens_sans_stopwords'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-600738e52f69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_sans_stopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3614\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tokens_sans_stopwords'"
     ]
    }
   ],
   "source": [
    "df.tokens_sans_stopwords.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', ',', 'confirment', \"l'absence\", 'de', 'solution', 'parfaites', '.']\n",
      "liste des signes de ponctuation\n",
      "\t!\"#$%&'()*+,./:;<=>?@[\\]^`{|}~“”…»«’\r\n",
      "“”…»«’\r\n",
      "“”…»«’\r\n",
      "_\n",
      "{'!': '', '\"': '', '#': '', '$': '', '%': '', '&': '', \"'\": '', '(': '', ')': '', '*': '', '+': '', ',': '', '.': '', '/': '', ':': '', ';': '', '<': '', '=': '', '>': '', '?': '', '@': '', '[': '', '\\\\': '', ']': '', '^': '', '`': '', '{': '', '|': '', '}': '', '~': '', '“': '', '”': '', '…': '', '»': '', '«': '', '’': '', '\\r': '', '\\n': '', '_': ''}\n",
      "\n",
      "=== phrase originale \n",
      "\tles médiateurs, confirment l'absence de solution parfaites.\n",
      "\n",
      "=== sans la ponctuation \n",
      "\tles médiateurs confirment labsence de solution parfaites\n",
      "\n",
      "['les', 'médiateurs', 'confirment', 'labsence', 'de', 'solution', 'parfaites']\n",
      "=== stopwords - français: \n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
      "['médiateurs', 'confirment', 'labsence', 'solution', 'parfaites']\n",
      "{'!': ' ', '\"': ' ', '#': ' ', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '.': ' ', '/': ' ', ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '`': ' ', '{': ' ', '|': ' ', '}': ' ', '~': ' ', '“': ' ', '”': ' ', '…': ' ', '»': ' ', '«': ' ', '’': ' ', '\\r': ' ', '\\n': ' ', '_': ' '}\n",
      "                                          paragraphe  \\\n",
      "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
      "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
      "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
      "3  « Le nombre des donneurs était en légère baiss...   \n",
      "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
      "\n",
      "                                       text_no_ponct  token_count  \n",
      "0  André Bauer  le Bonhomme de St-Dié   Alain Dag...         1156  \n",
      "1  Le Smash Entente Club de Lunéville  SECL  a re...          704  \n",
      "2  En tout cas  du côté du PS Dole qui reste en c...          638  \n",
      "3    Le nombre des donneurs était en légère baiss...          740  \n",
      "4  ELOYES   Les Ramoncenais n auront tenu qu une ...          683  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean      790.159000\n",
       "std       306.593495\n",
       "min       414.000000\n",
       "25%       648.000000\n",
       "50%       718.500000\n",
       "75%       851.000000\n",
       "max      7612.000000\n",
       "Name: token_count, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATA_PATH='../data/'\n",
    "filename= 'estrepublicain_annee_1999.csv'\n",
    "df=pd.read_csv(DATA_PATH+filename)\n",
    "\n",
    "df.shape\n",
    "df.head()\n",
    "df.columns=['paragraphe']\n",
    "df.head\n",
    "df=df[0:1000]\n",
    "df.shape\n",
    "#on a pris que les 1000 premières rangées\n",
    "\n",
    "#fonction .split('') permet de découper un texte\n",
    "\n",
    "# comme on est des bbrèles on utilise un package\n",
    "from nltk import word_tokenize\n",
    "sentence = \"les médiateurs, confirment l'absence de solution parfaites.\"\n",
    "tokens=word_tokenize(sentence)\n",
    "print(tokens)\n",
    "#pour avoir tous les signes de ponctuation\n",
    "import string\n",
    "print(\"liste des signes de ponctuation\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "d = {}\n",
    "for k in string.punctuation:\n",
    "    d[k]=''\n",
    "print(d)\n",
    "\n",
    "#on vire donc la ponct\n",
    "translator = str.maketrans(d)\n",
    "new_sentence=sentence.translate(translator)\n",
    "print()\n",
    "print(\"=== phrase originale \\n\\t{}\\n\".format(sentence))\n",
    "print(\"=== sans la ponctuation \\n\\t{}\\n\".format(new_sentence))\n",
    "\n",
    "# virer les mots relous\n",
    "tokens=word_tokenize(new_sentence)\n",
    "print(tokens)\n",
    "from nltk.corpus import stopwords\n",
    "print(\"=== stopwords - français: \\n{}\".format(stopwords.words('french')))\n",
    "list_stopwords=stopwords.words('french') + ['les', 'de']\n",
    "#tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "print(tokens_sans_stopwords)\n",
    "###########################################\n",
    "# maintenant on fait ça sur tout le corpus\n",
    "string.punctuation = ''.join([s for s in string.punctuation if s not in ['_', '-']  ]) + '“”…»«’\\r\\n_'\n",
    "dict_ponctuation = {}\n",
    "for k in string.punctuation:\n",
    "    dict_ponctuation[k] = ' '\n",
    "\n",
    "print(dict_ponctuation)\n",
    "# # L'operateur de translation\n",
    "translator = str.maketrans(dict_ponctuation)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# print(\"=== stopwords - français:\")\n",
    "# print(sorted(stopwords.words('french')))\n",
    "\n",
    "list_stopwords = stopwords.words('french') + ['les', 'de']\n",
    "\n",
    "# # equivalent:\n",
    "# stopwords = stopwords.words('french')\n",
    "# stopwords.append('les')\n",
    "# stopwords.append('de')\n",
    "\n",
    "#tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "## Equivalent \n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "\n",
    "df['text_no_ponct']=df.paragraphe.apply(lambda d : (\n",
    "    d.translate(translator)\n",
    "))\n",
    "df.head()\n",
    "\n",
    "# nombre de tokens par rangée\n",
    "pd.options.mode.chained_assignment = None\n",
    "df['token_count'] = df.paragraphe.apply( lambda r : len(r) )\n",
    "print(df.head())\n",
    "# repartition du nombre de tokens\n",
    "df.token_count.describe()\n",
    "\n",
    "# enlever le paragraphe le plus long\n",
    "condition_filtrage = df.token_count < 955\n",
    "print(df[condition_filtrage].shape)\n",
    "df = df[condition_filtrage]\n",
    "print(df[condition_filtrage].shape)\n",
    "print(df.token_count.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', ',', 'confirment', \"l'absence\", 'de', 'solution', 'parfaites', '.']\n",
      "liste des signes de ponctuation\n",
      "\t!\"#$%&'()*+,./:;<=>?@[\\]^`{|}~“”…»«’\r\n",
      "“”…»«’\r\n",
      "“”…»«’\r\n",
      "“”…»«’\r\n",
      "_\n",
      "{'!': '', '\"': '', '#': '', '$': '', '%': '', '&': '', \"'\": '', '(': '', ')': '', '*': '', '+': '', ',': '', '.': '', '/': '', ':': '', ';': '', '<': '', '=': '', '>': '', '?': '', '@': '', '[': '', '\\\\': '', ']': '', '^': '', '`': '', '{': '', '|': '', '}': '', '~': '', '“': '', '”': '', '…': '', '»': '', '«': '', '’': '', '\\r': '', '\\n': '', '_': ''}\n",
      "\n",
      "=== phrase originale \n",
      "\tles médiateurs, confirment l'absence de solution parfaites.\n",
      "\n",
      "=== sans la ponctuation \n",
      "\tles médiateurs confirment labsence de solution parfaites\n",
      "\n",
      "['les', 'médiateurs', 'confirment', 'labsence', 'de', 'solution', 'parfaites']\n",
      "=== stopwords - français: \n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
      "['médiateurs', 'confirment', 'labsence', 'solution', 'parfaites']\n",
      "{'!': ' ', '\"': ' ', '#': ' ', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '.': ' ', '/': ' ', ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '`': ' ', '{': ' ', '|': ' ', '}': ' ', '~': ' ', '“': ' ', '”': ' ', '…': ' ', '»': ' ', '«': ' ', '’': ' ', '\\r': ' ', '\\n': ' ', '_': ' '}\n",
      "                                          paragraphe  \\\n",
      "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
      "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
      "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
      "3  « Le nombre des donneurs était en légère baiss...   \n",
      "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
      "\n",
      "                                       text_no_ponct  token_count  \n",
      "0  André Bauer  le Bonhomme de St-Dié   Alain Dag...         1156  \n",
      "1  Le Smash Entente Club de Lunéville  SECL  a re...          704  \n",
      "2  En tout cas  du côté du PS Dole qui reste en c...          638  \n",
      "3    Le nombre des donneurs était en légère baiss...          740  \n",
      "4  ELOYES   Les Ramoncenais n auront tenu qu une ...          683  \n",
      "(845, 3)\n",
      "(845, 3)\n",
      "count    845.000000\n",
      "mean     709.865089\n",
      "std       99.897582\n",
      "min      414.000000\n",
      "25%      638.000000\n",
      "50%      691.000000\n",
      "75%      775.000000\n",
      "max      953.000000\n",
      "Name: token_count, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexis/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:95: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATA_PATH='../data/'\n",
    "filename= 'estrepublicain_annee_1999.csv'\n",
    "df=pd.read_csv(DATA_PATH+filename)\n",
    "\n",
    "df.shape\n",
    "df.head()\n",
    "df.columns=['paragraphe']\n",
    "df.head\n",
    "df=df[0:1000]\n",
    "df.shape\n",
    "#on a pris que les 1000 premières rangées\n",
    "\n",
    "#fonction .split('') permet de découper un texte\n",
    "\n",
    "# comme on est des bbrèles on utilise un package\n",
    "from nltk import word_tokenize\n",
    "sentence = \"les médiateurs, confirment l'absence de solution parfaites.\"\n",
    "tokens=word_tokenize(sentence)\n",
    "print(tokens)\n",
    "#pour avoir tous les signes de ponctuation\n",
    "import string\n",
    "print(\"liste des signes de ponctuation\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "d = {}\n",
    "for k in string.punctuation:\n",
    "    d[k]=''\n",
    "print(d)\n",
    "\n",
    "#on vire donc la ponct\n",
    "translator = str.maketrans(d)\n",
    "new_sentence=sentence.translate(translator)\n",
    "print()\n",
    "print(\"=== phrase originale \\n\\t{}\\n\".format(sentence))\n",
    "print(\"=== sans la ponctuation \\n\\t{}\\n\".format(new_sentence))\n",
    "\n",
    "# virer les mots relous\n",
    "tokens=word_tokenize(new_sentence)\n",
    "print(tokens)\n",
    "from nltk.corpus import stopwords\n",
    "print(\"=== stopwords - français: \\n{}\".format(stopwords.words('french')))\n",
    "list_stopwords=stopwords.words('french') + ['les', 'de']\n",
    "#tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "print(tokens_sans_stopwords)\n",
    "###########################################\n",
    "# maintenant on fait ça sur tout le corpus\n",
    "string.punctuation = ''.join([s for s in string.punctuation if s not in ['_', '-']  ]) + '“”…»«’\\r\\n_'\n",
    "dict_ponctuation = {}\n",
    "for k in string.punctuation:\n",
    "    dict_ponctuation[k] = ' '\n",
    "\n",
    "print(dict_ponctuation)\n",
    "# # L'operateur de translation\n",
    "translator = str.maketrans(dict_ponctuation)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# print(\"=== stopwords - français:\")\n",
    "# print(sorted(stopwords.words('french')))\n",
    "\n",
    "list_stopwords = stopwords.words('french') + ['les', 'de']\n",
    "\n",
    "# # equivalent:\n",
    "# stopwords = stopwords.words('french')\n",
    "# stopwords.append('les')\n",
    "# stopwords.append('de')\n",
    "\n",
    "#tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "## Equivalent \n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "\n",
    "df['text_no_ponct']=df.paragraphe.apply(lambda d : (\n",
    "    d.translate(translator)\n",
    "))\n",
    "df.head()\n",
    "\n",
    "# nombre de tokens par rangée\n",
    "pd.options.mode.chained_assignment = None\n",
    "df['token_count'] = df.paragraphe.apply( lambda r : len(r) )\n",
    "print(df.head())\n",
    "# repartition du nombre de tokens\n",
    "df.token_count.describe()\n",
    "\n",
    "# enlever le paragraphe le plus long\n",
    "condition_filtrage = df.token_count < 955\n",
    "print(df[condition_filtrage].shape)\n",
    "df = df[condition_filtrage]\n",
    "print(df[condition_filtrage].shape)\n",
    "print(df.token_count.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', ',', 'confirment', \"l'absence\", 'de', 'solution', 'parfaites', '.']\n",
      "liste des signes de ponctuation\n",
      "\t!\"#$%&'()*+,./:;<=>?@[\\]^`{|}~“”…»«’\r\n",
      "“”…»«’\r\n",
      "“”…»«’\r\n",
      "“”…»«’\r\n",
      "“”…»«’\r\n",
      "_\n",
      "{'!': '', '\"': '', '#': '', '$': '', '%': '', '&': '', \"'\": '', '(': '', ')': '', '*': '', '+': '', ',': '', '.': '', '/': '', ':': '', ';': '', '<': '', '=': '', '>': '', '?': '', '@': '', '[': '', '\\\\': '', ']': '', '^': '', '`': '', '{': '', '|': '', '}': '', '~': '', '“': '', '”': '', '…': '', '»': '', '«': '', '’': '', '\\r': '', '\\n': '', '_': ''}\n",
      "\n",
      "=== phrase originale \n",
      "\tles médiateurs, confirment l'absence de solution parfaites.\n",
      "\n",
      "=== sans la ponctuation \n",
      "\tles médiateurs confirment labsence de solution parfaites\n",
      "\n",
      "['les', 'médiateurs', 'confirment', 'labsence', 'de', 'solution', 'parfaites']\n",
      "=== stopwords - français: \n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
      "['médiateurs', 'confirment', 'labsence', 'solution', 'parfaites']\n",
      "{'!': ' ', '\"': ' ', '#': ' ', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '.': ' ', '/': ' ', ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '`': ' ', '{': ' ', '|': ' ', '}': ' ', '~': ' ', '“': ' ', '”': ' ', '…': ' ', '»': ' ', '«': ' ', '’': ' ', '\\r': ' ', '\\n': ' ', '_': ' '}\n",
      "                                          paragraphe  \\\n",
      "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
      "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
      "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
      "3  « Le nombre des donneurs était en légère baiss...   \n",
      "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
      "\n",
      "                                       text_no_ponct  token_count  \n",
      "0  André Bauer  le Bonhomme de St-Dié   Alain Dag...         1156  \n",
      "1  Le Smash Entente Club de Lunéville  SECL  a re...          704  \n",
      "2  En tout cas  du côté du PS Dole qui reste en c...          638  \n",
      "3    Le nombre des donneurs était en légère baiss...          740  \n",
      "4  ELOYES   Les Ramoncenais n auront tenu qu une ...          683  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean      790.159000\n",
       "std       306.593495\n",
       "min       414.000000\n",
       "25%       648.000000\n",
       "50%       718.500000\n",
       "75%       851.000000\n",
       "max      7612.000000\n",
       "Name: token_count, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATA_PATH='../data/'\n",
    "filename= 'estrepublicain_annee_1999.csv'\n",
    "df=pd.read_csv(DATA_PATH+filename)\n",
    "\n",
    "df.shape\n",
    "df.head()\n",
    "df.columns=['paragraphe']\n",
    "df.head\n",
    "df=df[0:1000]\n",
    "df.shape\n",
    "#on a pris que les 1000 premières rangées\n",
    "\n",
    "#fonction .split('') permet de découper un texte\n",
    "\n",
    "# comme on est des bbrèles on utilise un package\n",
    "from nltk import word_tokenize\n",
    "sentence = \"les médiateurs, confirment l'absence de solution parfaites.\"\n",
    "tokens=word_tokenize(sentence)\n",
    "print(tokens)\n",
    "#pour avoir tous les signes de ponctuation\n",
    "import string\n",
    "print(\"liste des signes de ponctuation\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "d = {}\n",
    "for k in string.punctuation:\n",
    "    d[k]=''\n",
    "print(d)\n",
    "\n",
    "#on vire donc la ponct\n",
    "translator = str.maketrans(d)\n",
    "new_sentence=sentence.translate(translator)\n",
    "print()\n",
    "print(\"=== phrase originale \\n\\t{}\\n\".format(sentence))\n",
    "print(\"=== sans la ponctuation \\n\\t{}\\n\".format(new_sentence))\n",
    "\n",
    "# virer les mots relous\n",
    "tokens=word_tokenize(new_sentence)\n",
    "print(tokens)\n",
    "from nltk.corpus import stopwords\n",
    "print(\"=== stopwords - français: \\n{}\".format(stopwords.words('french')))\n",
    "list_stopwords=stopwords.words('french') + ['les', 'de']\n",
    "#tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "print(tokens_sans_stopwords)\n",
    "###########################################\n",
    "# maintenant on fait ça sur tout le corpus\n",
    "string.punctuation = ''.join([s for s in string.punctuation if s not in ['_', '-']  ]) + '“”…»«’\\r\\n_'\n",
    "dict_ponctuation = {}\n",
    "for k in string.punctuation:\n",
    "    dict_ponctuation[k] = ' '\n",
    "\n",
    "print(dict_ponctuation)\n",
    "# # L'operateur de translation\n",
    "translator = str.maketrans(dict_ponctuation)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# print(\"=== stopwords - français:\")\n",
    "# print(sorted(stopwords.words('french')))\n",
    "\n",
    "list_stopwords = stopwords.words('french') + ['les', 'de']\n",
    "\n",
    "# # equivalent:\n",
    "# stopwords = stopwords.words('french')\n",
    "# stopwords.append('les')\n",
    "# stopwords.append('de')\n",
    "\n",
    "#tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "## Equivalent \n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "\n",
    "df['text_no_ponct']=df.paragraphe.apply(lambda d : (\n",
    "    d.translate(translator)\n",
    "))\n",
    "df.head()\n",
    "\n",
    "# nombre de tokens par rangée\n",
    "pd.options.mode.chained_assignment = None\n",
    "df['token_count'] = df.paragraphe.apply( lambda r : len(r) )\n",
    "print(df.head())\n",
    "# repartition du nombre de tokens\n",
    "df.token_count.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_word_list=get_stop_words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ai', 'aie', 'aient', 'aies', 'ait', 'alors', 'as', 'au', 'aucun', 'aura', 'aurai', 'auraient', 'aurais', 'aurait', 'auras', 'aurez', 'auriez', 'aurions', 'aurons', 'auront', 'aussi', 'autre', 'aux', 'avaient', 'avais', 'avait', 'avant', 'avec', 'avez', 'aviez', 'avions', 'avoir', 'avons', 'ayant', 'ayez', 'ayons', 'bon', 'car', 'ce', 'ceci', 'cela', 'ces', 'cet', 'cette', 'ceux', 'chaque', 'ci', 'comme', 'comment', 'd', 'dans', 'de', 'dedans', 'dehors', 'depuis', 'des', 'deux', 'devoir', 'devrait', 'devrez', 'devriez', 'devrions', 'devrons', 'devront', 'dois', 'doit', 'donc', 'dos', 'droite', 'du', 'dès', 'début', 'dù', 'elle', 'elles', 'en', 'encore', 'es', 'est', 'et', 'eu', 'eue', 'eues', 'eurent', 'eus', 'eusse', 'eussent', 'eusses', 'eussiez', 'eussions', 'eut', 'eux', 'eûmes', 'eût', 'eûtes', 'faire', 'fais', 'faisez', 'fait', 'faites', 'fois', 'font', 'force', 'furent', 'fus', 'fusse', 'fussent', 'fusses', 'fussiez', 'fussions', 'fut', 'fûmes', 'fût', 'fûtes', 'haut', 'hors', 'ici', 'il', 'ils', 'j', 'je', 'juste', 'l', 'la', 'le', 'les', 'leur', 'leurs', 'lui', 'là', 'm', 'ma', 'maintenant', 'mais', 'me', 'mes', 'moi', 'moins', 'mon', 'mot', 'même', 'n', 'ne', 'ni', 'nom', 'nommé', 'nommée', 'nommés', 'nos', 'notre', 'nous', 'nouveau', 'nouveaux', 'on', 'ont', 'ou', 'où', 'par', 'parce', 'parole', 'pas', 'personne', 'personnes', 'peu', 'peut', 'plupart', 'pour', 'pourquoi', 'qu', 'quand', 'que', 'quel', 'quelle', 'quelles', 'quels', 'qui', 'sa', 'sans', 'se', 'sera', 'serai', 'seraient', 'serais', 'serait', 'seras', 'serez', 'seriez', 'serions', 'serons', 'seront', 'ses', 'seulement', 'si', 'sien', 'soi', 'soient', 'sois', 'soit', 'sommes', 'son', 'sont', 'sous', 'soyez', 'soyons', 'suis', 'sujet', 'sur', 't', 'ta', 'tandis', 'te', 'tellement', 'tels', 'tes', 'toi', 'ton', 'tous', 'tout', 'trop', 'très', 'tu', 'un', 'une', 'valeur', 'voient', 'vois', 'voit', 'vont', 'vos', 'votre', 'vous', 'vu', 'y', 'à', 'ça', 'étaient', 'étais', 'était', 'étant', 'état', 'étiez', 'étions', 'été', 'étés', 'êtes', 'être']\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_word_list=get_stop_words('french')\n",
    "print(stop_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120, 1)\n",
      "= dimensions\n",
      "(15120, 1)\n",
      "= 5 premieres rangés\n",
      "                                                text\n",
      "0  Affluence des grands jours en mairie d'Autrey,...\n",
      "1  La floraison des mirabelliers s'appuie sur des...\n",
      "2  Cette manifestation exceptionnelle aura lieu d...\n",
      "3  A l'issue de cette dernière saison estivale af...\n",
      "4  Gabriel et Jeanne ont eu une belle famille de ...\n",
      "= Noms des colonnes\n",
      "Index(['text'], dtype='object')\n",
      "['les', 'médiateurs', 'confirment', '«', \"l'absence\", 'de', 'solution', 'parfaite', '»', '.']\n",
      "['les', 'médiateurs', 'confirment', '«', \"l'absence\", 'de', 'solution', 'parfaite', '».', '']\n",
      "Liste original de signe de ponctuations\n",
      "\t!\"#$%&'()*+,./:;<=>?@[\\]^`{|}~“”…»«’\n",
      "“”…»«’\n",
      "“”…»«’\n",
      "“”…»«’\n",
      "“”…»«’\n",
      "“”…»«’\n",
      "_\n",
      "{'!': ' ', '\"': ' ', '#': ' ', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '.': ' ', '/': ' ', ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '`': ' ', '{': ' ', '|': ' ', '}': ' ', '~': ' ', '“': ' ', '”': ' ', '…': ' ', '»': ' ', '«': ' ', '’': ' ', '\\r': ' ', '\\n': ' ', '_': ' '}\n",
      "les médiateurs confirment « l'absence de solution parfaite ». \n",
      "les médiateurs confirment   l absence de solution parfaite    \n",
      "['les', 'médiateurs', 'confirment', 'l', 'absence', 'de', 'solution', 'parfaite']\n",
      "['médiateurs', 'confirment', 'absence', 'solution', 'parfaite']\n",
      "Index(['text', 'text_no_punctuation', 'tokens_all', 'tokens'], dtype='object')\n",
      "Index(['text', 'tokens'], dtype='object')\n",
      "                                                text  \\\n",
      "0  Affluence des grands jours en mairie d'Autrey,...   \n",
      "1  La floraison des mirabelliers s'appuie sur des...   \n",
      "2  Cette manifestation exceptionnelle aura lieu d...   \n",
      "3  A l'issue de cette dernière saison estivale af...   \n",
      "4  Gabriel et Jeanne ont eu une belle famille de ...   \n",
      "\n",
      "                                              tokens  token_count  \n",
      "0  [affluence, grands, jours, mairie, autrey, lor...           69  \n",
      "1  [floraison, mirabelliers, appuie, conditions, ...           76  \n",
      "2  [cette, manifestation, exceptionnelle, lieu, s...           74  \n",
      "3  [a, issue, cette, dernière, saison, estivale, ...          111  \n",
      "4  [gabriel, jeanne, belle, famille, huit, enfant...           70  \n",
      "[]\n",
      "Int64Index([], dtype='int64')\n",
      "(15116, 3)\n",
      "(15118, 3)\n",
      "(15118, 3)\n",
      "count    15118.000000\n",
      "mean        79.329012\n",
      "std         34.960931\n",
      "min         40.000000\n",
      "25%         63.000000\n",
      "50%         71.000000\n",
      "75%         85.000000\n",
      "max        950.000000\n",
      "Name: token_count, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexis/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:203: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(88667 unique tokens: ['affluence', 'grands', 'jours', 'mairie', 'autrey']...)\n",
      "[[ \"Affluence des grands jours en mairie d'Autrey, lors de la traditionnelle rencontre des élus du canton. Maires et conseillers municipaux avaient répondu en masse à l'appel d'Henri Blanchot, autour duquel on retrouvait, outre les représentants des administrations locales, M. Mettelet, directeur général des services départementaux venu présenter les objectifs de l'Agenda 21, et Alain Maraval, directeur départemental de l'Agriculture, qui traitait des orientations haut-saônoises, en particulier pour ce qui est du CTE. Constat alarmant dont l'assemblée départementale semble avoir pris bonne note dans ce dernier domaine, la faible industrialisation de notre secteur risque d'entraîner, si l'on ne favorise pas l'installation de jeunes agriculteurs, un vieillissement de notre population.\"\n",
      "  list([(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1)])]\n",
      " [ \"La floraison des mirabelliers s'appuie sur des conditions essentielles : « Comme la variété du sol. Ici, il y a de l'argile et du calcaire, une terre idéale. En automne, il faut veiller à la taille du mirabellier et surtout suivre attentivement la floraison », souligne-t-il. Et malgré toutes ces précautions, la récolte n'est pas toujours à la hauteur des espérances : « En 1996, par exemple, la cueillette avait été tardive, seulement à partir du 20 août, suite à un hiver très rigoureux. Plus on attend, plus les fruits deviennent trop mûrs », reprend Thierry Bégel. Les floraisons trop précoces peuvent aussi conduire à certains déboires : « Parfois les arbres fleurissent tôt alors que les périodes de gel ne sont pas finies. On a un avantage dans les Vosges, on fleurit plus tardivement que dans la Meuse. »\"\n",
      "  list([(68, 2), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 2), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 3), (111, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1)])]\n",
      " [ \"Cette manifestation exceptionnelle aura lieu du samedi 5 au mercredi 9 juin avec le soutien de la commission européenne. Une traduction simultanée, en français et en allemand, sera assuré pendant toute la rencontre et pendant les excursions à Strasbourg (visite du parlement européen), à Metz (visite de la maison-musée de Robert Schumann) et à Verdun (visite du fort de Douaumont, diaporama sur la bataille de Verdun). Les frais de participation sont de 1.500 F (227,52 euros) par personne (hébergement, pension complète et activités). Pour tout renseignement sur le programme et les inscriptions : Maison de l'Europe de Franche-Comté, tel. 03.81.83.11.37 ; fax : 03.84.54.30.57. Clôture des inscriptions le 28 mai.\"\n",
      "  list([(7, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 2), (157, 1), (158, 1), (159, 1), (160, 3), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 2), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 2), (189, 1), (190, 1), (191, 1), (192, 1), (193, 2), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1)])]\n",
      " [ \"A l'issue de cette dernière saison estivale affichant un bilan satisfaisant par rapport à l'année précédente, le comité de l'office de tourisme s'est réuni sous la présidence de Monique Arnould. Du côté des propriétaires, certains trouvent qu'ils ont moins bien loué, parce qu'ils avaient l'habitude de travailler à partir du mois de juin. En effet, depuis la nouvelle répartition des vacances d'été, les touristes privilégient la période située entre le 15 juillet et le 15 août. Le service de réservation avance les chiffres suivants : 29 % de remplissage du 3 au 10 juillet, 46 % du 10 au 17 juillet, 60 % du 17 au 24 juillet, 61 % du 24 au 31 juillet, 89 % du 1 au 7 août, 95 % du 7 au 14 août, 61 % du 14 au 21 août et enfin 37 % du 21 au 28 août. Si le mois de juillet n'affiche pas les résultats escomptés, le mois d'août est cependant très acceptable. On note essentiellement une clientèle étrangère, et en majorité belge, hollandaise et allemande, qui privilégie les séjours de courte durée.\"\n",
      "  list([(34, 2), (53, 1), (61, 1), (77, 1), (103, 1), (105, 6), (108, 1), (124, 1), (139, 1), (147, 1), (174, 1), (197, 1), (204, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 2), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 3), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 2), (246, 6), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 2), (256, 1), (257, 2), (258, 1), (259, 2), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (265, 2), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 1), (281, 1), (282, 1)])]\n",
      " [ 'Gabriel et Jeanne ont eu une belle famille de huit enfants, presque tous retraités actuellement. Jean, marié et père de six enfants, demeure à Trévenans; André, marié et père de trois enfants est à Zainvillers; Juliette, épouse de Roger Didierlaurent, mère de cinq enfants habite Danjoutin; Bernadette, épouse de Pierre Géhin, mère de trois enfants, est à Belfort; Anne, épouse de Claude Thomas, mère de deux enfants, est à Vagney; Raymond, pré-retraité, marié, deux enfants, est aussi à Vagney; Geneviève, épouse de Maurice Claude, aide-ménagère, et mère de trois enfants, habite Le Ménil; et Michel, marié, employé du bâtiment est à Vagney.'\n",
      "  list([(122, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 8), (289, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 4), (295, 2), (296, 1), (297, 1), (298, 1), (299, 1), (300, 3), (301, 1), (302, 1), (303, 4), (304, 1), (305, 1), (306, 4), (307, 1), (308, 2), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 2), (316, 1), (317, 2), (318, 3), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 1), (325, 1), (326, 1), (327, 1)])]]\n",
      "\n",
      "=== topic #0\n",
      "0.007: a,  0.004: chasse,  0.003: club,  0.003: vol,  0.003: deux,  0.002: badminton,  0.002: plus,  0.002: propositions,  0.002: mn,  0.002: marseille\n",
      "\n",
      "=== topic #1\n",
      "0.015: f,  0.009: a,  0.007: enfants,  0.007: association,  0.007: cette,  0.007: 000,  0.006: tous,  0.006: jeunes,  0.005: centre,  0.005: année\n",
      "\n",
      "=== topic #2\n",
      "0.006: a,  0.006: plus,  0.004: faire,  0.003: pêche,  0.003: leurs,  0.003: deux,  0.003: enfants,  0.003: tout,  0.003: être,  0.003: si\n",
      "\n",
      "=== topic #3\n",
      "0.016: a,  0.011: équipe,  0.010: club,  0.010: ans,  0.009: deux,  0.009: plus,  0.008: jeunes,  0.007: cette,  0.006: saison,  0.005: équipes\n",
      "\n",
      "=== topic #4\n",
      "0.014: a,  0.006: plus,  0.005: comme,  0.005: tout,  0.004: deux,  0.004: bien,  0.004: ans,  0.003: enfants,  0.003: temps,  0.003: cette\n",
      "\n",
      "=== topic #5\n",
      "0.005: signifie,  0.005: pierrot,  0.004: exercé,  0.004: a,  0.004: maladies,  0.003: george,  0.003: sauvegarde,  0.003: capital,  0.003: 136,  0.003: ben\n",
      "\n",
      "=== topic #6\n",
      "0.007: conseil,  0.007: a,  0.006: subvention,  0.005: modification,  0.005: municipal,  0.005: communes,  0.004: convention,  0.004: ordre,  0.004: communale,  0.003: eau\n",
      "\n",
      "=== topic #7\n",
      "0.010: a,  0.006: mention,  0.005: façades,  0.003: lion,  0.003: terminale,  0.003: ceinture,  0.003: modèles,  0.003: paires,  0.003: plus,  0.003: bac\n",
      "\n",
      "=== topic #8\n",
      "0.011: a,  0.009: course,  0.007: france,  0.006: plus,  0.006: deux,  0.005: km,  0.005: sans,  0.005: après,  0.004: arrivée,  0.004: coureurs\n",
      "\n",
      "=== topic #9\n",
      "0.016: a,  0.008: plus,  0.007: 000,  0.007: cette,  0.005: travaux,  0.005: f,  0.005: être,  0.004: conseil,  0.004: mois,  0.003: projet\n",
      "\n",
      "=== topic #10\n",
      "0.013: ans,  0.007: a,  0.005: rue,  0.005: remiremont,  0.004: nancy,  0.004: veuve,  0.004: n2,  0.003: pierrat,  0.003: champigneulles,  0.003: retraité\n",
      "\n",
      "=== topic #11\n",
      "0.007: mp,  0.005: a,  0.004: jean-jacques,  0.003: juifs,  0.003: yvette,  0.003: joëlle,  0.003: evelyne,  0.003: annie,  0.003: francart,  0.003: expert\n",
      "\n",
      "=== topic #12\n",
      "0.005: église,  0.004: a,  0.004: derby,  0.003: associatif,  0.003: flammes,  0.003: saint-etienne,  0.003: mcl,  0.003: relie,  0.003: black,  0.003: aîné\n",
      "\n",
      "=== topic #13\n",
      "0.007: a,  0.005: produits,  0.004: producteurs,  0.003: heures,  0.003: cette,  0.003: emploie,  0.002: réagir,  0.002: violence,  0.002: deux,  0.002: être\n",
      "\n",
      "=== topic #14\n",
      "0.020: 1,  0.017: p,  0.015: 2,  0.014: f,  0.014: 4,  0.011: 3,  0.011: a,  0.010: 5,  0.009: 15,  0.009: 12\n",
      "\n",
      "=== topic #15\n",
      "0.012: a,  0.009: ils,  0.008: plus,  0.006: où,  0.006: cette,  0.005: très,  0.005: tout,  0.005: public,  0.004: sous,  0.004: village\n",
      "\n",
      "=== topic #16\n",
      "0.020: a,  0.006: président,  0.006: général,  0.006: maire,  0.005: cette,  0.005: conseil,  0.004: ville,  0.004: nouveau,  0.004: depuis,  0.004: vie\n",
      "\n",
      "=== topic #17\n",
      "0.005: estelle,  0.005: a,  0.003: mlle,  0.003: duc,  0.003: peugeot,  0.003: roy,  0.003: thérèse,  0.003: sapins,  0.003: ils,  0.003: salm\n",
      "\n",
      "=== topic #18\n",
      "0.047: ind,  0.010: p,  0.009: a,  0.006: g,  0.005: sncf,  0.005: f,  0.005: ascap,  0.004: b,  0.004: fra,  0.004: e\n",
      "\n",
      "=== topic #19\n",
      "0.013: a,  0.008: match,  0.008: plus,  0.008: deux,  0.006: équipe,  0.006: jeu,  0.006: face,  0.005: bien,  0.005: après,  0.005: première\n",
      "\n",
      "=== topic #20\n",
      "0.005: sentiers,  0.005: n°,  0.004: a,  0.004: bnp,  0.004: rond-point,  0.003: villarois,  0.003: pattes,  0.003: celles-ci,  0.003: euros,  0.003: percuté\n",
      "\n",
      "=== topic #21\n",
      "0.036: -,  0.035: 1,  0.024: 5,  0.023: 3,  0.023: h,  0.020: 6,  0.018: 4,  0.018: 2,  0.016: 30,  0.016: r\n",
      "\n",
      "=== topic #22\n",
      "0.009: f,  0.007: ht,  0.006: nancy,  0.005: vandoeuvre,  0.005: a,  0.004: 105,  0.004: mc,  0.004: seichamps,  0.004: toul,  0.004: corbeaux\n",
      "\n",
      "=== topic #23\n",
      "0.034: élèves,  0.034: école,  0.017: classe,  0.014: collège,  0.013: mme,  0.012: scolaire,  0.011: classes,  0.011: enfants,  0.011: parents,  0.010: maternelle\n",
      "\n",
      "=== topic #24\n",
      "0.054: a,  0.013: ans,  0.010: deux,  0.007: hier,  0.006: après,  0.004: avoir,  0.004: où,  0.004: trois,  0.004: fait,  0.004: comme\n",
      "\n",
      "=== topic #25\n",
      "0.027: vc,  0.011: ec,  0.010: asptt,  0.009: 2,  0.008: cc,  0.008: uc,  0.007: pontarlier,  0.007: dijon,  0.006: 3,  0.006: spinalien\n",
      "\n",
      "=== topic #26\n",
      "0.009: eau,  0.006: a,  0.005: environnement,  0.003: magazine,  0.003: nature,  0.003: insiste,  0.003: chasse,  0.003: chasseurs,  0.003: eaux,  0.002: magny\n",
      "\n",
      "=== topic #27\n",
      "0.049: 0,  0.007: a,  0.005: nadia,  0.004: vendée,  0.004: aubervilliers,  0.004: 3-0,  0.004: substitut,  0.003: avc,  0.003: lorraine,  0.003: bourgogne\n",
      "\n",
      "=== topic #28\n",
      "0.020: musique,  0.008: musiciens,  0.006: a,  0.006: chansons,  0.005: guitare,  0.005: groupe,  0.005: harmonie,  0.004: instruments,  0.004: concert,  0.004: musical\n",
      "\n",
      "=== topic #29\n",
      "0.029: a,  0.017: plus,  0.012: tout,  0.010: ils,  0.009: bien,  0.009: fait,  0.007: faire,  0.007: comme,  0.006: cette,  0.006: aussi\n",
      "\n",
      "=== topic #30\n",
      "0.005: éleveurs,  0.005: a,  0.005: 13e,  0.004: saint-max,  0.004: team,  0.004: boucher,  0.004: pâtisserie,  0.004: johan,  0.003: économies,  0.003: xertigny\n",
      "\n",
      "=== topic #31\n",
      "0.010: rue,  0.008: allée,  0.008: rues,  0.007: avenue,  0.005: a,  0.005: quai,  0.004: place,  0.003: poulet,  0.002: chaussée,  0.002: provoque\n",
      "\n",
      "=== topic #32\n",
      "0.011: chevaux,  0.010: cheval,  0.006: cavaliers,  0.005: a,  0.004: concorde,  0.004: portier,  0.004: équestre,  0.003: delestre,  0.003: défenseurs,  0.003: cavalier\n",
      "\n",
      "=== topic #33\n",
      "0.005: presbytère,  0.005: a,  0.004: fleur,  0.004: blessés,  0.004: vider,  0.003: rétrospective,  0.003: motivés,  0.003: nourriture,  0.003: bc,  0.003: dérouler\n",
      "\n",
      "=== topic #34\n",
      "0.013: nicolas,  0.012: julien,  0.008: sébastien,  0.007: david,  0.007: christophe,  0.007: julie,  0.007: aurélie,  0.007: laurent,  0.006: delphine,  0.006: céline\n",
      "\n",
      "=== topic #35\n",
      "0.008: a,  0.006: lure,  0.003: incident,  0.003: camions,  0.003: deux,  0.003: procéder,  0.003: consultation,  0.003: complément,  0.002: lourds,  0.002: vers\n",
      "\n",
      "=== topic #36\n",
      "0.006: parking,  0.006: stationnement,  0.006: a,  0.005: vierge,  0.005: places,  0.005: place,  0.003: circulation,  0.003: croissance,  0.003: rue,  0.003: lit\n",
      "\n",
      "=== topic #37\n",
      "0.174: h,  0.051: 30,  0.022: 15,  0.022: 14,  0.018: 18,  0.017: 19,  0.017: 20,  0.017: 17,  0.015: 16,  0.012: 13\n",
      "\n",
      "=== topic #38\n",
      "0.017: ans,  0.013: enfants,  0.011: famille,  0.010: a,  0.008: où,  0.008: épouse,  0.007: retraite,  0.007: âge,  0.007: né,  0.007: couple\n",
      "\n",
      "=== topic #39\n",
      "0.047: rue,  0.026: ab,  0.015: b,  0.007: cécile,  0.007: pierre,  0.006: céline,  0.005: avenue,  0.005: emilie,  0.004: marie,  0.004: laëtitia\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "filename  = 'estrepublicain_annee_1999.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename)\n",
    "# df = df[0:1000]\n",
    "\n",
    "# sampler \n",
    "\n",
    "df = df.sample(frac= 0.5)\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "df\n",
    "print(\"= dimensions\")\n",
    "print(df.shape)\n",
    "print(\"= 5 premieres rangés\")\n",
    "print(df.head())\n",
    "print(\"= Noms des colonnes\")\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "# Changer le nom de la colonne\n",
    "df.columns = ['paragraphe']\n",
    "df.columns\n",
    "\n",
    "\n",
    "# puis renommer colonne en 'text'\n",
    "\n",
    "df.columns = ['text']\n",
    "df.columns\n",
    "\n",
    "\n",
    "# Ne prendre que les 1000 premieres rangées\n",
    "\n",
    "# print(\"avant: {}\".format(df.shape))\n",
    "# df = df[0:1000]\n",
    "\n",
    "# print(\"apres: {}\".format(df.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# La phrase\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(tokens)\n",
    "tokens_on_space = sentence.split(' ')\n",
    "print(tokens_on_space)\n",
    "\n",
    "\n",
    "\n",
    "# ponctuation\n",
    "\n",
    "import string\n",
    "\n",
    "print(\"Liste original de signe de ponctuations\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "punctuation_chars = string.punctuation + \"«»\"\n",
    "\n",
    "#  Construction d'un dict { '~':' ', '$': ' ', ... }\n",
    "dict_ponctuation = {}\n",
    "for k in punctuation_chars:\n",
    "    dict_ponctuation[k] = ' '\n",
    "\n",
    "print(dict_ponctuation)\n",
    "\n",
    "# # L'operateur de translation\n",
    "translator = str.maketrans(dict_ponctuation)\n",
    "\n",
    "new_sentence = sentence.translate(translator)\n",
    "print(sentence)\n",
    "print(new_sentence)\n",
    "\n",
    "\n",
    "\n",
    "# stopwords\n",
    "tokens = word_tokenize(new_sentence)\n",
    "print(tokens)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# print(\"=== stopwords - français:\")\n",
    "# print(sorted(stopwords.words('french')))\n",
    "\n",
    "list_stopwords = stopwords.words('french') + ['les', 'de']\n",
    "\n",
    "# # equivalent:\n",
    "# stopwords = stopwords.words('french')\n",
    "# stopwords.append('les')\n",
    "# stopwords.append('de')\n",
    "\n",
    "\n",
    "# \n",
    "tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "## Equivalent \n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "\n",
    "\n",
    "\n",
    "print(tokens_sans_stopwords)\n",
    "\n",
    "\n",
    "\n",
    "# Enlevons la ponctuation\n",
    "\n",
    "df['text_no_punctuation'] = df.text.apply(lambda \n",
    "    r : ( r.translate(translator) ) \n",
    ")\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "\n",
    "df['tokens_all']  = df.text_no_punctuation.apply(\n",
    "    lambda r : word_tokenize(r.lower())\n",
    ")\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "def remove_stopword(tokens):\n",
    "     return [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "# Verifier que ca marche\n",
    "remove_stopword(tokens)\n",
    "\n",
    "# appliquer a la dataframe\n",
    "\n",
    "df['tokens'] = df.tokens_all.apply(\n",
    "    lambda tks : remove_stopword(tks) \n",
    ")\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "# enlever les colonnes intermediaires\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df = df[['text', 'tokens']]\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "# nombre de tokens par rangée\n",
    "pd.options.mode.chained_assignment = None\n",
    "df['token_count'] = df.tokens.apply( lambda r : len(r) )\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# repartition du nombre de tokens\n",
    "df.token_count.describe()\n",
    "\n",
    "\n",
    "\n",
    "# quel document a 955 tokens?\n",
    "\n",
    "condition = (df.token_count == 955)\n",
    "print(df[condition].text.values)\n",
    "\n",
    "\n",
    "\n",
    "# quel index\n",
    "print(df[condition].index)\n",
    "\n",
    "# \n",
    "condition = (df.token_count < 800)\n",
    "print(df[condition].shape)\n",
    "\n",
    "\n",
    "\n",
    "# enlever le paragraphe le plus long\n",
    "\n",
    "condition_filtrage = df.token_count < 955\n",
    "print(df[condition_filtrage].shape)\n",
    "\n",
    "df = df[condition_filtrage]\n",
    "print(df[condition_filtrage].shape)\n",
    "\n",
    "print(df.token_count.describe())\n",
    "\n",
    "\n",
    "\n",
    "# Gensim - Vocabulaire\n",
    "\n",
    "from gensim import corpora, models\n",
    "dictionary  = corpora.Dictionary(df.tokens)\n",
    "print(dictionary)\n",
    "\n",
    "\n",
    "\n",
    "# corpus_gensim\n",
    "\n",
    "df['corpus_gensim'] = df.tokens.apply(lambda d : dictionary.doc2bow(d))\n",
    "\n",
    "print(df[['text','corpus_gensim']] .head().values)\n",
    "\n",
    "\n",
    "\n",
    "corpus_gensim = [c for c in df.corpus_gensim ]\n",
    "\n",
    "\n",
    "num_topics= 10\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus_gensim,\n",
    "    id2word      = dictionary,\n",
    "    num_topics   = num_topics,\n",
    "    alpha        = 'asymmetric',\n",
    "    eta          = 'auto',\n",
    "    passes       = 2,\n",
    "    iterations   = 20\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(\"\\n=== topic #{}\".format(t[0]))\n",
    "    print(t[1].replace('*', ': ').replace(' +',', ').replace('\"',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== topic #0\n",
      "0.019: a,  0.011: plus,  0.008: cette,  0.005: tout,  0.005: deux,  0.004: jeunes,  0.004: ans,  0.004: faire,  0.004: comme,  0.003: bien\n",
      "\n",
      "=== topic #1\n",
      "0.017: a,  0.007: enfants,  0.007: plus,  0.006: ans,  0.006: ils,  0.006: tout,  0.005: cette,  0.004: tous,  0.004: comme,  0.004: où\n",
      "\n",
      "=== topic #2\n",
      "0.014: a,  0.005: cette,  0.005: plus,  0.004: être,  0.004: 000,  0.003: f,  0.003: conseil,  0.003: travaux,  0.003: projet,  0.003: entre\n",
      "\n",
      "=== topic #3\n",
      "0.030: a,  0.008: plus,  0.006: fait,  0.006: deux,  0.005: tout,  0.004: ils,  0.004: bien,  0.004: si,  0.004: comme,  0.004: ans\n",
      "\n",
      "=== topic #4\n",
      "0.031: f,  0.019: 1,  0.014: -,  0.012: ind,  0.010: 000,  0.010: a,  0.008: 2,  0.008: p,  0.007: 3,  0.006: 5\n",
      "\n",
      "=== topic #5\n",
      "0.008: a,  0.004: rue,  0.003: plus,  0.002: eau,  0.002: cette,  0.002: route,  0.002: vers,  0.002: soleil,  0.002: deux,  0.002: entre\n",
      "\n",
      "=== topic #6\n",
      "0.018: a,  0.005: maire,  0.005: ans,  0.005: conseil,  0.004: général,  0.004: président,  0.003: deux,  0.003: association,  0.003: commune,  0.003: mairie\n",
      "\n",
      "=== topic #7\n",
      "0.008: 1,  0.008: 4,  0.008: 2,  0.007: 5,  0.007: a,  0.007: b,  0.006: 3,  0.006: p,  0.006: ab,  0.005: 0\n",
      "\n",
      "=== topic #8\n",
      "0.087: h,  0.028: 30,  0.012: 15,  0.012: 14,  0.010: rue,  0.010: 20,  0.009: 18,  0.009: 19,  0.009: 17,  0.008: 16\n",
      "\n",
      "=== topic #9\n",
      "0.015: a,  0.007: deux,  0.006: plus,  0.006: équipe,  0.005: bien,  0.004: championnat,  0.004: après,  0.004: tout,  0.004: match,  0.003: première\n"
     ]
    }
   ],
   "source": [
    "num_topics= 100\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus_gensim,\n",
    "    id2word      = dictionary,\n",
    "    num_topics   = num_topics,\n",
    "    alpha        = 'asymmetric',\n",
    "    eta          = 'auto',\n",
    "    passes       = 2,\n",
    "    iterations   = 20\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(\"\\n=== topic #{}\".format(t[0]))\n",
    "    print(t[1].replace('*', ': ').replace(' +',', ').replace('\"',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c5903fc14af7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0meta\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpasses\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0miterations\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    671\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                     )\n\u001b[0;32m--> 673\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0;31m# the update for gamma gives this update. Cf. Lee&Seung 2001.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0mgammad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mphinorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElogthetad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mphinorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gensim/matutils.py\u001b[0m in \u001b[0;36mdirichlet_expectation\u001b[0;34m(alpha)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \"\"\"\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_topics= 100\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus_gensim,\n",
    "    id2word      = dictionary,\n",
    "    num_topics   = num_topics,\n",
    "    alpha        = 'asymmetric',\n",
    "    eta          = 'auto',\n",
    "    passes       = 2,\n",
    "    iterations   = 20\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(\"\\n=== topic #{}\".format(t[0]))\n",
    "    print(t[1].replace('*', ': ').replace(' +',', ').replace('\"',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
