{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "filename  = 'estrepublicain_annee_1999.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename)\n",
    "# df = df[0:1000]\n",
    "\n",
    "# sampler \n",
    "\n",
    "df = df.sample(frac= 0.5)\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "df\n",
    "print(\"= dimensions\")\n",
    "print(df.shape)\n",
    "print(\"= 5 premieres rangés\")\n",
    "print(df.head())\n",
    "print(\"= Noms des colonnes\")\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "# Changer le nom de la colonne\n",
    "df.columns = ['paragraphe']\n",
    "df.columns\n",
    "\n",
    "\n",
    "# puis renommer colonne en 'text'\n",
    "\n",
    "df.columns = ['text']\n",
    "df.columns\n",
    "\n",
    "\n",
    "# Ne prendre que les 1000 premieres rangées\n",
    "\n",
    "# print(\"avant: {}\".format(df.shape))\n",
    "# df = df[0:1000]\n",
    "\n",
    "# print(\"apres: {}\".format(df.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# La phrase\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(tokens)\n",
    "tokens_on_space = sentence.split(' ')\n",
    "print(tokens_on_space)\n",
    "\n",
    "\n",
    "\n",
    "# ponctuation\n",
    "\n",
    "import string\n",
    "\n",
    "print(\"Liste original de signe de ponctuations\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "punctuation_chars = string.punctuation + \"«»\"\n",
    "\n",
    "#  Construction d'un dict { '~':' ', '$': ' ', ... }\n",
    "dict_ponctuation = {}\n",
    "for k in punctuation_chars:\n",
    "    dict_ponctuation[k] = ' '\n",
    "\n",
    "print(dict_ponctuation)\n",
    "\n",
    "# # L'operateur de translation\n",
    "translator = str.maketrans(dict_ponctuation)\n",
    "\n",
    "new_sentence = sentence.translate(translator)\n",
    "print(sentence)\n",
    "print(new_sentence)\n",
    "\n",
    "\n",
    "\n",
    "# stopwords\n",
    "tokens = word_tokenize(new_sentence)\n",
    "print(tokens)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# print(\"=== stopwords - français:\")\n",
    "# print(sorted(stopwords.words('french')))\n",
    "\n",
    "list_stopwords = stopwords.words('french') + ['les', 'de']\n",
    "\n",
    "# # equivalent:\n",
    "# stopwords = stopwords.words('french')\n",
    "# stopwords.append('les')\n",
    "# stopwords.append('de')\n",
    "\n",
    "\n",
    "# \n",
    "tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "## Equivalent \n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "\n",
    "\n",
    "\n",
    "print(tokens_sans_stopwords)\n",
    "\n",
    "\n",
    "\n",
    "# Enlevons la ponctuation\n",
    "\n",
    "df['text_no_punctuation'] = df.text.apply(lambda \n",
    "    r : ( r.translate(translator) ) \n",
    ")\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "\n",
    "df['tokens_all']  = df.text_no_punctuation.apply(\n",
    "    lambda r : word_tokenize(r.lower())\n",
    ")\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "def remove_stopword(tokens):\n",
    "     return [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "# Verifier que ca marche\n",
    "remove_stopword(tokens)\n",
    "\n",
    "# appliquer a la dataframe\n",
    "\n",
    "df['tokens'] = df.tokens_all.apply(\n",
    "    lambda tks : remove_stopword(tks) \n",
    ")\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "# enlever les colonnes intermediaires\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df = df[['text', 'tokens']]\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "# nombre de tokens par rangée\n",
    "pd.options.mode.chained_assignment = None\n",
    "df['token_count'] = df.tokens.apply( lambda r : len(r) )\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# repartition du nombre de tokens\n",
    "df.token_count.describe()\n",
    "\n",
    "\n",
    "\n",
    "# quel document a 955 tokens?\n",
    "\n",
    "condition = (df.token_count == 955)\n",
    "print(df[condition].text.values)\n",
    "\n",
    "\n",
    "\n",
    "# quel index\n",
    "print(df[condition].index)\n",
    "\n",
    "# \n",
    "condition = (df.token_count < 800)\n",
    "print(df[condition].shape)\n",
    "\n",
    "\n",
    "\n",
    "# enlever le paragraphe le plus long\n",
    "\n",
    "condition_filtrage = df.token_count < 955\n",
    "print(df[condition_filtrage].shape)\n",
    "\n",
    "df = df[condition_filtrage]\n",
    "print(df[condition_filtrage].shape)\n",
    "\n",
    "print(df.token_count.describe())\n",
    "\n",
    "\n",
    "\n",
    "# Gensim - Vocabulaire\n",
    "\n",
    "from gensim import corpora, models\n",
    "dictionary  = corpora.Dictionary(df.tokens)\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_gensim\n",
    "\n",
    "df['corpus_gensim'] = df.tokens.apply(lambda d : dictionary.doc2bow(d))\n",
    "\n",
    "print(df[['text','corpus_gensim']] .head().values)\n",
    "\n",
    "\n",
    "\n",
    "corpus_gensim = [c for c in df.corpus_gensim ]\n",
    "\n",
    "\n",
    "num_topics= 10\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus_gensim,\n",
    "    id2word      = dictionary,\n",
    "    num_topics   = num_topics,\n",
    "    alpha        = 'asymmetric',\n",
    "    eta          = 'auto',\n",
    "    passes       = 2,\n",
    "    iterations   = 20\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(\"\\n=== topic #{}\".format(t[0]))\n",
    "    print(t[1].replace('*', ': ').replace(' +',', ').replace('\"','')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
