{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas est une  Data Analysis Library: ------> https://pandas.pydata.org/\n",
    "#on nome pandas comme pd pour agilizer l'histoire\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "filename  = 'estrepublicain_annee_1999.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>André Bauer, le Bonhomme de St-Dié ; Alain Dag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Smash Entente Club de Lunéville (SECL) a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En tout cas, du côté du PS Dole qui reste en c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>« Le nombre des donneurs était en légère baiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELOYES._ Les Ramoncenais n'auront tenu qu'une ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Heureusement, l'affaire ne relève en rien du t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Plus d'un Romarimontain sur deux, sans doute, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BESANÇON._ « C'est non ». La voix de Pierre-Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Une cinquantaine d'enfants de CE2, CM1 et CM2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Premiers à se lancer dans la course, les vétér...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L'assemblée générale de Tricot Couture Service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Aux HLM, en revanche, on a le sourire. Les prê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Le début du mois a vu le départ d'un groupe d'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Les risques ? « Comme tout ce qui vole, les mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>« Je comprends l'impatience des supporters qui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Annie Humbert a remercié l'ensemble des associ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HDL est une association au service des collect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Pendant que les diplomates s'écharpent en Macé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>L'équipe B disputera un match amical, le mardi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>En 1990, un programme expérimental d'hygiène b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>« Cette saison 98-99 a été l'année des jeunes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>OPPOSITION. Pour l'instant, le SLUC n'a rencon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Désireux de prendre sa revanche avec Dame coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>« Cette opération au Kosovo n'est pas une acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Le contact se fait tout naturellement. Assis e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Dans notre édition de lundi, il était fait éta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>- Je ne sais pas mais c'est quelqu'un qui a un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Avec l'Office du tourisme. De 11 h à 19 h, exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Tournier en vitrine. Les collégiens qui passen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Le Franch Country festival a commencé sous d'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30211</th>\n",
       "      <td>« Un jour, je suis monté dans mon grenier et j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30212</th>\n",
       "      <td>Ce n'est pas l'art qui imite la nature mais to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30213</th>\n",
       "      <td>Seule civette de Meuse, une des rares en Lorra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30214</th>\n",
       "      <td>La rétrospective s'ouvre par une série de tabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30215</th>\n",
       "      <td>Paradoxalement, les meilleures conditions de v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30216</th>\n",
       "      <td>S3 : 1. P. Billod (TCVB) 42 pts ; 2. R. Peter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30217</th>\n",
       "      <td>Ressuscitées depuis peu par le Kiwanis club de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30218</th>\n",
       "      <td>La première touche de la palette a été symboli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30219</th>\n",
       "      <td>L'association sportive des sourds de Sochaux-M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30220</th>\n",
       "      <td>En d'autres temps, semblable flambée des cours...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30221</th>\n",
       "      <td>Eddy Merckx, qui a inauguré mercredi soir le v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30222</th>\n",
       "      <td>En classe, il y avait étalés sur les tables le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30223</th>\n",
       "      <td>Raon passait alors un sale quart d'heure et on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30224</th>\n",
       "      <td>Le classement : 1. A. Dolguikh (SC Sarreguemin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30225</th>\n",
       "      <td>Gugnécourt, 16 h, hier après-midi : « T'as tou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30226</th>\n",
       "      <td>Hier matin, Guy Souhait, accompagné par Evelyn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30227</th>\n",
       "      <td>2481. Rigollot j. (Ind) ; 2482. Martinez y. (I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30228</th>\n",
       "      <td>Avant le grand départ pour les vacances, les q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30229</th>\n",
       "      <td>Avec un taux de participation de 49,42 % au sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30230</th>\n",
       "      <td>Des chiffres : les fêtes artisanales et artist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30231</th>\n",
       "      <td>Les disciples d'Asllepios, dieu mythologique g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30232</th>\n",
       "      <td>Ils ont venus avec leurs médailles, leurs drap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30233</th>\n",
       "      <td>Celui du mois d'août a connu un certain privil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30234</th>\n",
       "      <td>Punch Nancy bat Saint-Mihiel 15-12, 15-7 : Les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30235</th>\n",
       "      <td>La plupart des partants, pompiers professionne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30236</th>\n",
       "      <td>Et le rêve américain ne l'a plus quitté. « A m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30237</th>\n",
       "      <td>Deux mois après l'incendie du tunnel du Mont-B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30238</th>\n",
       "      <td>Cartons rouges aux installations sportives.- L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30239</th>\n",
       "      <td>81. Knapek (UST ADEGEM) 56'10 ; 82. Filliot (A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30240</th>\n",
       "      <td>Né à Eloyes, en 1908, issu d'une famille impla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30241 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      André Bauer, le Bonhomme de St-Dié ; Alain Dag...\n",
       "1      Le Smash Entente Club de Lunéville (SECL) a re...\n",
       "2      En tout cas, du côté du PS Dole qui reste en c...\n",
       "3      « Le nombre des donneurs était en légère baiss...\n",
       "4      ELOYES._ Les Ramoncenais n'auront tenu qu'une ...\n",
       "5      Heureusement, l'affaire ne relève en rien du t...\n",
       "6      Plus d'un Romarimontain sur deux, sans doute, ...\n",
       "7      BESANÇON._ « C'est non ». La voix de Pierre-Al...\n",
       "8      Une cinquantaine d'enfants de CE2, CM1 et CM2 ...\n",
       "9      Premiers à se lancer dans la course, les vétér...\n",
       "10     L'assemblée générale de Tricot Couture Service...\n",
       "11     Aux HLM, en revanche, on a le sourire. Les prê...\n",
       "12     Le début du mois a vu le départ d'un groupe d'...\n",
       "13     Les risques ? « Comme tout ce qui vole, les mo...\n",
       "14     « Je comprends l'impatience des supporters qui...\n",
       "15     Annie Humbert a remercié l'ensemble des associ...\n",
       "16     HDL est une association au service des collect...\n",
       "17     Pendant que les diplomates s'écharpent en Macé...\n",
       "18     L'équipe B disputera un match amical, le mardi...\n",
       "19     En 1990, un programme expérimental d'hygiène b...\n",
       "20     « Cette saison 98-99 a été l'année des jeunes ...\n",
       "21     OPPOSITION. Pour l'instant, le SLUC n'a rencon...\n",
       "22     Désireux de prendre sa revanche avec Dame coup...\n",
       "23     « Cette opération au Kosovo n'est pas une acti...\n",
       "24     Le contact se fait tout naturellement. Assis e...\n",
       "25     Dans notre édition de lundi, il était fait éta...\n",
       "26     - Je ne sais pas mais c'est quelqu'un qui a un...\n",
       "27     Avec l'Office du tourisme. De 11 h à 19 h, exc...\n",
       "28     Tournier en vitrine. Les collégiens qui passen...\n",
       "29     Le Franch Country festival a commencé sous d'e...\n",
       "...                                                  ...\n",
       "30211  « Un jour, je suis monté dans mon grenier et j...\n",
       "30212  Ce n'est pas l'art qui imite la nature mais to...\n",
       "30213  Seule civette de Meuse, une des rares en Lorra...\n",
       "30214  La rétrospective s'ouvre par une série de tabl...\n",
       "30215  Paradoxalement, les meilleures conditions de v...\n",
       "30216  S3 : 1. P. Billod (TCVB) 42 pts ; 2. R. Peter ...\n",
       "30217  Ressuscitées depuis peu par le Kiwanis club de...\n",
       "30218  La première touche de la palette a été symboli...\n",
       "30219  L'association sportive des sourds de Sochaux-M...\n",
       "30220  En d'autres temps, semblable flambée des cours...\n",
       "30221  Eddy Merckx, qui a inauguré mercredi soir le v...\n",
       "30222  En classe, il y avait étalés sur les tables le...\n",
       "30223  Raon passait alors un sale quart d'heure et on...\n",
       "30224  Le classement : 1. A. Dolguikh (SC Sarreguemin...\n",
       "30225  Gugnécourt, 16 h, hier après-midi : « T'as tou...\n",
       "30226  Hier matin, Guy Souhait, accompagné par Evelyn...\n",
       "30227  2481. Rigollot j. (Ind) ; 2482. Martinez y. (I...\n",
       "30228  Avant le grand départ pour les vacances, les q...\n",
       "30229  Avec un taux de participation de 49,42 % au sc...\n",
       "30230  Des chiffres : les fêtes artisanales et artist...\n",
       "30231  Les disciples d'Asllepios, dieu mythologique g...\n",
       "30232  Ils ont venus avec leurs médailles, leurs drap...\n",
       "30233  Celui du mois d'août a connu un certain privil...\n",
       "30234  Punch Nancy bat Saint-Mihiel 15-12, 15-7 : Les...\n",
       "30235  La plupart des partants, pompiers professionne...\n",
       "30236  Et le rêve américain ne l'a plus quitté. « A m...\n",
       "30237  Deux mois après l'incendie du tunnel du Mont-B...\n",
       "30238  Cartons rouges aux installations sportives.- L...\n",
       "30239  81. Knapek (UST ADEGEM) 56'10 ; 82. Filliot (A...\n",
       "30240  Né à Eloyes, en 1908, issu d'une famille impla...\n",
       "\n",
       "[30241 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tout les colones\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30241, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#les columns avec les rows en total\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>André Bauer, le Bonhomme de St-Dié ; Alain Dag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Smash Entente Club de Lunéville (SECL) a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En tout cas, du côté du PS Dole qui reste en c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>« Le nombre des donneurs était en légère baiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELOYES._ Les Ramoncenais n'auront tenu qu'une ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...\n",
       "1  Le Smash Entente Club de Lunéville (SECL) a re...\n",
       "2  En tout cas, du côté du PS Dole qui reste en c...\n",
       "3  « Le nombre des donneurs était en légère baiss...\n",
       "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#los entitulados\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renomer\n",
    "df.head = ['texte']\n",
    "df.columns = ['paragraphe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avant: (30241, 1)\n",
      "apres: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "#enlever des rows(de 0 a 1000)\n",
    "print(\"avant: {}\".format(df.shape))\n",
    "df = df[0:1000]\n",
    "\n",
    "print(\"apres: {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Le corpus contient 30241 rows et 1 colonne(s)\n",
      "\n",
      "Colonnes: Index(['text'], dtype='object') \n",
      "\n",
      "== Premier elements: \n",
      "[[ \"André Bauer, le Bonhomme de St-Dié ; Alain Dagosto, Bières de Vézelize (54) ; Paulette Gay, Pieds de cochons de Dommartin ; Corinne Dexemple, Tourte néocastrienne ; Bernadette Paulin, Pâté lorrain de Châtenois ; Andrée Labrux, Pétrou de Senones ; Marie-Thérèse Muller, Cochonneux de la Seille de Sillegny (57) ; Ginette Laporte, Eau de Contrexéville ; Danièle Richard, Framboise saulxuronne ; Josette Pouchucq, Cuisses de grenouilles de Vittel ; Daniel Léonard, Nostre damme de Chiney (Belgique) ; Sophie Valdenaire, Pissenlit de Xertigny ; Simone Leick, Cochon d'autrefois de Sierck les Bains (57) ; Yves Lievens, Kuulkappers de St-Gilles (Belgique) ; Brigitte Lievens, Miel de montagne de Plombières ; Louise Fallot, Madeleine de Commercy ; Claude Himbert, Macaron et bergamote de Nancy ; Robert Fuchs, Marmite d'or ; Jean-Pierre Ruspini, Saumon de Salm ; Jean-Marie Mougel, Andouille du Val d'Ajol ; Jean-François Ancel, Rognons blancs de Raon aux Bois ; Evelyne Bailleux, Fine coquille de Poseïdon de Recey sur Ource (21) ; Dominique Thaller, Tête de veau de Rambervillers ; Nicole Dubois, Boudin noir de Soissons (02) ; Martine Pattey, Image d'Epinal.\"]\n",
      " [ \"Le Smash Entente Club de Lunéville (SECL) a repris ses activités au complexe Berte. Les séances d'entraînement du mercredi soir, de 20 h à 22 h, sont ouvertes à toute personne adulte désirant pratiquer le volley-ball dans un esprit de loisir et de détente. Pour ceux ou celles qui souhaitent participer au championnat loisir, un entraînement spécifique leur est proposé le lundi soir, aux mêmes horaires. L'assemblée générale du club aura lieu le mercredi 29 septembre, à 20 h, dans la salle de réunion de la maison des sports. Tous les membres du club y sont conviés ainsi que toutes les personnes intéressées. Pour tout renseignement complémentaire, s'adresser au président J. Ravaine (03.83.74.12.28).\"]\n",
      " [ \"En tout cas, du côté du PS Dole qui reste en course pour l'accession parmi l'élite régionale après un nouveau et net succès contre Devecey (4-1), on espère bien mettre à profit ces paramètres pour une qualification dans le dernier carré : « nous aborderons cette rencontre sans aucun complexe » précisait hier le président jurassien Alain Viennot. « Nous disposons d'un groupe homogène capable de se transcender dans les grandes occasions ». Celle-ci, surtout à domicile, en est une et les Dolois comptent bien tout mettre en oeuvre au côté de son buteur maison, Euvrard (17 buts et leader des buteurs de PH-B) pour parvenir à leurs fins.\"]\n",
      " [ \"« Le nombre des donneurs était en légère baisse lors de la dernière collecte à Neuves-Maisons. C'est inquiétant car les besoins sont énormes. Au niveau départemental, les stocks sont très faibles, et ne permettent de faire face à la demande que pour une période de cinq jours au grand maximum. Que se passerait-il si un sinistre d'une gravité exceptionnelle se produisait ? Il serait sans doute impossible de répondre à la demande. C'est pourquoi, nous invitons les habitants de Neuves-Maisons à participer massivement à la prochaine collecte prévue le dimanche 5 septembre, de 8 h 30 à 12 h, au centre socio culturel Jean-L'Hôte...» explique Jean Marie Greiner, président de l'association des donneurs de sang du secteur de Neuves-Maisons.\"]\n",
      " [ \"ELOYES._ Les Ramoncenais n'auront tenu qu'une heure face aux réservistes loyas, incontestablement supérieurs techniquement et au niveau de la condition physique. Hajaji allait se montrer l'attaquant le plus remuant d'entrée de jeu (6') ou avec son compère El Khala (13') permettant à Paillard de sauver les meubles. Cela tenait, jusqu'à une belle combinaison entre les deux, plus Abou en relayeur, et Hajaji trouvait les filets (17'). On retrouvait les mêmes par la suite : El Khala (20'), Hajaji (30', 37' et 44'), souvent bien servis par le jeune Claudel. Les visiteurs s'étaient néanmoins montrés dangereux par une action Peduzzi-Blouin (10') et les raids de Martins (18' et 32').\"]]\n"
     ]
    }
   ],
   "source": [
    "#ceci viens du Doc 01 \"est...republican\"\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "filename  = 'estrepublicain_annee_1999.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename)\n",
    "\n",
    "print(\"\\nLe corpus contient {} rows et {} colonne(s)\".format(df.shape[0], df.shape[1]))\n",
    "\n",
    "print(\"\\nColonnes: {} \".format(df.columns))\n",
    "\n",
    "print(\"\\n== Premier elements: \")\n",
    "\n",
    "print(df.head().values)\n",
    "\n",
    "# Ne garder que les 1000 premiers commentaires\n",
    "\n",
    "df = df[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', 'confirment', '«', \"l'absence\", 'de', 'solution', 'parfaite', '»', '.']\n",
      "['les', 'médiateurs', 'confirment', '«', \"l'absence\", 'de', 'solution', 'parfaite', '».', '']\n"
     ]
    }
   ],
   "source": [
    "#Il faut mtn tokenizer,virer la ponctuation, et enlever les mots inutiles a l'analise\n",
    "\n",
    "#1 Tokenization<----\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# La phrase\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(tokens)\n",
    "tokens_on_space = sentence.split(' ')\n",
    "print(tokens_on_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste original de signe de ponctuations\n",
      "\t!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#2 ponctuation<----\n",
    "\n",
    "import string\n",
    "\n",
    "print(\"Liste original de signe de ponctuations\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "punctuation_chars = string.punctuation + \"«»\"\n",
    "#ici il exiten pas les «» dans la liste importe, donc il faut les ñmettre aussie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': ' ', '\"': ' ', '#': ' ', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '-': ' ', '.': ' ', '/': ' ', ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '_': ' ', '`': ' ', '{': ' ', '|': ' ', '}': ' ', '~': ' ', '«': ' ', '»': ' '}\n"
     ]
    }
   ],
   "source": [
    "#  Construction d'un dict { '~':' ', '$': ' ', ... }\n",
    "dict_ponctuation = {}\n",
    "for k in punctuation_chars:\n",
    "    dict_ponctuation[k] = ' '\n",
    "\n",
    "print(dict_ponctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les médiateurs confirment « l'absence de solution parfaite ». \n",
      "les médiateurs confirment   l absence de solution parfaite    \n"
     ]
    }
   ],
   "source": [
    "# # L'operateur de translation\n",
    "translator = str.maketrans(dict_ponctuation)\n",
    "\n",
    "new_sentence = sentence.translate(translator)\n",
    "print(sentence)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['les', 'médiateurs', 'confirment', 'l', 'absence', 'de', 'solution', 'parfaite']\n",
      "['médiateurs', 'confirment', 'absence', 'solution', 'parfaite']\n"
     ]
    }
   ],
   "source": [
    "#3 stopwords <----\n",
    "tokens = word_tokenize(new_sentence)\n",
    "print(tokens)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# print(\"=== stopwords - français:\")\n",
    "# print(sorted(stopwords.words('french')))\n",
    "\n",
    "list_stopwords = stopwords.words('french') + ['les', 'de']\n",
    "\n",
    "# # equivalent:\n",
    "# stopwords = stopwords.words('french')\n",
    "# stopwords.append('les')\n",
    "# stopwords.append('de')\n",
    "\n",
    "\n",
    "# alternative\n",
    "#from stop_words import get_stop_words\n",
    "#list_stopwords = get_stop_words(‘en’)\n",
    "# \n",
    "tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "## Equivalent \n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "\n",
    "\n",
    "\n",
    "print(tokens_sans_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>André Bauer, le Bonhomme de St-Dié ; Alain Dag...</td>\n",
       "      <td>André Bauer  le Bonhomme de St Dié   Alain Dag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Smash Entente Club de Lunéville (SECL) a re...</td>\n",
       "      <td>Le Smash Entente Club de Lunéville  SECL  a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En tout cas, du côté du PS Dole qui reste en c...</td>\n",
       "      <td>En tout cas  du côté du PS Dole qui reste en c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>« Le nombre des donneurs était en légère baiss...</td>\n",
       "      <td>Le nombre des donneurs était en légère baiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELOYES._ Les Ramoncenais n'auront tenu qu'une ...</td>\n",
       "      <td>ELOYES   Les Ramoncenais n auront tenu qu une ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
       "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
       "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
       "3  « Le nombre des donneurs était en légère baiss...   \n",
       "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
       "\n",
       "                                 text_no_punctuation  \n",
       "0  André Bauer  le Bonhomme de St Dié   Alain Dag...  \n",
       "1  Le Smash Entente Club de Lunéville  SECL  a re...  \n",
       "2  En tout cas  du côté du PS Dole qui reste en c...  \n",
       "3    Le nombre des donneurs était en légère baiss...  \n",
       "4  ELOYES   Les Ramoncenais n auront tenu qu une ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MTN on refqit mqis qvec le  CORPUS \"est....rep\" c'st donc bien de essayer dans les petite frases le coe avant de apliquer a tout le corpus\n",
    "# Enlevons la ponctuation\n",
    "\n",
    "df['text_no_punctuation'] = df.text.apply(lambda \n",
    "    r : ( r.translate(translator) ) \n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "      <th>tokens_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>André Bauer, le Bonhomme de St-Dié ; Alain Dag...</td>\n",
       "      <td>André Bauer  le Bonhomme de St Dié   Alain Dag...</td>\n",
       "      <td>[andré, bauer, le, bonhomme, de, st, dié, alai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Smash Entente Club de Lunéville (SECL) a re...</td>\n",
       "      <td>Le Smash Entente Club de Lunéville  SECL  a re...</td>\n",
       "      <td>[le, smash, entente, club, de, lunéville, secl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En tout cas, du côté du PS Dole qui reste en c...</td>\n",
       "      <td>En tout cas  du côté du PS Dole qui reste en c...</td>\n",
       "      <td>[en, tout, cas, du, côté, du, ps, dole, qui, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>« Le nombre des donneurs était en légère baiss...</td>\n",
       "      <td>Le nombre des donneurs était en légère baiss...</td>\n",
       "      <td>[le, nombre, des, donneurs, était, en, légère,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELOYES._ Les Ramoncenais n'auront tenu qu'une ...</td>\n",
       "      <td>ELOYES   Les Ramoncenais n auront tenu qu une ...</td>\n",
       "      <td>[eloyes, les, ramoncenais, n, auront, tenu, qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
       "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
       "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
       "3  « Le nombre des donneurs était en légère baiss...   \n",
       "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
       "\n",
       "                                 text_no_punctuation  \\\n",
       "0  André Bauer  le Bonhomme de St Dié   Alain Dag...   \n",
       "1  Le Smash Entente Club de Lunéville  SECL  a re...   \n",
       "2  En tout cas  du côté du PS Dole qui reste en c...   \n",
       "3    Le nombre des donneurs était en légère baiss...   \n",
       "4  ELOYES   Les Ramoncenais n auront tenu qu une ...   \n",
       "\n",
       "                                          tokens_all  \n",
       "0  [andré, bauer, le, bonhomme, de, st, dié, alai...  \n",
       "1  [le, smash, entente, club, de, lunéville, secl...  \n",
       "2  [en, tout, cas, du, côté, du, ps, dole, qui, r...  \n",
       "3  [le, nombre, des, donneurs, était, en, légère,...  \n",
       "4  [eloyes, les, ramoncenais, n, auront, tenu, qu...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "df['tokens_all']  = df.text_no_punctuation.apply(\n",
    "    lambda r : word_tokenize(r.lower())\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "      <th>tokens_all</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>André Bauer, le Bonhomme de St-Dié ; Alain Dag...</td>\n",
       "      <td>André Bauer  le Bonhomme de St Dié   Alain Dag...</td>\n",
       "      <td>[andré, bauer, le, bonhomme, de, st, dié, alai...</td>\n",
       "      <td>[andré, bauer, bonhomme, st, dié, alain, dagos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Smash Entente Club de Lunéville (SECL) a re...</td>\n",
       "      <td>Le Smash Entente Club de Lunéville  SECL  a re...</td>\n",
       "      <td>[le, smash, entente, club, de, lunéville, secl...</td>\n",
       "      <td>[smash, entente, club, lunéville, secl, a, rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En tout cas, du côté du PS Dole qui reste en c...</td>\n",
       "      <td>En tout cas  du côté du PS Dole qui reste en c...</td>\n",
       "      <td>[en, tout, cas, du, côté, du, ps, dole, qui, r...</td>\n",
       "      <td>[tout, cas, côté, ps, dole, reste, course, acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>« Le nombre des donneurs était en légère baiss...</td>\n",
       "      <td>Le nombre des donneurs était en légère baiss...</td>\n",
       "      <td>[le, nombre, des, donneurs, était, en, légère,...</td>\n",
       "      <td>[nombre, donneurs, légère, baisse, lors, derni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELOYES._ Les Ramoncenais n'auront tenu qu'une ...</td>\n",
       "      <td>ELOYES   Les Ramoncenais n auront tenu qu une ...</td>\n",
       "      <td>[eloyes, les, ramoncenais, n, auront, tenu, qu...</td>\n",
       "      <td>[eloyes, ramoncenais, tenu, heure, face, réser...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
       "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
       "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
       "3  « Le nombre des donneurs était en légère baiss...   \n",
       "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
       "\n",
       "                                 text_no_punctuation  \\\n",
       "0  André Bauer  le Bonhomme de St Dié   Alain Dag...   \n",
       "1  Le Smash Entente Club de Lunéville  SECL  a re...   \n",
       "2  En tout cas  du côté du PS Dole qui reste en c...   \n",
       "3    Le nombre des donneurs était en légère baiss...   \n",
       "4  ELOYES   Les Ramoncenais n auront tenu qu une ...   \n",
       "\n",
       "                                          tokens_all  \\\n",
       "0  [andré, bauer, le, bonhomme, de, st, dié, alai...   \n",
       "1  [le, smash, entente, club, de, lunéville, secl...   \n",
       "2  [en, tout, cas, du, côté, du, ps, dole, qui, r...   \n",
       "3  [le, nombre, des, donneurs, était, en, légère,...   \n",
       "4  [eloyes, les, ramoncenais, n, auront, tenu, qu...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [andré, bauer, bonhomme, st, dié, alain, dagos...  \n",
       "1  [smash, entente, club, lunéville, secl, a, rep...  \n",
       "2  [tout, cas, côté, ps, dole, reste, course, acc...  \n",
       "3  [nombre, donneurs, légère, baisse, lors, derni...  \n",
       "4  [eloyes, ramoncenais, tenu, heure, face, réser...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "\n",
    "def remove_stopword(tokens):\n",
    "     return [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "# Verifier que ca marche\n",
    "remove_stopword(tokens)\n",
    "\n",
    "# appliquer a la dataframe\n",
    "\n",
    "df['tokens'] = df.tokens_all.apply(\n",
    "    lambda tks : remove_stopword(tks) \n",
    ")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'text_no_punctuation', 'tokens_all', 'tokens'], dtype='object')\n",
      "Index(['text', 'tokens'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# enlever les colonnes intermediaires\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df = df[['text', 'tokens']]\n",
    "\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  André Bauer, le Bonhomme de St-Dié ; Alain Dag...   \n",
      "1  Le Smash Entente Club de Lunéville (SECL) a re...   \n",
      "2  En tout cas, du côté du PS Dole qui reste en c...   \n",
      "3  « Le nombre des donneurs était en légère baiss...   \n",
      "4  ELOYES._ Les Ramoncenais n'auront tenu qu'une ...   \n",
      "\n",
      "                                              tokens  token_count  \n",
      "0  [andré, bauer, bonhomme, st, dié, alain, dagos...          130  \n",
      "1  [smash, entente, club, lunéville, secl, a, rep...           74  \n",
      "2  [tout, cas, côté, ps, dole, reste, course, acc...           69  \n",
      "3  [nombre, donneurs, légère, baisse, lors, derni...           72  \n",
      "4  [eloyes, ramoncenais, tenu, heure, face, réser...           71  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean       80.729000\n",
       "std        37.439301\n",
       "min        44.000000\n",
       "25%        64.000000\n",
       "50%        73.000000\n",
       "75%        87.000000\n",
       "max       955.000000\n",
       "Name: token_count, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nombre de tokens par rangée\n",
    "pd.options.mode.chained_assignment = None\n",
    "df['token_count'] = df.tokens.apply( lambda r : len(r) )\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# repartition du nombre de tokens\n",
    "df.token_count.describe()\n",
    "\n",
    "#count est egale aux element, faire attention car on peur se tronmper!!!!!\n",
    "# les reste des donnes sont en relation a la distrubution des tokens par raport aux lignes, sa distribution a partir de la mediane, a.k.a. on travaille avec la variabilité "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \"161. Brisset (CSA BA 133) ; 162. Tascone ; 163. Leroy (SNCF) ; 164. Cassin (Tomblaine) ; 165. Aubert (Frouard) ; 166. Pascaot ; 167. Lagrange (Messein) ; 168. Vermandé (Messein) ; 169. Rouyer (Commercy) ; 170. Chrisment (Nancy) ; 171. Lepeltier (Ecrouves) ; 172. Jacquot ; 173. Delery ; 174. Billy ; 175. Lamy ; 176. Charpentier (Ludres) ; 177. Tousch (Gondrecourt) ; 178. Carbillet (La Poste) ; 179. Brunot (Philips) ; 180. Benyoussef ; 181. Starosse (Allamps) ; 182. Seiler (Vandoeuvre) ; 183.Discristofano ; 184. Fossard (Vandoeuvre) ; 185. Boyer ; 186. Leroy ; 187. Carreiras (Golbey) ; 188. Evrard ; 189. Vagne (CSA BA 133) ; 190. Georges (Tomblaine) ; 191. Aubert (Maidières) ; 192. Leroux ; 193. Geoffroy ; 194. Clausse (Chaudenay) ; 195. Deuze (Villers) ; 196. P. Knapek (Ecrouves) ; 197. F. Kanpek (Toul) ; 198. Noël ; 199. Maire (Varangéville) ; 200. Mercky ; 201. Gaumer ; 202. Vincent (Sommerviller) ; 203. Socha (Aingeray) ; 204. Boulot (Blénod) ; 205. Houin (Amicale CHU) ; 206. Borgniet (Jarville) ; 207. Cazzulani (Bulligny) ; 208. Parisot (Lenoncourt) ; 209. Baron (Gondreville) ; 210. Marchal . 211. Ingret (Essey-lès-Nancy) ; 212. Leonardi (Saizerais) ; 213. Bar (St-Max) ; 214. Labarbe (Nancy) ; 215. Picart (Villers) ; 216. Pageot (Essey-lès-Nancy) ; 217. Guillou (Art-sur-Meurthe) ; 218. Bourgeois (Gondrecourt) ; 219. Sutter (Maxéville) ; 220. Marchi (Vandoeuvre) ; 221. Gonthier ; 222. Girardi ; 223. Heckel (Maxéville) ; 224. Lee (Philips) ; 225. Compagnon (Richardménil) ; 226. Obeltz (Gars Val Cross) ; 227. Othelet (Tantonville) ; 228. Kovac (Pexonne) ; 229. T. Godfroy ; 230. Barin . 231. Payeur (Chaudenay) ; 232. Baraud (Thierville) ; 233. Pochit ; 234. Antoine ; 235. Peiffer (Chaudenay) ; 236. R. Deloy (Nomexy) ; 237. G. Deloy ; 238. Milbach ; 239. Andre (Benney) ; 240. Petitjean ; 241. Perrin (Lunéville) ; 242. Pernel (Blénod) ; 243. Ferreira ; 244. Alfieri (Laneuveville) ; 245. Hutinet ; 246. Defente (Blénod) ; 247. Villaume ; 248. Gérard (Nves-Maisons) ; 249. Payeur (Commercy) ; 250. Bournon ; 251. Collet (Philips) ; 252. Ludmann (Hériménil) ; 253. Vallance (Goviller) ; 254. Belin (Vandoeuvre) ; 255. Verd ; 256. Renard . 257. Bastien ; 258. Hincelin ; 259. Roger (Essey-lès-Nancy) ; 260. Rakotoarison (St-Max) ; 261. Deshayes (CSA BA 133) ; 262. Debias ; 263. Mouillé (Tomblaine) ; 264. Remy (Nancy) ; 265. Baechler (Dombasle) ; 266. Maillet ; 267. Saint Jours ; 268. Pierre (Blénod) ; 269. Morisot . 270. Serveur (Essey-lès-Nancy) ; 271. Fritz ; 272. Mangenot ; 273. Butin (Toul) ; 274. J. Etienne ; 275. D. Etienne ; 276. Mathis ; 277. Boyer (Gondrecourt) ; 278. Mandra (Gars Val Cross) ; 279. Belhote (Pulnoy) ; 280. Alison (Nimes) ; 281. Liegey (Varangéville) ; 282. Pierrat (Spiridon Club) ; 283. Humbert (Seichamps) ; 284. Back (Bouzonville) ; 285. Dewitte ; 286. Philippe (Thiébauménil) ; 287. Boyer ; 288. Durand (Houdemont) ; 289. Thieblemont ; 290. Baumann (Boncourt) ; 291. Tabouret (Toul) ; 292. Husson (Blénod) ; 293. Emmanuel (Damelevieres) ; 294. Pernot ; 295. Schwoerer (Nancy) ; 296. Sitz (Dieulouard) ; 297. Chuste ; 298. Laithier (Nancy) ; 299. Abscheidt ; 300. Pagliarela (Philips) ; 301. Tardy (Golbey) ; 302. Koffolt (Vincey) ; 303. Chassatte (Dombasle) ; 304. Vaxelaire ; 305. Robert (Nancy) ; 306. Chretien (Villers) ; 307. Paulin (Belleville) ; 308. Morville (Villers) ; 309. Ziegler (Bataville) ; 310. Morville (Gondrecourt) ; 311. Nowakowski (SNCF) ; 312. Fourar (Vandoeuvre) ; 313. Osiewiez (Energ.Pavois) ; 314. Dominiak (Vandoeuvre) ; 315. Ricatte (Lunéville) ; 316. Faltot (Lunéville) ; 317. Candat ; 318. Bardot (Lunéville) ; 319. Martin . 320. Ferry ; 321. Mathieu (Nancy) ; 322. Chenin ; 323. Geoffroy (AFPA) ; 324. Filliot (AFPA) ; 325. Grapinet ; 326. Pelon (Laquenexy) ; 327. Tournoy (Laneuveville) ; 328. Picazo ; 329. Poirot (Velaine-en-Haye) ; 330. Lostetier ; 331. Zamboni (Varangéville) ; 332. Denat ; 333. Charpentier (Ludres) ; 334. Michel ; 335. Dolveck (Blénod) ; 336. Buchi ; 337. Berteaux ; 338. Legast (Chaudenay) ; 339. Pawlowski (Dieulouard) ; 340. Reinhard (Langley) ; 341. Moine (Heillecourt) ; 342. Carpentier (SNCF) ; 343. Liegeois (Ars-sur-Moselle) ; 344. Gousse (Villers) ; 345. Poinsard (Lunéville) ; 346. Dimarcq ; 347. Blaise (Blénod) ; 348. Desgranges (Gondreville) ; 349. Falchetto ; 350. Flament (Méréville) ; 351. Heloir (Messein) ; 352. Arson (Gars Val Cross) ; 353. Richard (Einvaux) ; 354. Mercier (Rosières-aux-Salines) ; 355. Pernossi (Alstom Moteurs) ; 356. Poncet (Vincey) ; 357. Wolfarth (Malleloy) ; 358. Toldre (Laneuveville) ; 359. Touillet (Nancy) ; 360. Brice (Spiridon Club) ; 361. Seyer (Metz) ; 362. Mauclair (Lunéville) ; 363. Bourtembourg (Saulxures-lès-Nancy) ; 364. Gallois ; 365. Leroy (Blénod) ; 366. Schneider (Pagny-sur-Moselle) ; 367. Pierre (Mont-sur-Meurthe) ; 368. Vachon . 369. Lorphelin (Laneuville) ; 370. Thil ; 371. Lafont (Nancy) ; 372. Lunéville) ; 373. Martinez (La Poste) ; 374. Drahon (Toul) ; 375. Cuny (Gars Val Cross) ; 376. Rousseau (Malzéville) ; 377. Vernay (PAM) ; 378. Borgniet (Jarville) ; 379. Caballero (Ecrouves) ; 380. Hacquard (Champigneulles) ; 381. Remy (Neuves-Maisons) ; 382. Ingret (Essey-lès-Nancy) ; 383. Liegeois (Ars-sur-Moselle) ; 384. Thivet . 385. Deutzer (Philips) ; 386. Zminka (Toul) ; 387. François (Energ.Pavois) ; 388. Charrue (Paris) ; 389. Mercier (Bouxières) ; 390. Leclere ; 391. Bourguignon (Blainville) ; 392. Durand (Gars Val Cross) ; 393. Maron (Custines) ; 394. Huttier ; 395. Hesse (Bicquelet) ; 396. Moine (Heillecourt) ; 397. Boul ; 398. Evezard ; 399. Charpentier (Ludres) ; 400. Melich ; 401. Allard ; 402. Richard ; 403. Steck (Lucey) ; 404. Flament (Richardménil) ; 405. Genot (Heillecourt) ; 406. Mougel (Ludres) ; 407. Richard (Sports Loisirs) ; 408. Baillot ; 409. S. Bour (Heillecourt) ; 410. A. Bour (Heillecourt) ; 411. Lance ; 412. Melchior (Laxou) ; 413. Haut (Metz) ; 414. Bruant (Tomblaine) ; 415. Barbette ; 416. Chassatte (Dombasle) ; 417. Joublin ; 418. Liegey (Varangéville) ; 419. Marchal ; 420. Henard ; 421. Mouchet (Sport Loisirs) ; 422. Stephan (Pulnoy) ; 423. Gauchey (Dommartin-lès-Toul) ; 424. Geoffroy ; 425. Gallois (Maxéville) ; 426. Chapelle (Golbey) ; 427. Serveur (Essey-lès-Ncy) ; 428. Piazuelo (Toul) ; 429. Arson (Gars Val Cross) ; 430. Sitz (Dieulouard) ; 431. Poinsignon (Seichamps) ; 432. Thirion ; 433. Larbaletrier (Malzéville) ; 434. Desloges (Blénod) ; 435. Pelte (Réméréville) ; 436. Simon ; 437. Keniche (Nancy) ; 438. Othelet ; 439. Marchal ; 440. Belin (Vandoeuvre) ; 441. Carpentier (SNCF) ; 442. Heloir (Messein) ; 443. Borgniet (Laneuveville) ; 444. L'Huillier ; 445. Boes (Menaucourt) ; 446. Morlon (Jarville) ; 447. Dieudonne (Varangéville) ; 448. Prignot (Villers-lès-Ncy) ; 449. Ihry (Neuves-Maisons) ; 450. Lottin (Laferte-sur-Am.) ; 451. Lotto ; 452. Durand (Gars Val Cross) ; 453. C. Larbaletrier (CHU Nancy) ; 454. Mathis (Rosières-aux-Salines) ; 455. Schatzbe ; 456. Dannequin ; 457. Cael ; 458. Genelot ; 459. Rahnena (Ludres) ; 460. Montovani ; 461. Philippot ; 462. Schertz ; 463. Barozzi (Nancy) ; 464. Simon (Seichamps) ; 465. Heinrich ; 466. Deny (Neuves-Maisons) ; 467. Varnerot (Dommartin-lès-Toul) ; 468. Deveze ; 469. Bicquelet ; 470. Haut (Metz) ; 471. Rouyer (AFPA) ; 472. Pierot (Sport Loisir) ; 473. Legroux (AFPA) ; 474. Voitot (Toul) ; 475. Fourcaulx (Fléville) ; 476. Dubail (Neugartheim) ; 477. G. Dannequin ; 478. D. Bicquelet ; 479. Martin ; 480. Durand (Gars Val Cross) ; 481. Tekieli (Rosières-aux-Salines) ; 482. Marc (Ars-sur-Moselle) ; 483. Thivet ; 484. Gauchey (Dommartin-lès-Toul) ; 485. Genelot ; 486. Picot (Jarville) ; 487. Moriset.\"]\n"
     ]
    }
   ],
   "source": [
    "# On voit que AU MOINS un document a 955 token, il faut le cherche. On veut savoir les valeurs extremes car on se trouve dans des tecniques de analyse base tus la distribution normale. \n",
    "#Pour les apliquer il nous faut idealement une distrubution normale---> este vergueo es como las rgresiones, qedemos \"distribucion normale, pero al final a la mara le pela el niessss\n",
    "\n",
    "condition = (df.token_count == 955)\n",
    "print(df[condition].text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([311], dtype='int64')\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(5, 3)\n",
      "(139, 3)\n"
     ]
    }
   ],
   "source": [
    "#de maniere exploratire on veut voir les cas extremes\n",
    "# quel index\n",
    "print(df[condition].index)\n",
    "\n",
    "# \n",
    "condition = (df.token_count > 500)\n",
    "print(df[condition].shape)\n",
    "condition1 = (df.token_count > 400)\n",
    "print(df[condition1].shape)\n",
    "condition2 = (df.token_count > 300)\n",
    "print(df[condition2].shape)\n",
    "condition3 = (df.token_count > 200)\n",
    "print(df[condition3].shape)\n",
    "condition4 = (df.token_count > 100)\n",
    "print(df[condition4].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 3)\n",
      "(999, 3)\n",
      "count    999.000000\n",
      "mean      79.853854\n",
      "std       25.228111\n",
      "min       44.000000\n",
      "25%       64.000000\n",
      "50%       73.000000\n",
      "75%       87.000000\n",
      "max      275.000000\n",
      "Name: token_count, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexis/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# enlever le paragraphe le plus long\n",
    "\n",
    "condition_filtrage = df.token_count < 955\n",
    "print(df[condition_filtrage].shape)\n",
    "\n",
    "df = df[condition_filtrage]\n",
    "print(df[condition_filtrage].shape)\n",
    "\n",
    "print(df.token_count.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(19325 unique tokens: ['andré', 'bauer', 'bonhomme', 'st', 'dié']...)\n"
     ]
    }
   ],
   "source": [
    "# Gensim - Vocabulaire\n",
    "\n",
    "from gensim import corpora, models\n",
    "dictionary  = corpora.Dictionary(df.tokens)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ list([(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 3), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1)])\n",
      " list([(123, 1), (124, 1), (125, 3), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 2), (135, 2), (136, 2), (137, 2), (138, 3), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 2), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1)])\n",
      " list([(5, 1), (131, 1), (168, 1), (177, 2), (181, 1), (188, 1), (189, 2), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1), (206, 1), (207, 2), (208, 2), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1), (247, 1)])\n",
      " list([(28, 1), (85, 2), (138, 2), (154, 1), (165, 1), (181, 1), (186, 1), (217, 1), (248, 1), (249, 2), (250, 1), (251, 1), (252, 1), (253, 1), (254, 2), (255, 3), (256, 3), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 2), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1), (299, 1), (300, 1), (301, 1), (302, 1)])\n",
      " list([(137, 1), (159, 1), (207, 1), (239, 1), (261, 1), (268, 1), (293, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 3), (315, 1), (316, 1), (317, 1), (318, 2), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 2), (325, 2), (326, 1), (327, 1), (328, 1), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 1), (341, 1), (342, 1), (343, 1), (344, 1), (345, 1), (346, 1), (347, 1), (348, 1), (349, 1), (350, 1), (351, 1), (352, 1), (353, 1), (354, 1), (355, 1), (356, 1), (357, 1), (358, 1), (359, 1), (360, 1), (361, 1)])]\n"
     ]
    }
   ],
   "source": [
    "# corpus_gensim\n",
    "\n",
    "df['corpus_gensim'] = df.tokens.apply(lambda d : dictionary.doc2bow(d))\n",
    "\n",
    "print(df['corpus_gensim'] .head().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggreger les rows\n",
    "corpus_gensim = [c for c in df.corpus_gensim ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics= 40\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus_gensim,\n",
    "    id2word      = dictionary,\n",
    "    num_topics   = num_topics,\n",
    "    alpha        = 'asymmetric',\n",
    "    eta          = 'auto',\n",
    "    passes       = 2,\n",
    "    iterations   = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== topic #0\n",
      "0.014: a,  0.007: plus,  0.004: ils,  0.003: deux,  0.003: h,  0.003: peu,  0.002: avant,  0.002: autre,  0.002: tout,  0.002: communes\n",
      "\n",
      "=== topic #1\n",
      "0.010: a,  0.008: plus,  0.005: ils,  0.004: tous,  0.004: tout,  0.003: comme,  0.003: ans,  0.003: bien,  0.003: où,  0.003: après\n",
      "\n",
      "=== topic #2\n",
      "0.015: a,  0.015: h,  0.008: ans,  0.006: 30,  0.004: plus,  0.004: fait,  0.004: 1,  0.004: cette,  0.004: 2,  0.004: 15\n",
      "\n",
      "=== topic #3\n",
      "0.020: a,  0.004: ind,  0.004: cette,  0.004: plus,  0.003: jean,  0.003: comme,  0.003: enfants,  0.003: où,  0.003: tout,  0.003: ans\n",
      "\n",
      "=== topic #4\n",
      "0.015: a,  0.006: deux,  0.004: plus,  0.004: sous,  0.004: ans,  0.004: cette,  0.003: après,  0.003: être,  0.003: depuis,  0.003: année\n",
      "\n",
      "=== topic #5\n",
      "0.009: a,  0.007: plus,  0.004: ans,  0.004: 9,  0.004: 7,  0.004: retraite,  0.004: 6,  0.003: 8,  0.003: tout,  0.003: 5\n",
      "\n",
      "=== topic #6\n",
      "0.012: a,  0.004: f,  0.004: comme,  0.004: plus,  0.004: tout,  0.003: ans,  0.003: cette,  0.003: très,  0.003: bien,  0.003: sans\n",
      "\n",
      "=== topic #7\n",
      "0.009: a,  0.003: 3,  0.003: deux,  0.003: 4,  0.003: tout,  0.002: après,  0.002: plus,  0.002: où,  0.002: ans,  0.002: jean\n",
      "\n",
      "=== topic #8\n",
      "0.007: h,  0.006: a,  0.005: saint,  0.004: plus,  0.004: jean,  0.003: 2,  0.003: ils,  0.003: 14,  0.003: moins,  0.002: tout\n",
      "\n",
      "=== topic #9\n",
      "0.014: a,  0.005: née,  0.004: ans,  0.004: h,  0.004: deux,  0.004: bien,  0.003: lorraine,  0.003: fait,  0.003: tous,  0.003: depuis\n",
      "\n",
      "=== topic #10\n",
      "0.013: a,  0.004: fait,  0.003: plus,  0.003: après,  0.003: cette,  0.003: deux,  0.003: première,  0.002: si,  0.002: faire,  0.002: jusqu\n",
      "\n",
      "=== topic #11\n",
      "0.012: a,  0.009: plus,  0.007: h,  0.006: cette,  0.005: ils,  0.004: 30,  0.003: aussi,  0.003: tout,  0.002: 3,  0.002: 18\n",
      "\n",
      "=== topic #12\n",
      "0.009: h,  0.008: a,  0.006: 13,  0.005: 15,  0.004: ils,  0.004: f,  0.004: 3,  0.004: 14,  0.003: 10,  0.003: asptt\n",
      "\n",
      "=== topic #13\n",
      "0.020: rue,  0.014: a,  0.005: place,  0.004: centre,  0.004: plus,  0.004: direction,  0.003: ans,  0.003: fait,  0.003: h,  0.003: bains\n",
      "\n",
      "=== topic #14\n",
      "0.018: a,  0.009: ans,  0.005: plus,  0.004: deux,  0.004: après,  0.003: comme,  0.003: 6,  0.003: jean,  0.003: bien,  0.003: camp\n",
      "\n",
      "=== topic #15\n",
      "0.008: h,  0.005: a,  0.003: 5,  0.003: 6,  0.003: julien,  0.003: coupe,  0.002: tout,  0.002: 4,  0.002: jean,  0.002: da\n",
      "\n",
      "=== topic #16\n",
      "0.014: a,  0.007: h,  0.007: 4,  0.005: rue,  0.004: deux,  0.003: tout,  0.003: plus,  0.003: bien,  0.003: fait,  0.003: ans\n",
      "\n",
      "=== topic #17\n",
      "0.017: 6,  0.013: a,  0.007: plus,  0.007: bat,  0.006: ans,  0.005: 3,  0.004: 5,  0.004: 7,  0.004: 4,  0.003: 2\n",
      "\n",
      "=== topic #18\n",
      "0.043: h,  0.009: 14,  0.009: a,  0.009: 30,  0.006: 15,  0.005: 18,  0.004: 13,  0.004: 17,  0.004: 16,  0.004: ans\n",
      "\n",
      "=== topic #19\n",
      "0.013: a,  0.007: f,  0.006: cette,  0.004: plus,  0.003: tout,  0.003: h,  0.003: enfants,  0.003: tous,  0.003: pts,  0.002: être\n",
      "\n",
      "=== topic #20\n",
      "0.012: a,  0.007: h,  0.005: cette,  0.003: plus,  0.003: 30,  0.003: bien,  0.002: comme,  0.002: si,  0.002: fait,  0.002: sans\n",
      "\n",
      "=== topic #21\n",
      "0.007: a,  0.005: h,  0.004: deux,  0.004: tous,  0.003: 7,  0.003: cette,  0.003: tout,  0.003: mp,  0.003: vendredi,  0.003: f\n",
      "\n",
      "=== topic #22\n",
      "0.010: a,  0.003: h,  0.003: cette,  0.003: tout,  0.003: après,  0.003: comme,  0.002: trois,  0.002: aussi,  0.002: plus,  0.002: ans\n",
      "\n",
      "=== topic #23\n",
      "0.005: a,  0.003: fait,  0.002: h,  0.002: deux,  0.002: nancy,  0.002: 2,  0.002: très,  0.002: plus,  0.002: cette,  0.002: jarville\n",
      "\n",
      "=== topic #24\n",
      "0.010: h,  0.009: ab,  0.007: a,  0.005: plus,  0.004: b,  0.003: 14,  0.003: ans,  0.003: cette,  0.002: 2,  0.002: deux\n",
      "\n",
      "=== topic #25\n",
      "0.019: a,  0.010: plus,  0.004: cette,  0.003: tout,  0.003: jean,  0.003: deux,  0.003: ans,  0.003: bien,  0.003: tour,  0.003: après\n",
      "\n",
      "=== topic #26\n",
      "0.012: a,  0.009: h,  0.007: deux,  0.004: née,  0.004: ans,  0.004: 30,  0.004: 13,  0.003: enfants,  0.003: plus,  0.003: 15\n",
      "\n",
      "=== topic #27\n",
      "0.014: a,  0.008: h,  0.006: cette,  0.004: aussi,  0.003: tout,  0.003: plus,  0.003: après,  0.003: ville,  0.003: place,  0.003: sans\n",
      "\n",
      "=== topic #28\n",
      "0.007: a,  0.005: ans,  0.004: tout,  0.003: deux,  0.002: très,  0.002: jeunes,  0.002: plus,  0.002: aussi,  0.002: également,  0.002: cette\n",
      "\n",
      "=== topic #29\n",
      "0.011: a,  0.004: plus,  0.004: h,  0.003: ans,  0.003: deux,  0.002: après,  0.002: autres,  0.002: sans,  0.002: côté,  0.002: tout\n",
      "\n",
      "=== topic #30\n",
      "0.008: a,  0.004: ans,  0.003: cette,  0.003: où,  0.003: saint,  0.003: deux,  0.003: hier,  0.002: rue,  0.002: être,  0.002: ils\n",
      "\n",
      "=== topic #31\n",
      "0.016: a,  0.010: f,  0.006: deux,  0.004: ans,  0.004: 500,  0.003: personnes,  0.003: jean,  0.003: cette,  0.003: trois,  0.003: dont\n",
      "\n",
      "=== topic #32\n",
      "0.008: a,  0.007: h,  0.005: plus,  0.003: place,  0.003: cette,  0.003: ans,  0.003: équipe,  0.003: comme,  0.003: bien,  0.003: ville\n",
      "\n",
      "=== topic #33\n",
      "0.015: a,  0.006: ils,  0.005: plus,  0.005: deux,  0.004: tout,  0.003: fait,  0.003: entre,  0.003: comme,  0.002: heures,  0.002: cette\n",
      "\n",
      "=== topic #34\n",
      "0.014: a,  0.004: plus,  0.003: cette,  0.003: deux,  0.003: fait,  0.003: francs,  0.002: saison,  0.002: tout,  0.002: 000,  0.002: temps\n",
      "\n",
      "=== topic #35\n",
      "0.020: a,  0.007: ind,  0.005: f,  0.005: h,  0.005: cette,  0.003: tout,  0.003: plus,  0.003: ville,  0.003: rue,  0.003: jeunes\n",
      "\n",
      "=== topic #36\n",
      "0.065: h,  0.013: 30,  0.010: a,  0.009: r,  0.006: 45,  0.006: 1,  0.006: journal,  0.006: 21,  0.006: 20,  0.005: 13\n",
      "\n",
      "=== topic #37\n",
      "0.009: a,  0.007: rue,  0.004: plus,  0.004: deux,  0.003: partie,  0.003: si,  0.003: comme,  0.003: jean,  0.003: aussi,  0.003: entre\n",
      "\n",
      "=== topic #38\n",
      "0.011: a,  0.007: plus,  0.004: comme,  0.004: bien,  0.004: place,  0.003: rue,  0.003: très,  0.003: fait,  0.003: où,  0.002: h\n",
      "\n",
      "=== topic #39\n",
      "0.009: a,  0.005: dimanche,  0.003: deux,  0.003: samedi,  0.002: fait,  0.002: saint,  0.002: décembre,  0.002: vente,  0.002: après,  0.002: plus\n"
     ]
    }
   ],
   "source": [
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(\"\\n=== topic #{}\".format(t[0]))\n",
    "    print(t[1].replace('*', ': ').replace(' +',', ').replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== topic #0\n",
      "0.010: a,  0.006: tout,  0.006: h,  0.005: ils,  0.004: cette,  0.003: rue,  0.003: place,  0.003: autres,  0.003: plus,  0.003: être\n",
      "\n",
      "=== topic #1\n",
      "0.010: a,  0.006: plus,  0.004: fait,  0.004: h,  0.003: ans,  0.003: cette,  0.003: après,  0.003: place,  0.003: président,  0.003: aussi\n",
      "\n",
      "=== topic #2\n",
      "0.019: a,  0.006: cette,  0.006: plus,  0.005: h,  0.005: fait,  0.004: tout,  0.004: comme,  0.004: très,  0.004: bien,  0.003: ans\n",
      "\n",
      "=== topic #3\n",
      "0.023: a,  0.004: plus,  0.004: tout,  0.004: pierre,  0.003: cette,  0.003: encore,  0.003: ans,  0.003: deux,  0.003: où,  0.003: ils\n",
      "\n",
      "=== topic #4\n",
      "0.029: h,  0.011: a,  0.011: 30,  0.005: r,  0.005: cette,  0.005: 21,  0.004: 18,  0.004: 20,  0.004: plus,  0.004: 17\n",
      "\n",
      "=== topic #5\n",
      "0.012: a,  0.005: cette,  0.005: ils,  0.004: ville,  0.003: tous,  0.003: plus,  0.003: bien,  0.003: entre,  0.003: autres,  0.003: occasion\n",
      "\n",
      "=== topic #6\n",
      "0.046: h,  0.015: 30,  0.010: 14,  0.009: 15,  0.008: dimanche,  0.007: a,  0.007: 13,  0.006: samedi,  0.006: 18,  0.005: match\n",
      "\n",
      "=== topic #7\n",
      "0.033: a,  0.007: plus,  0.005: ans,  0.004: née,  0.003: être,  0.003: deux,  0.003: enfants,  0.003: comme,  0.003: tout,  0.003: depuis\n",
      "\n",
      "=== topic #8\n",
      "0.013: a,  0.006: plus,  0.005: aussi,  0.004: président,  0.003: jean,  0.003: depuis,  0.003: peu,  0.003: premier,  0.002: être,  0.002: bien\n",
      "\n",
      "=== topic #9\n",
      "0.020: h,  0.017: a,  0.006: plus,  0.006: deux,  0.005: ans,  0.004: après,  0.004: 30,  0.004: 14,  0.003: tout,  0.003: cette\n",
      "\n",
      "=== topic #10\n",
      "0.020: a,  0.008: ans,  0.006: plus,  0.006: cette,  0.005: deux,  0.004: tout,  0.004: temps,  0.003: tous,  0.003: faire,  0.003: aussi\n",
      "\n",
      "=== topic #11\n",
      "0.007: h,  0.007: a,  0.006: plus,  0.003: saint,  0.003: effet,  0.003: 1,  0.003: cette,  0.003: alors,  0.002: deux,  0.002: 4\n",
      "\n",
      "=== topic #12\n",
      "0.012: a,  0.007: cette,  0.004: plus,  0.004: comme,  0.004: être,  0.003: après,  0.003: où,  0.003: tout,  0.003: année,  0.003: également\n",
      "\n",
      "=== topic #13\n",
      "0.016: a,  0.007: plus,  0.005: ans,  0.004: ils,  0.004: deux,  0.004: comme,  0.004: bien,  0.004: rue,  0.004: place,  0.003: sous\n",
      "\n",
      "=== topic #14\n",
      "0.012: h,  0.009: a,  0.006: ans,  0.005: 8,  0.005: 7,  0.004: ville,  0.004: vendredi,  0.004: deux,  0.004: 1,  0.004: 30\n",
      "\n",
      "=== topic #15\n",
      "0.009: a,  0.008: plus,  0.004: deux,  0.004: cette,  0.003: centre,  0.003: place,  0.003: heures,  0.003: fait,  0.003: sans,  0.003: sous\n",
      "\n",
      "=== topic #16\n",
      "0.010: a,  0.005: h,  0.005: deux,  0.004: ans,  0.004: jean,  0.004: plus,  0.003: ils,  0.003: centre,  0.003: hier,  0.003: cette\n",
      "\n",
      "=== topic #17\n",
      "0.016: a,  0.009: plus,  0.005: f,  0.005: comme,  0.004: ind,  0.003: après,  0.003: bien,  0.003: tout,  0.003: ans,  0.003: faire\n",
      "\n",
      "=== topic #18\n",
      "0.016: h,  0.009: ans,  0.008: a,  0.007: 6,  0.006: 2,  0.005: 30,  0.005: deux,  0.005: plus,  0.005: fait,  0.005: 9\n",
      "\n",
      "=== topic #19\n",
      "0.015: a,  0.007: plus,  0.006: mp,  0.005: 1,  0.005: ans,  0.004: f,  0.004: tous,  0.003: tout,  0.003: deux,  0.003: comme\n",
      "\n",
      "=== topic #20\n",
      "0.010: h,  0.010: rue,  0.007: a,  0.006: centre,  0.004: direction,  0.003: si,  0.003: depuis,  0.003: enfants,  0.003: 30,  0.003: plus\n",
      "\n",
      "=== topic #21\n",
      "0.007: a,  0.005: comme,  0.005: plus,  0.003: ans,  0.003: bien,  0.003: tout,  0.003: francs,  0.003: quatre,  0.003: très,  0.003: jean\n",
      "\n",
      "=== topic #22\n",
      "0.019: a,  0.007: ans,  0.007: plus,  0.004: rue,  0.003: tout,  0.003: comme,  0.003: deux,  0.003: leurs,  0.003: remiremont,  0.003: premier\n",
      "\n",
      "=== topic #23\n",
      "0.015: a,  0.004: h,  0.004: tous,  0.004: public,  0.004: aussi,  0.004: rue,  0.003: très,  0.003: deux,  0.003: quelques,  0.003: depuis\n",
      "\n",
      "=== topic #24\n",
      "0.019: h,  0.013: a,  0.006: f,  0.005: 10,  0.005: 45,  0.005: plus,  0.004: 30,  0.004: 3,  0.004: 2,  0.004: 16\n",
      "\n",
      "=== topic #25\n",
      "0.023: h,  0.010: a,  0.005: 10,  0.005: 15,  0.004: rue,  0.004: 14,  0.003: 13,  0.003: 30,  0.003: st,  0.003: 11\n",
      "\n",
      "=== topic #26\n",
      "0.015: a,  0.006: ils,  0.004: fait,  0.004: plus,  0.004: cette,  0.004: bien,  0.003: être,  0.003: après,  0.003: autre,  0.003: si\n",
      "\n",
      "=== topic #27\n",
      "0.015: a,  0.006: comme,  0.005: bien,  0.004: plus,  0.004: cette,  0.003: peu,  0.003: ans,  0.003: deux,  0.003: fait,  0.003: travail\n",
      "\n",
      "=== topic #28\n",
      "0.015: 6,  0.010: a,  0.007: 0,  0.006: 5,  0.005: plus,  0.004: bat,  0.004: 1,  0.004: 15,  0.004: 3,  0.004: 2\n",
      "\n",
      "=== topic #29\n",
      "0.015: a,  0.008: rue,  0.006: plus,  0.004: tout,  0.004: h,  0.003: deux,  0.003: bien,  0.003: très,  0.003: faut,  0.003: ans\n",
      "\n",
      "=== topic #30\n",
      "0.007: rue,  0.004: h,  0.003: a,  0.003: temps,  0.003: cette,  0.003: saint,  0.003: plus,  0.003: vont,  0.002: enfants,  0.002: salle\n",
      "\n",
      "=== topic #31\n",
      "0.018: a,  0.015: f,  0.008: ans,  0.004: plus,  0.003: 500,  0.003: enfants,  0.003: puis,  0.003: après,  0.003: tous,  0.003: jean\n",
      "\n",
      "=== topic #32\n",
      "0.007: a,  0.006: f,  0.003: h,  0.003: enfants,  0.003: tout,  0.003: plus,  0.003: jeandelaincourt,  0.003: trois,  0.002: communes,  0.002: deux\n",
      "\n",
      "=== topic #33\n",
      "0.016: ans,  0.009: h,  0.008: a,  0.005: plus,  0.004: deux,  0.004: jeunes,  0.003: mme,  0.003: stade,  0.003: 13,  0.003: rdv\n",
      "\n",
      "=== topic #34\n",
      "0.010: a,  0.004: ville,  0.004: plus,  0.003: comme,  0.003: choses,  0.003: 6,  0.003: bien,  0.003: belfort,  0.003: rue,  0.003: si\n",
      "\n",
      "=== topic #35\n",
      "0.024: h,  0.010: ind,  0.009: rue,  0.007: a,  0.004: plus,  0.004: 30,  0.003: 8,  0.003: 16,  0.003: 11,  0.003: deux\n",
      "\n",
      "=== topic #36\n",
      "0.004: a,  0.003: juin,  0.003: cette,  0.003: air,  0.003: direction,  0.003: jour,  0.003: capitaine,  0.002: née,  0.002: tout,  0.002: comme\n",
      "\n",
      "=== topic #37\n",
      "0.008: a,  0.005: deux,  0.003: cette,  0.003: trois,  0.003: où,  0.003: ans,  0.003: aussi,  0.003: fait,  0.003: comme,  0.003: jusqu\n",
      "\n",
      "=== topic #38\n",
      "0.008: née,  0.007: f,  0.004: a,  0.004: ans,  0.003: 6,  0.003: plus,  0.003: année,  0.002: école,  0.002: 000,  0.002: 2\n",
      "\n",
      "=== topic #39\n",
      "0.018: a,  0.009: h,  0.005: deux,  0.004: après,  0.004: plus,  0.004: 30,  0.004: cette,  0.003: ans,  0.003: tout,  0.003: première\n",
      "\n",
      "=== topic #40\n",
      "0.015: a,  0.005: plus,  0.003: où,  0.003: boues,  0.003: aussi,  0.003: saison,  0.003: tout,  0.003: ils,  0.002: 2,  0.002: deux\n",
      "\n",
      "=== topic #41\n",
      "0.005: a,  0.004: saison,  0.004: deux,  0.003: plus,  0.003: jeunes,  0.003: 3,  0.003: fait,  0.003: après,  0.003: soir,  0.003: ans\n",
      "\n",
      "=== topic #42\n",
      "0.007: a,  0.006: jean,  0.004: ans,  0.004: lac,  0.003: point,  0.003: cette,  0.003: comme,  0.003: entre,  0.002: autres,  0.002: aussi\n",
      "\n",
      "=== topic #43\n",
      "0.014: a,  0.011: ab,  0.004: plus,  0.004: b,  0.004: tout,  0.003: après,  0.003: comme,  0.003: robert,  0.003: jean,  0.003: rue\n",
      "\n",
      "=== topic #44\n",
      "0.005: a,  0.005: plus,  0.003: si,  0.003: ils,  0.003: très,  0.003: gouvernement,  0.002: projet,  0.002: associations,  0.002: hier,  0.002: kosovo\n",
      "\n",
      "=== topic #45\n",
      "0.004: année,  0.004: a,  0.003: toujours,  0.003: sans,  0.003: entre,  0.003: plus,  0.002: personnel,  0.002: cette,  0.002: tout,  0.002: françois\n",
      "\n",
      "=== topic #46\n",
      "0.021: h,  0.012: a,  0.010: 4,  0.007: ans,  0.006: plus,  0.004: 30,  0.004: n°,  0.003: 3,  0.003: jury,  0.003: 1\n",
      "\n",
      "=== topic #47\n",
      "0.017: 6,  0.009: 3,  0.009: a,  0.007: bat,  0.005: 2,  0.005: h,  0.005: 7,  0.004: 4,  0.004: cette,  0.004: plus\n",
      "\n",
      "=== topic #48\n",
      "0.010: h,  0.008: 4,  0.006: lorraine,  0.005: rue,  0.005: 3,  0.004: f,  0.004: place,  0.003: a,  0.003: salm,  0.003: club\n",
      "\n",
      "=== topic #49\n",
      "0.012: a,  0.009: h,  0.007: 1,  0.004: tout,  0.004: cette,  0.004: partie,  0.003: jusqu,  0.003: plus,  0.003: jean,  0.003: depuis\n",
      "\n",
      "=== topic #50\n",
      "0.007: ind,  0.005: ans,  0.005: a,  0.004: plus,  0.003: h,  0.003: enfants,  0.003: année,  0.003: tour,  0.002: après,  0.002: f\n",
      "\n",
      "=== topic #51\n",
      "0.009: 3,  0.007: a,  0.006: plus,  0.006: ans,  0.005: asptt,  0.005: 02,  0.005: 13,  0.003: depuis,  0.003: h,  0.003: tout\n",
      "\n",
      "=== topic #52\n",
      "0.012: a,  0.006: pts,  0.006: avoir,  0.005: plus,  0.004: cette,  0.004: après,  0.004: autres,  0.003: ans,  0.003: bien,  0.003: fait\n",
      "\n",
      "=== topic #53\n",
      "0.010: a,  0.005: h,  0.003: cette,  0.003: tout,  0.003: collège,  0.003: ils,  0.003: deux,  0.003: enfants,  0.002: jeunes,  0.002: être\n",
      "\n",
      "=== topic #54\n",
      "0.021: h,  0.012: a,  0.009: 1,  0.007: 14,  0.007: 9,  0.007: 15,  0.006: 7,  0.005: 2,  0.005: 16,  0.004: 18\n",
      "\n",
      "=== topic #55\n",
      "0.006: a,  0.004: 2,  0.004: maîche,  0.004: deux,  0.004: enfants,  0.004: encore,  0.003: marie,  0.003: plus,  0.003: fait,  0.003: michel\n",
      "\n",
      "=== topic #56\n",
      "0.007: a,  0.003: jeunes,  0.003: retraités,  0.003: 000,  0.003: cette,  0.003: 5,  0.003: france,  0.003: 7,  0.003: ils,  0.002: tout\n",
      "\n",
      "=== topic #57\n",
      "0.014: a,  0.007: deux,  0.005: plus,  0.004: ans,  0.003: comme,  0.003: françois,  0.003: cette,  0.003: jean,  0.002: simon,  0.002: travail\n",
      "\n",
      "=== topic #58\n",
      "0.009: rue,  0.008: a,  0.006: h,  0.004: là,  0.004: place,  0.004: 13,  0.004: 30,  0.004: jean,  0.004: deux,  0.003: environnement\n",
      "\n",
      "=== topic #59\n",
      "0.010: a,  0.008: ab,  0.004: plus,  0.004: deux,  0.004: b,  0.003: h,  0.002: soir,  0.002: ils,  0.002: également,  0.002: fait\n"
     ]
    }
   ],
   "source": [
    "num_topics= 60\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus_gensim,\n",
    "    id2word      = dictionary,\n",
    "    num_topics   = num_topics,\n",
    "    alpha        = 'asymmetric',\n",
    "    eta          = 'auto',\n",
    "    passes       = 2,\n",
    "    iterations   = 20\n",
    ")\n",
    "\n",
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(\"\\n=== topic #{}\".format(t[0]))\n",
    "    print(t[1].replace('*', ': ').replace(' +',', ').replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== topic #0\n",
      "0.013: a,  0.005: deux,  0.004: plus,  0.004: enfants,  0.004: fait,  0.003: née,  0.003: ils,  0.003: ans,  0.003: être,  0.003: où\n",
      "\n",
      "=== topic #1\n",
      "0.030: h,  0.009: 30,  0.008: 15,  0.008: a,  0.006: 13,  0.005: 1,  0.005: 11,  0.004: 14,  0.004: 9,  0.004: 17\n",
      "\n",
      "=== topic #2\n",
      "0.014: a,  0.007: ab,  0.003: cette,  0.003: b,  0.003: ils,  0.003: bien,  0.003: après,  0.003: tout,  0.002: où,  0.002: plus\n",
      "\n",
      "=== topic #3\n",
      "0.007: a,  0.004: 7,  0.004: 9,  0.003: 1,  0.003: 5,  0.003: 3,  0.003: après,  0.003: 15,  0.003: dimanche,  0.003: équipe\n",
      "\n",
      "=== topic #4\n",
      "0.011: ans,  0.009: a,  0.006: f,  0.005: deux,  0.005: comme,  0.004: plus,  0.004: h,  0.004: enfants,  0.003: tout,  0.003: jean\n",
      "\n",
      "=== topic #5\n",
      "0.025: a,  0.007: plus,  0.007: deux,  0.005: ils,  0.004: h,  0.003: ans,  0.003: comme,  0.003: fait,  0.003: cette,  0.003: tout\n",
      "\n",
      "=== topic #6\n",
      "0.011: a,  0.006: plus,  0.005: deux,  0.005: cette,  0.004: ils,  0.004: ans,  0.003: être,  0.003: après,  0.003: très,  0.003: fait\n",
      "\n",
      "=== topic #7\n",
      "0.019: h,  0.011: a,  0.007: 30,  0.005: ans,  0.004: plus,  0.004: 18,  0.004: dimanche,  0.003: 14,  0.003: après,  0.003: sans\n",
      "\n",
      "=== topic #8\n",
      "0.007: a,  0.005: 3,  0.005: club,  0.005: plus,  0.004: h,  0.004: 13,  0.003: cette,  0.003: ville,  0.003: saison,  0.003: deux\n",
      "\n",
      "=== topic #9\n",
      "0.010: a,  0.006: f,  0.006: h,  0.004: plus,  0.003: lorraine,  0.003: très,  0.003: 30,  0.003: 0,  0.003: travaux,  0.003: 00\n",
      "\n",
      "=== topic #10\n",
      "0.021: a,  0.008: plus,  0.004: ind,  0.004: cette,  0.003: tout,  0.003: après,  0.003: bien,  0.003: leurs,  0.003: aussi,  0.002: si\n",
      "\n",
      "=== topic #11\n",
      "0.008: f,  0.008: a,  0.004: deux,  0.004: 500,  0.003: ind,  0.003: r,  0.003: cette,  0.003: 3,  0.003: h,  0.002: être\n",
      "\n",
      "=== topic #12\n",
      "0.011: a,  0.004: cette,  0.004: h,  0.003: plus,  0.003: après,  0.003: deux,  0.003: ans,  0.002: toujours,  0.002: tout,  0.002: aussi\n",
      "\n",
      "=== topic #13\n",
      "0.008: a,  0.007: rue,  0.006: plus,  0.004: place,  0.004: comme,  0.004: ans,  0.003: deux,  0.003: très,  0.003: entre,  0.002: cette\n",
      "\n",
      "=== topic #14\n",
      "0.018: a,  0.007: ans,  0.006: rue,  0.004: tout,  0.004: plus,  0.004: année,  0.004: fait,  0.003: cette,  0.003: deux,  0.003: comme\n",
      "\n",
      "=== topic #15\n",
      "0.035: h,  0.011: a,  0.010: 30,  0.005: 14,  0.004: cette,  0.004: plus,  0.004: ans,  0.003: 16,  0.003: 2,  0.003: 17\n",
      "\n",
      "=== topic #16\n",
      "0.004: rue,  0.003: nicolas,  0.003: a,  0.002: grégory,  0.002: lac,  0.002: yannick,  0.002: philippe,  0.002: pharmacie,  0.002: jean,  0.002: rémi\n",
      "\n",
      "=== topic #17\n",
      "0.015: 6,  0.009: a,  0.009: ans,  0.006: h,  0.006: 3,  0.006: plus,  0.005: 5,  0.004: pts,  0.004: 14,  0.004: 2\n",
      "\n",
      "=== topic #18\n",
      "0.018: a,  0.009: plus,  0.004: cette,  0.004: comme,  0.003: fait,  0.003: être,  0.003: bien,  0.003: sous,  0.003: ans,  0.003: 7\n",
      "\n",
      "=== topic #19\n",
      "0.009: a,  0.005: maîche,  0.004: soeurs,  0.003: charquemont,  0.003: jeunes,  0.003: marie,  0.003: bien,  0.002: sans,  0.002: hier,  0.002: gauche\n",
      "\n",
      "=== topic #20\n",
      "0.010: h,  0.007: a,  0.005: rue,  0.005: cette,  0.004: 4,  0.003: plus,  0.003: trois,  0.003: année,  0.003: bien,  0.003: après\n",
      "\n",
      "=== topic #21\n",
      "0.005: rue,  0.005: a,  0.003: cette,  0.003: entre,  0.003: partie,  0.003: fait,  0.003: équipe,  0.002: sens,  0.002: plus,  0.002: tout\n",
      "\n",
      "=== topic #22\n",
      "0.017: a,  0.005: plus,  0.004: comme,  0.003: autres,  0.003: 000,  0.003: aussi,  0.003: après,  0.002: fait,  0.002: cette,  0.002: faire\n",
      "\n",
      "=== topic #23\n",
      "0.023: h,  0.009: 1,  0.007: a,  0.003: jean,  0.003: 10,  0.003: deux,  0.003: tout,  0.003: 25,  0.003: 29,  0.003: plus\n",
      "\n",
      "=== topic #24\n",
      "0.012: a,  0.003: tout,  0.003: cette,  0.003: h,  0.003: tous,  0.003: ans,  0.002: deux,  0.002: plus,  0.002: mois,  0.002: 000\n"
     ]
    }
   ],
   "source": [
    "num_topics= 25\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus_gensim,\n",
    "    id2word      = dictionary,\n",
    "    num_topics   = num_topics,\n",
    "    alpha        = 'asymmetric',\n",
    "    eta          = 'auto',\n",
    "    passes       = 3,\n",
    "    iterations   = 40\n",
    ")\n",
    "\n",
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(\"\\n=== topic #{}\".format(t[0]))\n",
    "    print(t[1].replace('*', ': ').replace(' +',', ').replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour resumer, on agite la baguete magique, on teste le nombre de topics, le nombre de iteration et le nombre de passes jusqu'a \n",
    "#trouver un vomit de topic minimement interpretable pour mettre des mot savant tel que \"attentint la stabilité\", \n",
    "#\"apres une selection exhaustive\" qui veut dire que on sait pas pourquoi mais on se la pete de ecrire deux ligne de code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
