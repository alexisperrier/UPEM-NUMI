{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120, 1)\n",
      "= dimensions\n",
      "(15120, 1)\n",
      "= 5 premieres rangés\n",
      "                                                text\n",
      "0  Le député maire, Claude Jacquot, et le maire J...\n",
      "1  Pour la première fois, Le Val d'Ajol recevait ...\n",
      "2  Depuis la fin des années 60 en effet, la ville...\n",
      "3  « La leçon à retenir est qu'il faut être plus ...\n",
      "4  BESANÇON._ Le sénateur UDF de Haute-Saône, Ber...\n",
      "= Noms des colonnes\n",
      "Index(['text'], dtype='object')\n",
      "['les', 'médiateurs', 'confirment', '«', \"l'absence\", 'de', 'solution', 'parfaite', '»', '.']\n",
      "['les', 'médiateurs', 'confirment', '«', \"l'absence\", 'de', 'solution', 'parfaite', '».', '']\n",
      "Liste original de signe de ponctuations\n",
      "\t!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "{'!': ' ', '\"': ' ', '#': ' ', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '-': ' ', '.': ' ', '/': ' ', ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '_': ' ', '`': ' ', '{': ' ', '|': ' ', '}': ' ', '~': ' ', '«': ' ', '»': ' '}\n",
      "les médiateurs confirment « l'absence de solution parfaite ». \n",
      "les médiateurs confirment   l absence de solution parfaite    \n",
      "['les', 'médiateurs', 'confirment', 'l', 'absence', 'de', 'solution', 'parfaite']\n",
      "['médiateurs', 'confirment', 'absence', 'solution', 'parfaite']\n",
      "Index(['text', 'text_no_punctuation', 'tokens_all', 'tokens'], dtype='object')\n",
      "Index(['text', 'tokens'], dtype='object')\n",
      "                                                text  \\\n",
      "0  Le député maire, Claude Jacquot, et le maire J...   \n",
      "1  Pour la première fois, Le Val d'Ajol recevait ...   \n",
      "2  Depuis la fin des années 60 en effet, la ville...   \n",
      "3  « La leçon à retenir est qu'il faut être plus ...   \n",
      "4  BESANÇON._ Le sénateur UDF de Haute-Saône, Ber...   \n",
      "\n",
      "                                              tokens  token_count  \n",
      "0  [député, maire, claude, jacquot, maire, jacky,...           71  \n",
      "1  [première, fois, val, ajol, recevait, groupe, ...           66  \n",
      "2  [depuis, fin, années, 60, effet, ville, gère, ...           61  \n",
      "3  [leçon, retenir, faut, être, plus, réaliste, p...           73  \n",
      "4  [besançon, sénateur, udf, haute, saône, bernar...           53  \n",
      "[]\n",
      "Int64Index([], dtype='int64')\n",
      "(15119, 3)\n",
      "(15119, 3)\n",
      "(15119, 3)\n",
      "count    15119.000000\n",
      "mean        80.308817\n",
      "std         31.684881\n",
      "min         44.000000\n",
      "25%         64.000000\n",
      "50%         72.000000\n",
      "75%         86.000000\n",
      "max        776.000000\n",
      "Name: token_count, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexis/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:165: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(81946 unique tokens: ['député', 'maire', 'claude', 'jacquot', 'jacky']...)\n",
      "[[ \"Le député maire, Claude Jacquot, et le maire Jacky Chozerot se sont accordés à reconnaître l'esprit inventif des enfants. C'est avec beaucoup d'émotion dans la voix que Chantal Gérard a dédié cette exposition à Robert George qui fut, a-t-elle dit, un excellent pédagogue, un professeur d'arts plastiques remarquable, un grand artiste, un grand homme. Avant de remercier son fils Jean-Claude George, « grâce auquel je me suis lancée dans ce projet » a-t-elle poursuivi, « merci aussi à Mme Robert, conseillère pédagogique en arts plastiques, à M. Piesch pour leurs excellents conseils et merci surtout aux enfants qui se sont investis toute l'année et qui sont aujourd'hui les maîtres d'œuvres de cette exposition ».\"\n",
      "  list([(0, 1), (1, 2), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 3), (17, 1), (18, 2), (19, 2), (20, 2), (21, 2), (22, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 2), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1)])]\n",
      " [ \"Pour la première fois, Le Val d'Ajol recevait un groupe folklorique roumain. Les vingt-huit danseurs et musiciens, originaires de Deva, achèveront leur tournée en France dans une semaine. Le groupe est issu du « Palais des élèves », une institution de Transylvanie, qui propose des activités culturelles à ses pensionnaires, comme la danse et le chant mais aussi le petit artisanat. Un stand installé dans le hall de la salle des fêtes donnait une idée des multiples talents du peuple roumain. En plus du violon, spécialité de la ville de Reghin au centre des Carpates, proposé à moins de 1.000 F, les amateurs de clarinette pouvaient s'initier au « taragot », un instrument spécifique à la région de Deva.\"\n",
      "  list([(42, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 2), (64, 1), (65, 2), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1)])]\n",
      " [ \"Depuis la fin des années 60 en effet, la ville ne gère plus directement son domaine forestier. Tout le travail d'entretien et de maintenance est confié à l'ONF par convention tacitement renouvelable, comme le rappelle Gérard Kœberlé : « L'office prévoit son échéancier de travaux chaque année pour la réfection des routes forestières. De même, la ville lui confie la concession des sources. A charge pour nous de régler la facture en retour, à chaque fin d'année...». Un dispositif qui pourrait fonctionner en faveur de l'abri forestier des Trois Fauteuils, si un jour l'optimisme prévaut sur la lassitude imposée par les casseurs et les pyromanes.\"\n",
      "  list([(15, 1), (16, 1), (53, 2), (85, 1), (100, 1), (103, 2), (120, 1), (121, 2), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 2), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 2), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1)])]\n",
      " [ \"« La leçon à retenir est qu'il faut être plus réaliste, plus efficace », reconnaissait l'international junior, « les Anglais ont trois ou quatre occasions. A l'arrivée, cela fait trois buts. Ils ont aussi du métier. Ils savent mettre des coups quand il faut, sans se faire voir. Sur chaque appel de balle, chaque saut, je prenais un petit coup dans le ventre. Il m'a fallu du temps pour arriver à me positionner face à mon attaquant sans faire de faute. De plus, après que mon adversaire ait râlé une ou deux fois, l'arbitre a commencé à siffler contre moi. Je suis jeune. Si j'avais eu 27-28 ans, il ne l'aurait pas fait. Il faut que ce match me serve de leçon »\"\n",
      "  list([(16, 3), (42, 1), (59, 1), (88, 1), (100, 3), (144, 2), (160, 2), (162, 1), (170, 2), (171, 1), (172, 3), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 2), (185, 1), (186, 2), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 2), (193, 2), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1)])]\n",
      " [ \"BESANÇON._ Le sénateur UDF de Haute-Saône, Bernard Joly, avait écrit à Dominique Voynet pour lui faire part de ses craintes quant à la possibilité de voir son département prochainement privé de la prime à l'aménagement du territoire. En lui rappelant que cette même PAT avait permis « d'orienter la localisation de nouvelles entreprises et ainsi de créer plus de 300 emplois » dans les « zones défavorisées » de Haute-Saône, le parlementaire avait insisté sur « la nécessité de poursuivre le désenclavement » de cette dernière. Et de rapidement la doter « d'infrastructures routières et ferroviaires dignes de ce nom ».\"\n",
      "  list([(18, 2), (100, 1), (193, 1), (194, 1), (222, 1), (223, 1), (224, 1), (225, 2), (226, 2), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1)])]]\n",
      "\n",
      "=== topic #0\n",
      "0.026: a,  0.008: cette,  0.006: plus,  0.004: être,  0.004: deux,  0.004: depuis,  0.003: aussi,  0.003: maire,  0.003: fait,  0.003: tout\n",
      "\n",
      "=== topic #1\n",
      "0.019: président,  0.012: a,  0.010: club,  0.009: jean,  0.005: comité,  0.005: jeunes,  0.005: cette,  0.005: assemblée,  0.005: michel,  0.004: générale\n",
      "\n",
      "=== topic #2\n",
      "0.006: classe,  0.006: a,  0.004: école,  0.004: cette,  0.004: élèves,  0.003: chevaux,  0.003: crédit,  0.003: ils,  0.003: enfants,  0.003: gagnées\n",
      "\n",
      "=== topic #3\n",
      "0.024: a,  0.008: plus,  0.006: deux,  0.005: cette,  0.005: ans,  0.004: après,  0.004: jours,  0.004: tous,  0.004: où,  0.004: 000\n",
      "\n",
      "=== topic #4\n",
      "0.059: 6,  0.031: 0,  0.020: 4,  0.020: 5,  0.020: 3,  0.019: 2,  0.016: bat,  0.014: 1,  0.013: 7,  0.011: rue\n",
      "\n",
      "=== topic #5\n",
      "0.006: a,  0.006: jeunes,  0.006: catégorie,  0.005: saint,  0.004: baccarat,  0.004: ans,  0.003: catégories,  0.003: etape,  0.003: france,  0.003: dimanche\n",
      "\n",
      "=== topic #6\n",
      "0.006: a,  0.005: exposition,  0.004: art,  0.004: deux,  0.004: pratiques,  0.004: cette,  0.003: plus,  0.003: jury,  0.003: nancy,  0.003: peinture\n",
      "\n",
      "=== topic #7\n",
      "0.008: a,  0.004: cette,  0.003: plus,  0.003: autorités,  0.003: commissariat,  0.003: après,  0.003: deux,  0.003: sans,  0.003: hier,  0.003: dim\n",
      "\n",
      "=== topic #8\n",
      "0.020: a,  0.008: fête,  0.008: plus,  0.008: cette,  0.007: tout,  0.007: musique,  0.007: spectacle,  0.006: public,  0.005: tous,  0.005: après\n",
      "\n",
      "=== topic #9\n",
      "0.069: rue,  0.009: eau,  0.008: place,  0.006: chemin,  0.005: a,  0.004: chantier,  0.004: entre,  0.004: m3,  0.004: avenue,  0.003: eaux\n",
      "\n",
      "=== topic #10\n",
      "0.014: ils,  0.009: a,  0.008: village,  0.007: visite,  0.006: deux,  0.006: bois,  0.006: après,  0.005: où,  0.005: route,  0.005: enfants\n",
      "\n",
      "=== topic #11\n",
      "0.011: 4e,  0.010: 6e,  0.010: élèves,  0.009: collège,  0.008: a,  0.007: 3e,  0.006: classes,  0.005: 5e,  0.005: classe,  0.004: cm2\n",
      "\n",
      "=== topic #12\n",
      "0.020: plus,  0.019: a,  0.009: bien,  0.009: tout,  0.008: fait,  0.007: ans,  0.007: comme,  0.007: faire,  0.007: si,  0.007: ils\n",
      "\n",
      "=== topic #13\n",
      "0.015: rue,  0.008: a,  0.007: gendarmerie,  0.007: allée,  0.007: pompiers,  0.006: place,  0.005: chef,  0.005: véhicules,  0.005: circulation,  0.004: général\n",
      "\n",
      "=== topic #14\n",
      "0.014: mme,  0.006: a,  0.005: professeur,  0.004: jean,  0.003: deux,  0.003: 1983,  0.003: chocolat,  0.003: médaille,  0.003: jacqueline,  0.003: comédiens\n",
      "\n",
      "=== topic #15\n",
      "0.012: a,  0.004: plus,  0.004: vis,  0.004: ruisseau,  0.003: contre,  0.003: cette,  0.003: conception,  0.002: tué,  0.002: danjoutin,  0.002: ans\n",
      "\n",
      "=== topic #16\n",
      "0.057: f,  0.032: 000,  0.013: travaux,  0.008: conseil,  0.008: 500,  0.008: 1,  0.007: communes,  0.005: a,  0.005: commune,  0.005: francs\n",
      "\n",
      "=== topic #17\n",
      "0.037: a,  0.011: ans,  0.008: ils,  0.007: famille,  0.006: très,  0.006: tout,  0.006: fait,  0.006: où,  0.005: comme,  0.005: deux\n",
      "\n",
      "=== topic #18\n",
      "0.013: saint,  0.009: a,  0.006: église,  0.006: eau,  0.005: secours,  0.005: deux,  0.005: mètres,  0.004: fave,  0.003: abbé,  0.003: pompiers\n",
      "\n",
      "=== topic #19\n",
      "0.007: mihiel,  0.006: km,  0.005: aix,  0.005: thionville,  0.004: revers,  0.004: clermont,  0.004: a,  0.004: hettange,  0.004: stenay,  0.004: sarreguemines\n",
      "\n",
      "=== topic #20\n",
      "0.005: a,  0.004: bien,  0.004: jaune,  0.004: tir,  0.003: devant,  0.003: ceinture,  0.003: supporters,  0.003: moto,  0.003: deux,  0.003: après\n",
      "\n",
      "=== topic #21\n",
      "0.008: hadol,  0.006: laure,  0.004: a,  0.004: jazz,  0.004: corinne,  0.004: marie,  0.004: juliette,  0.003: ben,  0.003: jérémie,  0.003: mireille\n",
      "\n",
      "=== topic #22\n",
      "0.069: ind,  0.011: a,  0.010: p,  0.006: belfort,  0.006: psa,  0.005: ascap,  0.005: f,  0.005: dtat,  0.005: g,  0.004: e\n",
      "\n",
      "=== topic #23\n",
      "0.023: a,  0.010: deux,  0.010: équipe,  0.010: plus,  0.007: tout,  0.006: saison,  0.006: joueurs,  0.006: après,  0.006: bien,  0.006: club\n",
      "\n",
      "=== topic #24\n",
      "0.013: pts,  0.006: esp,  0.006: fra,  0.005: yamaha,  0.005: a,  0.005: nc,  0.005: adam,  0.004: ingénieurs,  0.004: vauthier,  0.004: fromages\n",
      "\n",
      "=== topic #25\n",
      "0.017: 00,  0.010: ii,  0.005: a,  0.004: croit,  0.003: tactique,  0.003: 0,  0.003: peltre,  0.003: faverney,  0.003: cités,  0.003: pénalité\n",
      "\n",
      "=== topic #26\n",
      "0.011: nicolas,  0.008: julien,  0.008: thomas,  0.008: pierre,  0.008: sébastien,  0.008: marie,  0.007: david,  0.007: jean,  0.007: anne,  0.007: emilie\n",
      "\n",
      "=== topic #27\n",
      "0.029: 1,  0.021: 2,  0.015: 3,  0.015: p,  0.014: 4,  0.012: 5,  0.011: 8,  0.010: 10,  0.010: f,  0.009: a\n",
      "\n",
      "=== topic #28\n",
      "0.036: ab,  0.017: b,  0.009: damien,  0.006: florence,  0.005: benjamin,  0.004: nicolas,  0.004: tb,  0.004: martial,  0.003: a,  0.003: magali\n",
      "\n",
      "=== topic #29\n",
      "0.016: a,  0.004: plus,  0.004: france,  0.004: gouvernement,  0.004: milliards,  0.003: europe,  0.003: ministre,  0.003: aujourd,  0.003: hui,  0.003: européen\n",
      "\n",
      "=== topic #30\n",
      "0.018: mp,  0.006: champagne,  0.006: nice,  0.005: tribune,  0.004: euro,  0.004: a,  0.004: canards,  0.004: redevance,  0.003: d2,  0.003: ouvrière\n",
      "\n",
      "=== topic #31\n",
      "0.012: tri,  0.008: kg,  0.005: a,  0.005: ce1,  0.005: cp,  0.004: instituteurs,  0.004: générosité,  0.004: ce2,  0.003: bouteilles,  0.003: douche\n",
      "\n",
      "=== topic #32\n",
      "0.135: h,  0.041: 30,  0.018: 15,  0.018: 14,  0.016: 20,  0.016: 18,  0.014: 17,  0.014: 10,  0.014: 12,  0.012: 16\n",
      "\n",
      "=== topic #33\n",
      "0.021: enfants,  0.011: école,  0.010: jeunes,  0.010: activités,  0.008: année,  0.007: parents,  0.007: club,  0.007: tous,  0.007: association,  0.007: centre\n",
      "\n",
      "=== topic #34\n",
      "0.022: a,  0.008: prison,  0.008: tribunal,  0.006: faits,  0.006: mois,  0.005: voiture,  0.005: avoir,  0.005: homme,  0.004: hier,  0.004: ans\n",
      "\n",
      "=== topic #35\n",
      "0.016: pêche,  0.010: eau,  0.009: chasse,  0.007: éclipse,  0.006: cartes,  0.006: étang,  0.006: poissons,  0.006: a,  0.005: chasseurs,  0.005: pêcheurs\n",
      "\n",
      "=== topic #36\n",
      "0.006: jumping,  0.005: uc,  0.005: lallemand,  0.004: a,  0.004: nomeny,  0.004: omnisports,  0.004: seille,  0.004: comprise,  0.003: etoile,  0.003: dizier\n",
      "\n",
      "=== topic #37\n",
      "0.006: bonnet,  0.005: a,  0.005: corse,  0.005: régiment,  0.004: ancien,  0.004: anciens,  0.004: bernard,  0.004: colonel,  0.004: préfet,  0.003: rencontrent\n",
      "\n",
      "=== topic #38\n",
      "0.019: a,  0.008: plus,  0.005: bien,  0.004: hier,  0.003: coups,  0.003: fait,  0.003: deux,  0.003: quelques,  0.003: lait,  0.003: faire\n",
      "\n",
      "=== topic #39\n",
      "0.013: verdun,  0.013: ans,  0.012: jean,  0.011: marie,  0.010: bar,  0.008: née,  0.007: claude,  0.007: duc,  0.007: commercy,  0.006: saint\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "filename  = 'estrepublicain_annee_1999.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename)\n",
    "# df = df[0:1000]\n",
    "\n",
    "# sampler \n",
    "\n",
    "df = df.sample(frac= 0.5)\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "print(df.shape)\n",
    "\n",
    "df\n",
    "print(\"= dimensions\")\n",
    "print(df.shape)\n",
    "print(\"= 5 premieres rangés\")\n",
    "print(df.head())\n",
    "print(\"= Noms des colonnes\")\n",
    "print(df.columns)\n",
    "\n",
    "# Changer le nom de la colonne\n",
    "df.columns = ['paragraphe']\n",
    "df.columns\n",
    "\n",
    "# puis renommer colonne en 'text'\n",
    "\n",
    "df.columns = ['text']\n",
    "df.columns\n",
    "\n",
    "# Tokenization\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# La phrase\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(tokens)\n",
    "tokens_on_space = sentence.split(' ')\n",
    "print(tokens_on_space)\n",
    "\n",
    "# ponctuation\n",
    "\n",
    "import string\n",
    "\n",
    "print(\"Liste original de signe de ponctuations\")\n",
    "print(\"\\t{}\".format(string.punctuation))\n",
    "\n",
    "sentence = \"les médiateurs confirment « l'absence de solution parfaite ». \"\n",
    "punctuation_chars = string.punctuation + \"«»\"\n",
    "\n",
    "#  Construction d'un dict { '~':' ', '$': ' ', ... }\n",
    "dict_ponctuation = {}\n",
    "for k in punctuation_chars:\n",
    "    dict_ponctuation[k] = ' '\n",
    "\n",
    "print(dict_ponctuation)\n",
    "\n",
    "# # L'operateur de translation\n",
    "translator = str.maketrans(dict_ponctuation)\n",
    "\n",
    "new_sentence = sentence.translate(translator)\n",
    "print(sentence)\n",
    "print(new_sentence)\n",
    "\n",
    "# stopwords\n",
    "tokens = word_tokenize(new_sentence)\n",
    "print(tokens)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# print(\"=== stopwords - français:\")\n",
    "# print(sorted(stopwords.words('french')))\n",
    "\n",
    "list_stopwords = stopwords.words('french') + ['les', 'de']\n",
    "\n",
    "# # equivalent:\n",
    "# stopwords = stopwords.words('french')\n",
    "# stopwords.append('les')\n",
    "# stopwords.append('de')\n",
    "\n",
    "\n",
    "# \n",
    "tokens_sans_stopwords = [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "## Equivalent \n",
    "tokens_sans_stopwords = []\n",
    "for w in tokens:\n",
    "    if w not in list_stopwords:\n",
    "        tokens_sans_stopwords.append(w)\n",
    "\n",
    "\n",
    "\n",
    "print(tokens_sans_stopwords)\n",
    "\n",
    "# Enlevons la ponctuation\n",
    "\n",
    "df['text_no_punctuation'] = df.text.apply(lambda \n",
    "    r : ( r.translate(translator) ) \n",
    ")\n",
    "df.head()\n",
    "\n",
    "# Tokenizer\n",
    "\n",
    "df['tokens_all']  = df.text_no_punctuation.apply(\n",
    "    lambda r : word_tokenize(r.lower())\n",
    ")\n",
    "\n",
    "df.head()\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "def remove_stopword(tokens):\n",
    "     return [w for w in tokens if (w not in list_stopwords) ]\n",
    "\n",
    "# Verifier que ca marche\n",
    "remove_stopword(tokens)\n",
    "\n",
    "# appliquer a la dataframe\n",
    "\n",
    "df['tokens'] = df.tokens_all.apply(\n",
    "    lambda tks : remove_stopword(tks) \n",
    ")\n",
    "\n",
    "df.head()\n",
    "\n",
    "# enlever les colonnes intermediaires\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df = df[['text', 'tokens']]\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# nombre de tokens par rangée\n",
    "pd.options.mode.chained_assignment = None\n",
    "df['token_count'] = df.tokens.apply( lambda r : len(r) )\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# repartition du nombre de tokens\n",
    "df.token_count.describe()\n",
    "\n",
    "# quel document a 955 tokens?\n",
    "\n",
    "condition = (df.token_count == 955)\n",
    "print(df[condition].text.values)\n",
    "\n",
    "# quel index\n",
    "print(df[condition].index)\n",
    "\n",
    "# \n",
    "condition = (df.token_count < 800)\n",
    "print(df[condition].shape)\n",
    "\n",
    "# enlever le paragraphe le plus long\n",
    "\n",
    "condition_filtrage = df.token_count < 955\n",
    "print(df[condition_filtrage].shape)\n",
    "\n",
    "df = df[condition_filtrage]\n",
    "print(df[condition_filtrage].shape)\n",
    "\n",
    "print(df.token_count.describe())\n",
    "\n",
    "# Gensim - Vocabulaire\n",
    "\n",
    "from gensim import corpora, models\n",
    "dictionary  = corpora.Dictionary(df.tokens)\n",
    "print(dictionary)\n",
    "\n",
    "# corpus_gensim\n",
    "\n",
    "df['corpus_gensim'] = df.tokens.apply(lambda d : dictionary.doc2bow(d))\n",
    "\n",
    "print(df[['text','corpus_gensim']] .head().values)\n",
    "\n",
    "corpus_gensim = [c for c in df.corpus_gensim ]\n",
    "\n",
    "num_topics= 40\n",
    "\n",
    "# Le model LDA\n",
    "lda = models.LdaModel(corpus_gensim,\n",
    "    id2word      = dictionary,\n",
    "    num_topics   = num_topics,\n",
    "    alpha        = 'asymmetric',\n",
    "    eta          = 'auto',\n",
    "    passes       = 2,\n",
    "    iterations   = 20\n",
    ")\n",
    "\n",
    "for t in lda.show_topics(num_topics=num_topics, formatted=True, log = False):\n",
    "    print(\"\\n=== topic #{}\".format(t[0]))\n",
    "    print(t[1].replace('*', ': ').replace(' +',', ').replace('\"',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
